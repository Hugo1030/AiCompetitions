
2018.6.1
neck droupout 0
```
lr=1e-4
Epoch 1/1
254/254 [==============================] - 213s 840ms/step - loss: 0.3348 - acc: 0.9076 - val_loss: 0.4522 - val_acc: 0.8358

Epoch 00001: val_loss improved from inf to 0.45216, saving model to models/neck2_0601_xception_psd_dr.h5
CPU times: user 5min 19s, sys: 1min 5s, total: 6min 24s
Wall time: 3min 46s

lr=1e-5
Epoch 1/1
254/254 [==============================] - 210s 828ms/step - loss: 0.3077 - acc: 0.9232 - val_loss: 0.4291 - val_acc: 0.8480

Epoch 00001: val_loss improved from inf to 0.42915, saving model to models/neck3_0601_xception_psd_dr.h5
CPU times: user 5min 18s, sys: 1min 3s, total: 6min 22s
Wall time: 3min 43s

lr=1e-6

Epoch 1/1
254/254 [==============================] - 211s 832ms/step - loss: 0.2900 - acc: 0.9248 - val_loss: 0.4300 - val_acc: 0.8444

Epoch 00001: val_loss improved from inf to 0.43003, saving model to models/neck3_0601_xception_psd_dr.h5
CPU times: user 5min 18s, sys: 1min 5s, total: 6min 24s
Wall time: 3min 46s

```
neck droupout 0.3 
```
Epoch 1/1
254/254 [==============================] - 213s 837ms/step - loss: 0.3476 - acc: 0.9049 - val_loss: 0.4267 - val_acc: 0.8505

Epoch 00001: val_loss improved from inf to 0.42674, saving model to models/neck2_0601_xception_psd_dr.h5
CPU times: user 5min 23s, sys: 1min 3s, total: 6min 26s
Wall time: 3min 49s

Epoch 1/1
254/254 [==============================] - 214s 842ms/step - loss: 0.3012 - acc: 0.9228 - val_loss: 0.4206 - val_acc: 0.8444

Epoch 00001: val_loss improved from inf to 0.42056, saving model to models/neck3_0601_xception_psd_dr.h5
CPU times: user 5min 24s, sys: 1min 3s, total: 6min 27s
Wall time: 3min 51s

Epoch 1/1
254/254 [==============================] - 215s 845ms/step - loss: 0.2954 - acc: 0.9257 - val_loss: 0.4195 - val_acc: 0.8456

Epoch 00001: val_loss improved from inf to 0.41954, saving model to models/neck3_0601_xception_psd_dr.h5
CPU times: user 5min 23s, sys: 1min 3s, total: 6min 27s
Wall time: 3min 50s

```

neck droupout 0.5
```
lr=1e-4

Epoch 1/1
254/254 [==============================] - 214s 841ms/step - loss: 0.3431 - acc: 0.9117 - val_loss: 0.4196 - val_acc: 0.8493

Epoch 00001: val_loss improved from inf to 0.41961, saving model to models/neck2_0601_xception_psd.h5
CPU times: user 5min 21s, sys: 1min 9s, total: 6min 30s
Wall time: 3min 49s

lr=1e-5

Epoch 1/1
254/254 [==============================] - 213s 839ms/step - loss: 0.3083 - acc: 0.9202 - val_loss: 0.4151 - val_acc: 0.8382

Epoch 00001: val_loss improved from inf to 0.41506, saving model to models/neck3_0601_xception_psd.h5
CPU times: user 5min 21s, sys: 1min 6s, total: 6min 27s
Wall time: 3min 47s

lr=1e-6
Epoch 1/1
254/254 [==============================] - 214s 842ms/step - loss: 0.3063 - acc: 0.9203 - val_loss: 0.4148 - val_acc: 0.8419

Epoch 00001: val_loss improved from inf to 0.41484, saving model to models/neck4_0601_xception_psd.h5
CPU times: user 5min 23s, sys: 1min 7s, total: 6min 31s
Wall time: 3min 50s

```
neck droupout 0.7
```
lr=1e-4
Epoch 1/2
254/254 [==============================] - 218s 857ms/step - loss: 0.3488 - acc: 0.9062 - val_loss: 0.4362 - val_acc: 0.8431

Epoch 00001: val_loss improved from inf to 0.43621, saving model to models/neck2_0601_xception_psd_dr.h5
Epoch 2/2
254/254 [==============================] - 201s 790ms/step - loss: 0.3146 - acc: 0.9217 - val_loss: 0.4271 - val_acc: 0.8407

Epoch 00002: val_loss improved from 0.43621 to 0.42709, saving model to models/neck2_0601_xception_psd_dr.h5


lr=1e-5
Epoch 1/2
254/254 [==============================] - 218s 858ms/step - loss: 0.2860 - acc: 0.9302 - val_loss: 0.4131 - val_acc: 0.8566

Epoch 00001: val_loss improved from inf to 0.41313, saving model to models/neck3_0601_xception_psd_dr.h5
Epoch 2/2
254/254 [==============================] - 202s 793ms/step - loss: 0.2883 - acc: 0.9327 - val_loss: 0.4149 - val_acc: 0.8566

Epoch 00002: val_loss did not improve from 0.41313
CPU times: user 10min 25s, sys: 2min 1s, total: 12min 27s
Wall time: 7min 19s

lr=1e-6
Epoch 1/2
254/254 [==============================] - 221s 868ms/step - loss: 0.2887 - acc: 0.9306 - val_loss: 0.4151 - val_acc: 0.8554

Epoch 00001: val_loss improved from inf to 0.41512, saving model to models/neck3_0601_xception_psd_dr.h5
Epoch 2/2
254/254 [==============================] - 201s 791ms/step - loss: 0.2838 - acc: 0.9329 - val_loss: 0.4132 - val_acc: 0.8578

Epoch 00002: val_loss improved from 0.41512 to 0.41316, saving model to models/neck3_0601_xception_psd_dr.h5
CPU times: user 10min 29s, sys: 2min 3s, total: 12min 32s
Wall time: 7min 23s

```

2018.5.31
- 训练类型：coat  
- 前提条件：coat_augment.best0525_Inception.h5
- coat_augment.best0527_Xception.h5
- 训练参数和模型 第二轮使用 padding rotation30 rotation-30 flip+his
- 环境：google GPU
Woody0531_coat_augment_inception_all
Woody0531_coat_augment_xception_all
xception/inception
第二轮lr=0.0001 padding+顺逆30°+round1_padding
第三轮lr=0.00001 padding+顺逆30°+round1_padding
```
for i in tqdm(range(len(df_load_1))):
    tmp_label = df_load_1['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_1/{0}'.format(df_load_1['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_1[i] = cv2.resize(pad_img, (width, width), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y_1[i][tmp_label.find('y')] = 1

#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,1,2]
n_times=len(trans_style)
X_train = np.zeros((n*n_times+len(X_1), width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times+len(y_1), n_class), dtype=np.uint8)

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    for j in range(len(trans_style)):
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1

X_train[39024:]= X_1
y_train[39024:]= y_1

adam = Adam(lr=0.0001)

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

save_path='models/{0}_augment.best0531_Inception.h5'.format(prefix_cls)
load_path='models/{0}_augment.best0525_Inception.h5'.format(prefix_cls)              
```
lr=0.0001 coat3_augment.best0531_Xception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1309s 25ms/step - loss: 0.7222 - acc: 0.6882 - val_loss: 0.8555 - val_acc: 0.6722

Epoch 00001: val_loss improved from inf to 0.85549, saving model to models/coat3_augment.best0531_Xception.h5
Epoch 2/3
52032/52032 [==============================] - 1288s 25ms/step - loss: 0.4603 - acc: 0.7845 - val_loss: 0.9791 - val_acc: 0.6598

Epoch 00002: val_loss did not improve from 0.85549
Epoch 3/3
  608/52032 [..............................] - ETA: 21:09 - loss: 0.2961 - acc: 0.8618KeyboardInterrupt


```
lr=0.00001 coat4_augment.best0531_Xception.h5
=>coat_0531_Xception_068.csv
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1297s 25ms/step - loss: 0.4624 - acc: 0.7851 - val_loss: 0.9136 - val_acc: 0.6812

Epoch 00001: val_loss improved from inf to 0.91356, saving model to models/coat4_augment.best0531_Xception.h5
Epoch 2/3
52032/52032 [==============================] - 1301s 25ms/step - loss: 0.4154 - acc: 0.8024 - val_loss: 0.9457 - val_acc: 0.6757

Epoch 00002: val_loss did not improve from 0.91356
Epoch 3/3
23808/52032 [============>.................] - ETA: 11:33 - loss: 0.3814 - acc: 0.8181KeyboardInterrupt

无tta
```

lr=0.0001 coat_augment.best0531_Inception.h5

```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1272s 24ms/step - loss: 0.7759 - acc: 0.6647 - val_loss: 0.8837 - val_acc: 0.6653

Epoch 00001: val_loss improved from inf to 0.88366, saving model to models/coat_augment.best0531_Inception.h5
Epoch 2/3
33536/52032 [==================>...........] - ETA: 7:11 - loss: 0.5596 - acc: 0.7470KeyboardInterrupt

```
lr=0.0001 coat4_augment.best0531_Inception.h5
=>coat_0531_Inception_tta_068.csv
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1188s 23ms/step - loss: 0.5478 - acc: 0.7536 - val_loss: 0.9424 - val_acc: 0.6667

Epoch 00001: val_loss improved from inf to 0.94242, saving model to models/coat4_augment.best0531_Inception.h5
Epoch 2/3
 7360/52032 [===>..........................] - ETA: 16:38 - loss: 0.5080 - acc: 0.7686KeyboardInterrupt

 tta:0.6811 四个结果tta

```


2018.5.30

- 训练类型：coat  
- 前提条件：coat_augment.best0525_Inception.h5
- coat_augment.best0527_Xception.h5
- 训练参数和模型 第二轮使用 padding rotation30 rotation-30 flip+his
- 环境：google GPU
Woody0530_coat_augment_Inception
Woody0530_coat_augment_xception

xception/inception
```
#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,1,8,2]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)


valid_np=(valid_np_padding+valid_np_filp+valid_np_contrast)/3
```
lr=0.0001 coat2_augment.best0530_Xception.h5
=>coat_0530_Xception_067_tta.csv
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1298s 25ms/step - loss: 0.7179 - acc: 0.7113 - val_loss: 0.8865 - val_acc: 0.6653

Epoch 00001: val_loss improved from inf to 0.88654, saving model to models/coat2_augment.best0530_Xception.h5
Epoch 2/3
52032/52032 [==============================] - 1303s 25ms/step - loss: 0.4331 - acc: 0.8254 - val_loss: 1.0728 - val_acc: 0.6604

Epoch 00002: val_loss did not improve from 0.88654
Epoch 3/3
 1280/52032 [..............................] - ETA: 21:00 - loss: 0.2495 - acc: 0.9047KeyboardInterrupt

tta 0.6721991701244814
```
lr=0.0001 coat2_augment.best0530_Inception.h5
=>coat__0530_Inception_066_tta.csv
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1266s 24ms/step - loss: 0.7429 - acc: 0.7069 - val_loss: 0.9543 - val_acc: 0.6611

Epoch 00001: val_loss improved from inf to 0.95428, saving model to models/coat2_augment.best0530_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1210s 23ms/step - loss: 0.5038 - acc: 0.7997 - val_loss: 1.0842 - val_acc: 0.6549

Epoch 00002: val_loss did not improve from 0.95428
Epoch 3/3
  832/52032 [..............................] - ETA: 19:37 - loss: 0.3079 - acc: 0.8858KeyboardInterrupt

tta 0.661134163208852

```


2018.5.29
- 训练类型：coat  
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 padding + flip + contrast + flip_his  multi inputs
```
#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style_1=[0,8]
n_times_1=len(trans_style_1)
X_train_1 = np.zeros((n*n_times_1, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times_1, n_class), dtype=np.uint8)

trans_style_2=[6,5]
n_times_2=len(trans_style_2)
X_train_2 = np.zeros((n*n_times_2, width, width, 3), dtype=np.uint8)

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    #human=int(df_load['human'][i])
    for j in range(len(trans_style_1)):
        X_train_1[i*n_times_1+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style_1[j]), (width, width))
        X_train_2[i*n_times_2+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style_2[j]), (width, width))
        y_train[i*n_times_1+j][tmp_label.find('y')] = 1

df_valid.reset_index(inplace=True)
del df_valid['index']
n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)

for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    img=cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_valid[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')


cnn_model_2 = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')


inputs_1 = Input((width, width, 3))
x1 = inputs_1
x1 = Lambda(preprocess_input, name='preprocessing1')(x1)
x1 = cnn_model(x1)
#x1 = GlobalAveragePooling2D()(x1)
#x1 = Dropout(0.5)(x1) 
#x1 = Dense(n_class, activation='softmax', name='softmax1')(x1)



inputs_2 = Input((width, width, 3))
x2 = inputs_2
x2 = Lambda(preprocess_input, name='preprocessing2')(x2)
x2 = cnn_model_2(x2)
x2 = Conv2D(2560, (3, 3))(x2)
#x2 = GlobalAveragePooling2D()(x2)
#x2 = Dropout(0.5)(x2) 
#x2 = Dense(n_class, activation='softmax', name='softmax2')(x2)
#model = Model(inputs, x)

x3=Concatenate()([x1,x2])
x3=GlobalAveragePooling2D()(x3)
x3=Dense(512, activation='relu', name='dense_1')(x3)
x3=Dropout(0.5)(x3)

x3 = Dense(n_class, activation='softmax', name='softmax')(x3)

#x_all=Average()([x1,x2])#1、在哪一层结合，2、用什么算法结合，3、以及input输入部分都可以尝试

model = Model(inputs=[inputs_1,inputs_2],outputs=x3)

 
adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 3
batch_size = 16

save_path='models/{0}_multi_inputs_augment.best0529_Inception.h5'.format(prefix_cls)
load_path='models/{0}_multi_inputs_augment.best0529_Inception.h5'.format(prefix_cls)

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True,save_weights_only=True)

h = model.fit([X_train_1,X_train_2], y_train, batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=([X_valid,X_valid_filp], y_valid))


```
lr=0.001
```
Train on 26016 samples, validate on 1446 samples
Epoch 1/3
26016/26016 [==============================] - 1557s 60ms/step - loss: 1.5664 - acc: 0.3665 - val_loss: 1.3637 - val_acc: 0.4613

Epoch 00001: val_loss improved from inf to 1.36366, saving model to models/coat_multi_inputs_augment.best0529_Inception.h5
Epoch 2/3
26016/26016 [==============================] - 1531s 59ms/step - loss: 1.3291 - acc: 0.4726 - val_loss: 1.1750 - val_acc: 0.5152

Epoch 00002: val_loss improved from 1.36366 to 1.17496, saving model to models/coat_multi_inputs_augment.best0529_Inception.h5
Epoch 3/3
26016/26016 [==============================] - 1531s 59ms/step - loss: 1.1774 - acc: 0.5316 - val_loss: 1.0986 - val_acc: 0.5602

Epoch 00003: val_loss improved from 1.17496 to 1.09857, saving model to models/coat_multi_inputs_augment.best0529_Inception.h5

Train on 26016 samples, validate on 1446 samples
Epoch 1/3
26016/26016 [==============================] - 1534s 59ms/step - loss: 1.0323 - acc: 0.5828 - val_loss: 1.0492 - val_acc: 0.5698

Epoch 00001: val_loss improved from inf to 1.04921, saving model to models/coat2_multi_inputs_augment.best0529_Inception.h5
Epoch 2/3
26016/26016 [==============================] - 1461s 56ms/step - loss: 0.9227 - acc: 0.6278 - val_loss: 1.0699 - val_acc: 0.5685

```
lr=0.0001
```
Train on 26016 samples, validate on 1446 samples
Epoch 1/3
26016/26016 [==============================] - 1553s 60ms/step - loss: 0.7683 - acc: 0.6904 - val_loss: 0.9678 - val_acc: 0.6210

Epoch 00001: val_loss improved from inf to 0.96782, saving model to models/coat3_multi_inputs_augment.best0529_Inception.h5
Epoch 2/3
26016/26016 [==============================] - 1483s 57ms/step - loss: 0.6457 - acc: 0.7369 - val_loss: 1.0395 - val_acc: 0.6113

```
- 训练类型：coat  
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 padding + flip + contrast + flip_his  multi outputshis
Woody0528_IceptionV2_coat_augment_muti_output
效果不明显
```
trans_style=[0,6,8,5]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train_main = np.zeros((n*n_times, n_class), dtype=np.uint8)
y_train_auxi = np.zeros((n*n_times, 1), dtype=np.uint8)


for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    human=int(df_load['human'][i])
    for j in range(len(trans_style)):
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train_main[i*n_times+j][tmp_label.find('y')] = 1
        y_train_auxi[i*n_times+j][0] = human


df_valid.reset_index(inplace=True)
del df_valid['index']
n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid_main = np.zeros((n_valid, n_class), dtype=np.uint8)
y_valid_auxi = np.zeros((n_valid, 1), dtype=np.uint8)

for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    img=cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_valid[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    y_valid_main[i][tmp_label.find('y')] = 1
    y_valid_auxi[i][0]=int(df_valid['human'][i])


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))
x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
output_main = Dense(n_class, activation='softmax', name='softmax')(x)
output_auxi = Dense(1, activation='sigmoid', name='sigmoid')(x)

model = Model(inputs, [output_main,output_auxi])

adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

model.compile(optimizer=adam,
              loss={'softmax': 'categorical_crossentropy', 'sigmoid': 'binary_crossentropy'},
              loss_weights={'softmax': 1., 'sigmoid': 0.2},
              metrics=['accuracy'])

epochs = 3
batch_size = 32

save_path='models/{0}_multi_outputs_augment.best0529_Inception.h5'.format(prefix_cls)
load_path='models/{0}_multi_outputs_augment.best0529_Inception.h5'.format(prefix_cls)

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True,save_weights_only=True)

h = model.fit(X_train, [y_train_main,y_train_auxi], batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, [y_valid_main,y_valid_auxi]))

```
lr=0.001 coat_multi_outputs_augment.best0529_Inception.h5

```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1282s 25ms/step - loss: 1.1542 - softmax_loss: 1.1371 - sigmoid_loss: 0.0857 - softmax_acc: 0.5463 - sigmoid_acc: 0.9745 - val_loss: 1.0429 - val_softmax_loss: 1.0320 - val_sigmoid_loss: 0.0548 - val_softmax_acc: 0.5961 - val_sigmoid_acc: 0.9855

Epoch 00001: val_loss improved from inf to 1.04294, saving model to models/coat_multi_outputs_augment.best0529_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1220s 23ms/step - loss: 0.7964 - softmax_loss: 0.7871 - sigmoid_loss: 0.0464 - softmax_acc: 0.6844 - sigmoid_acc: 0.9877 - val_loss: 1.1667 - val_softmax_loss: 1.1297 - val_sigmoid_loss: 0.1849 - val_softmax_acc: 0.5982 - val_sigmoid_acc: 0.9419

Epoch 00002: val_loss did not improve from 1.04294
Epoch 3/3
 2368/52032 [>.............................] - ETA: 19:12 - loss: 0.5947 - softmax_loss: 0.5888 - sigmoid_loss: 0.0295 - softmax_acc: 0.7711 - sigmoid_acc: 0.9937

```
lr=0.0001 coat_multi_outputs_augment.best0529_Inception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1255s 24ms/step - loss: 0.5514 - softmax_loss: 0.5450 - sigmoid_loss: 0.0324 - softmax_acc: 0.7788 - sigmoid_acc: 0.9922 - val_loss: 1.0471 - val_softmax_loss: 1.0350 - val_sigmoid_loss: 0.0604 - val_softmax_acc: 0.6549 - val_sigmoid_acc: 0.9855

Epoch 00001: val_loss improved from inf to 1.04707, saving model to models/coat2_multi_outputs_augment.best0529_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1200s 23ms/step - loss: 0.2480 - softmax_loss: 0.2422 - sigmoid_loss: 0.0293 - softmax_acc: 0.9035 - sigmoid_acc: 0.9931 - val_loss: 1.3939 - val_softmax_loss: 1.3825 - val_sigmoid_loss: 0.0566 - val_softmax_acc: 0.6245 - val_sigmoid_acc: 0.9896

Epoch 00002: val_loss did not improve from 1.04707
Epoch 3/3
52032/52032 [==============================] - 1201s 23ms/step - loss: 0.1139 - softmax_loss: 0.1093 - sigmoid_loss: 0.0229 - softmax_acc: 0.9588 - sigmoid_acc: 0.9941 - val_loss: 1.7274 - val_softmax_loss: 1.7154 - val_sigmoid_loss: 0.0597 - val_softmax_acc: 0.6349 - val_sigmoid_acc: 0.9876


```

- 训练类型：neck  
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：clip  Xception+inception_resnet_v2 multi_input
Woody0529_transfer_learning_neck_clipnew_multi_input

```
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')


cnn_model_2 = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')


inputs_1 = Input((width, width, 3))
x1 = inputs_1
x1 = Lambda(preprocess_input, name='preprocessing1')(x1)
x1 = cnn_model(x1)
x1 = GlobalAveragePooling2D()(x1)
x1 = Dropout(0.5)(x1) 
x1 = Dense(n_class, activation='softmax', name='softmax1')(x1)



inputs_2 = Input((width, width, 3))
x2 = inputs_2
x2 = Lambda(preprocess_input, name='preprocessing2')(x2)
x2 = cnn_model_2(x2)
x2 = GlobalAveragePooling2D()(x2)
x2 = Dropout(0.5)(x2) 
x2 = Dense(n_class, activation='softmax', name='softmax2')(x2)
#model = Model(inputs, x)

x_all=Average()([x1,x2])#1、在哪一层结合，2、用什么算法结合，3、以及input输入部分都可以尝试

model = Model(inputs=[inputs_1,inputs_2],outputs=x_all)


datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)

def generator_two_img(X1, X2, y, batch_size):
    genX1 = datagen.flow(X1, y,  batch_size=batch_size, seed=1)
    genX2 = datagen.flow(X2, y, batch_size=batch_size, seed=1)
    while True:
        X1i = genX1.next()
        X2i = genX2.next()
        yield [X1i[0], X2i[0]], X1i[1]


inputgenerator=generator_two_img(X_train, X_train, y_train, batch_size)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_muti_inputs_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(inputgenerator,
                                  epochs=epochs, validation_data=([X_valid,X_valid],y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```


lr=0.001 neck_clip_muti_inputs_all_mem_weights.h5
```
Epoch 1/32
458/458 [==============================] - 497s 1s/step - loss: 1.2613 - acc: 0.4967 - val_loss: 1.2907 - val_acc: 0.5343

Epoch 00001: val_loss improved from inf to 1.29073, saving model to models/neck_clip_muti_inputs_all_mem_weights.h5
Epoch 2/32
458/458 [==============================] - 413s 903ms/step - loss: 1.0866 - acc: 0.6064 - val_loss: 1.5585 - val_acc: 0.5453

Epoch 00002: val_loss did not improve from 1.29073
Epoch 3/32
458/458 [==============================] - 413s 901ms/step - loss: 1.0184 - acc: 0.6562 - val_loss: 1.0870 - val_acc: 0.6593

Epoch 00003: val_loss improved from 1.29073 to 1.08696, saving model to models/neck_clip_muti_inputs_all_mem_weights.h5
Epoch 4/32
458/458 [==============================] - 412s 900ms/step - loss: 1.0204 - acc: 0.6628 - val_loss: 1.9036 - val_acc: 0.4228

Epoch 00004: val_loss did not improve from 1.08696
Epoch 5/32
458/458 [==============================] - 412s 900ms/step - loss: 0.9645 - acc: 0.7010 - val_loss: 1.1997 - val_acc: 0.4044

Epoch 00005: val_loss did not improve from 1.08696
Epoch 6/32
458/458 [==============================] - 413s 902ms/step - loss: 0.9631 - acc: 0.7022 - val_loss: 0.9749 - val_acc: 0.6998

Epoch 00006: val_loss improved from 1.08696 to 0.97485, saving model to models/neck_clip_muti_inputs_all_mem_weights.h5
Epoch 7/32
458/458 [==============================] - 413s 901ms/step - loss: 0.9165 - acc: 0.7330 - val_loss: 0.9717 - val_acc: 0.6998

Epoch 00007: val_loss improved from 0.97485 to 0.97174, saving model to models/neck_clip_muti_inputs_all_mem_weights.h5
Epoch 8/32
458/458 [==============================] - 412s 900ms/step - loss: 0.9010 - acc: 0.7437 - val_loss: 0.9560 - val_acc: 0.7145

Epoch 00008: val_loss improved from 0.97174 to 0.95596, saving model to models/neck_clip_muti_inputs_all_mem_weights.h5
Epoch 9/32
458/458 [==============================] - 412s 901ms/step - loss: 0.8792 - acc: 0.7546 - val_loss: 0.9936 - val_acc: 0.7157

Epoch 00009: val_loss did not improve from 0.95596
Epoch 10/32
131/458 [=======>......................] - ETA: 4:43 - loss: 0.8710 - acc: 0.7553KeyboardInterrupt
CPU times: user 1h 58min 54s, sys: 11min 13s, total: 2h 10min 7s
Wall time: 1h 5min 45s

```
lr=0.00001 neck2_clip_muti_inputs_all_mem_weights.h5
```
Epoch 1/32
458/458 [==============================] - 499s 1s/step - loss: 0.7868 - acc: 0.8122 - val_loss: 0.7929 - val_acc: 0.8235

Epoch 00001: val_loss improved from inf to 0.79286, saving model to models/neck2_clip_muti_inputs_all_mem_weights.h5
Epoch 2/32
458/458 [==============================] - 414s 904ms/step - loss: 0.7401 - acc: 0.8364 - val_loss: 0.7846 - val_acc: 0.8333

Epoch 00002: val_loss improved from 0.79286 to 0.78464, saving model to models/neck2_clip_muti_inputs_all_mem_weights.h5
Epoch 3/32
458/458 [==============================] - 413s 903ms/step - loss: 0.7251 - acc: 0.8430 - val_loss: 0.7714 - val_acc: 0.8125

Epoch 00003: val_loss improved from 0.78464 to 0.77137, saving model to models/neck2_clip_muti_inputs_all_mem_weights.h5
Epoch 4/32
458/458 [==============================] - 413s 902ms/step - loss: 0.6952 - acc: 0.8596 - val_loss: 0.7957 - val_acc: 0.8223

Epoch 00004: val_loss did not improve from 0.77137
Epoch 5/32
458/458 [==============================] - 413s 902ms/step - loss: 0.6840 - acc: 0.8666 - val_loss: 0.7889 - val_acc: 0.8174

Epoch 00005: val_loss did not improve from 0.77137
Epoch 6/32
458/458 [==============================] - 413s 901ms/step - loss: 0.6589 - acc: 0.8780 - val_loss: 0.7913 - val_acc: 0.8297

Epoch 00006: val_loss did not improve from 0.77137
Epoch 7/32
124/458 [=======>......................] - ETA: 4:51 - loss: 0.6609 - acc: 0.8785KeyboardInterrupt
CPU times: user 1h 21min 29s, sys: 7min 22s, total: 1h 28min 51s
Wall time: 45min 13s
```

2018.5.28

- 训练类型：coat  
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 padding + flip + contrast + flip_his + multilabel
- Woody0528_IceptionV2_coat_augment_mutilabel
```
#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,6,8,5]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    human=int(df_load['human'][i])
    for j in range(len(trans_style)):
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1
        y_train[i*n_times+j][-1] = human

df_valid.reset_index(inplace=True)
del df_valid['index']
n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class+1), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    img=cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_valid[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    y_valid[i][tmp_label.find('y')] = 1
    y_valid[i][-1]=int(df_valid['human'][i])


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))
x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class+1, activation='sigmoid', name='sigmoid')(x)

model = Model(inputs, x)

adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

epochs = 3
batch_size = 32

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True)

h = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, y_valid))                         

```
lr=0.001 coat_multi_augment.best0528_Inception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1311s 25ms/step - loss: 0.2151 - acc: 0.9095 - val_loss: 0.2206 - val_acc: 0.9090

Epoch 00001: val_loss improved from inf to 0.22056, saving model to models/coat_multi_augment.best0528_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1239s 24ms/step - loss: 0.1551 - acc: 0.9335 - val_loss: 0.2030 - val_acc: 0.9163

Epoch 00002: val_loss improved from 0.22056 to 0.20296, saving model to models/coat_multi_augment.best0528_Inception.h5
Epoch 3/3
 4192/52032 [=>............................] - ETA: 18:55 - loss: 0.1170 - acc: 0.9500

```
lr=0.0001  coat2_multi_augment.best0528_Inception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1260s 24ms/step - loss: 0.0694 - acc: 0.9712 - val_loss: 0.2395 - val_acc: 0.9222

Epoch 00001: val_loss improved from inf to 0.23945, saving model to models/coat2_multi_augment.best0528_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1203s 23ms/step - loss: 0.0305 - acc: 0.9879 - val_loss: 0.3124 - val_acc: 0.9203

Epoch 00002: val_loss did not improve from 0.23945
Epoch 3/3
 1088/52032 [..............................] - ETA: 19:28 - loss: 0.0139 - acc: 0.9951KeyboardInterrupt
```

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPUmu
- keyWords ：InceptionResNetV2  多标签 muti_label
- 
- 模型文件 ：

```
df_train = pd.read_csv('data-raw/train_2/Annotations/label_5.csv')
#df_train.columns = ['image_id', 'class', 'label']
df_train.head()

classes = ['collar_design_labels', 'neckline_design_labels', 'skirt_length_labels',
          'sleeve_length_labels', 'neck_design_labels', 'coat_length_labels', 'lapel_design_labels',
          'pant_length_labels']

cur_class = classes[5]
df_load = df_train[(df_train['class'] == cur_class)].copy()
df_load.reset_index(inplace=True)
del df_load['index']

print('{0}: {1}'.format(cur_class, len(df_load)))
df_load.head()

n = len(df_load)
n_class = len(df_load['label'][0])
width = 299 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
y = np.zeros((n, n_class+1), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y[i][tmp_label.find('y')] = 1
    y[i][-1]=int(df_load['human'][i])


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class+1, activation='sigmoid', name='sigmoid')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_models/{0}_mult_0528_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')



```
lr=0.001 coat_mult_0528_label.h5
```
Epoch 1/32
406/406 [==============================] - 377s 929ms/step - loss: 0.2670 - acc: 0.8906 - val_loss: 0.2893 - val_acc: 0.8859

Epoch 00001: val_loss improved from inf to 0.28931, saving model to models/coat_mult_0528_label.h5
Epoch 2/32
406/406 [==============================] - 307s 756ms/step - loss: 0.2308 - acc: 0.9016 - val_loss: 0.2268 - val_acc: 0.9033

Epoch 00002: val_loss improved from 0.28931 to 0.22680, saving model to models/coat_mult_0528_label.h5
Epoch 3/32
406/406 [==============================] - 307s 757ms/step - loss: 0.2149 - acc: 0.9069 - val_loss: 0.2107 - val_acc: 0.9095

Epoch 00003: val_loss improved from 0.22680 to 0.21075, saving model to models/coat_mult_0528_label.h5
Epoch 4/32
406/406 [==============================] - 307s 756ms/step - loss: 0.2025 - acc: 0.9114 - val_loss: 0.2209 - val_acc: 0.9053

Epoch 00004: val_loss did not improve from 0.21075
Epoch 5/32
406/406 [==============================] - 307s 757ms/step - loss: 0.1920 - acc: 0.9157 - val_loss: 0.2100 - val_acc: 0.9106

Epoch 00005: val_loss improved from 0.21075 to 0.21001, saving model to models/coat_mult_0528_label.h5
Epoch 6/32
406/406 [==============================] - 307s 756ms/step - loss: 0.1829 - acc: 0.9190 - val_loss: 0.1936 - val_acc: 0.9185

Epoch 00006: val_loss improved from 0.21001 to 0.19362, saving model to models/coat_mult_0528_label.h5
Epoch 7/32
406/406 [==============================] - 308s 759ms/step - loss: 0.1735 - acc: 0.9240 - val_loss: 0.1939 - val_acc: 0.9169

Epoch 00007: val_loss did not improve from 0.19362
Epoch 8/32
406/406 [==============================] - 310s 763ms/step - loss: 0.1649 - acc: 0.9269 - val_loss: 0.2308 - val_acc: 0.9056

Epoch 00008: val_loss did not improve from 0.19362
Epoch 9/32
406/406 [==============================] - 308s 758ms/step - loss: 0.1568 - acc: 0.9312 - val_loss: 0.1839 - val_acc: 0.9252

Epoch 00009: val_loss improved from 0.19362 to 0.18387, saving model to models/coat_mult_0528_label.h5
Epoch 10/32
406/406 [==============================] - 307s 756ms/step - loss: 0.1519 - acc: 0.9333 - val_loss: 0.1961 - val_acc: 0.9153

Epoch 00010: val_loss did not improve from 0.18387
Epoch 11/32
406/406 [==============================] - 307s 756ms/step - loss: 0.1424 - acc: 0.9381 - val_loss: 0.2098 - val_acc: 0.9117

Epoch 00011: val_loss did not improve from 0.18387
Epoch 12/32
105/406 [======>.......................] - ETA: 3:38 - loss: 0.1254 - acc: 0.9462KeyboardInterrupt
CPU times: user 1h 54min 55s, sys: 10min 36s, total: 2h 5min 32s
Wall time: 59min 24s

```

lr=0.0001 coat2_mult_0528_label.h5
```
Epoch 1/32
406/406 [==============================] - 363s 894ms/step - loss: 0.1182 - acc: 0.9490 - val_loss: 0.1863 - val_acc: 0.9261

Epoch 00001: val_loss improved from inf to 0.18635, saving model to models/coat2_mult_0528_label.h5
Epoch 2/32
406/406 [==============================] - 306s 753ms/step - loss: 0.1039 - acc: 0.9552 - val_loss: 0.1945 - val_acc: 0.9269

Epoch 00002: val_loss did not improve from 0.18635
Epoch 3/32
406/406 [==============================] - 305s 752ms/step - loss: 0.0941 - acc: 0.9601 - val_loss: 0.1998 - val_acc: 0.9265

Epoch 00003: val_loss did not improve from 0.18635
Epoch 4/32
406/406 [==============================] - 305s 752ms/step - loss: 0.0830 - acc: 0.9643 - val_loss: 0.2139 - val_acc: 0.9246

Epoch 00004: val_loss did not improve from 0.18635
Epoch 5/32
139/406 [=========>....................] - ETA: 3:12 - loss: 0.0764 - acc: 0.9665KeyboardInterrupt
CPU times: user 45min 12s, sys: 3min 57s, total: 49min 9s
Wall time: 23min 34s

```
2018.5.27

- 训练类型：neck  
- 前提条件：
- 训练参数和模型 xcetption
- 环境：google GPU
- keyWords ：InceptionResNetV2 399*399
Woody0527_transfer_learning_neck_clipnew_xception_399
lr=0.001 neck_clip_399_xception_all_mem_weights.h5 效果比较差
```
Epoch 1/32
458/458 [==============================] - 339s 741ms/step - loss: 1.1156 - acc: 0.5614 - val_loss: 1.0748 - val_acc: 0.6360

Epoch 00001: val_loss improved from inf to 1.07475, saving model to models/neck_clip_399_xception_all_mem_weights.h5
Epoch 2/32
458/458 [==============================] - 328s 716ms/step - loss: 0.8236 - acc: 0.6981 - val_loss: 0.9093 - val_acc: 0.6777

Epoch 00002: val_loss improved from 1.07475 to 0.90932, saving model to models/neck_clip_399_xception_all_mem_weights.h5
Epoch 3/32
458/458 [==============================] - 328s 716ms/step - loss: 0.7040 - acc: 0.7447 - val_loss: 0.7108 - val_acc: 0.7328

Epoch 00003: val_loss improved from 0.90932 to 0.71078, saving model to models/neck_clip_399_xception_all_mem_weights.h5
Epoch 4/32
458/458 [==============================] - 327s 714ms/step - loss: 0.6268 - acc: 0.7761 - val_loss: 1.1088 - val_acc: 0.6605

Epoch 00004: val_loss did not improve from 0.71078
Epoch 5/32
458/458 [==============================] - 327s 714ms/step - loss: 0.5629 - acc: 0.7984 - val_loss: 0.6905 - val_acc: 0.7672

Epoch 00005: val_loss improved from 0.71078 to 0.69052, saving model to models/neck_clip_399_xception_all_mem_weights.h5
Epoch 6/32
458/458 [==============================] - 328s 715ms/step - loss: 0.5341 - acc: 0.8123 - val_loss: 0.6218 - val_acc: 0.7806

Epoch 00006: val_loss improved from 0.69052 to 0.62184, saving model to models/neck_clip_399_xception_all_mem_weights.h5
Epoch 7/32
458/458 [==============================] - 327s 714ms/step - loss: 0.4786 - acc: 0.8268 - val_loss: 0.6048 - val_acc: 0.7843

Epoch 00007: val_loss improved from 0.62184 to 0.60478, saving model to models/neck_clip_399_xception_all_mem_weights.h5
Epoch 8/32
458/458 [==============================] - 327s 715ms/step - loss: 0.4340 - acc: 0.8475 - val_loss: 0.6961 - val_acc: 0.7782

Epoch 00008: val_loss did not improve from 0.60478
Epoch 9/32
458/458 [==============================] - 327s 714ms/step - loss: 0.4155 - acc: 0.8544 - val_loss: 0.6926 - val_acc: 0.7733

Epoch 00009: val_loss did not improve from 0.60478
Epoch 10/32
 32/458 [=>............................] - ETA: 4:56 - loss: 0.4799 - acc: 0.8223KeyboardInterrupt

```
lr=0.0001 neck2_clip_399_xception_all_mem_weights.h5
```
Epoch 1/32
458/458 [==============================] - 338s 739ms/step - loss: 0.3012 - acc: 0.8915 - val_loss: 0.4653 - val_acc: 0.8309

Epoch 00001: val_loss improved from inf to 0.46534, saving model to models/neck2_clip_399_xception_all_mem_weights.h5
Epoch 2/32
458/458 [==============================] - 327s 714ms/step - loss: 0.2300 - acc: 0.9196 - val_loss: 0.4850 - val_acc: 0.8395

Epoch 00002: val_loss did not improve from 0.46534
Epoch 3/32
458/458 [==============================] - 327s 715ms/step - loss: 0.1930 - acc: 0.9341 - val_loss: 0.4950 - val_acc: 0.8382

Epoch 00003: val_loss did not improve from 0.46534
Epoch 4/32
458/458 [==============================] - 326s 712ms/step - loss: 0.1685 - acc: 0.9441 - val_loss: 0.5475 - val_acc: 0.8358

Epoch 00004: val_loss did not improve from 0.46534
Epoch 5/32
458/458 [==============================] - 326s 712ms/step - loss: 0.1439 - acc: 0.9498 - val_loss: 0.5621 - val_acc: 0.8248

Epoch 00005: val_loss did not improve from 0.46534
Epoch 6/32
398/458 [=========================>....] - ETA: 41s - loss: 0.1251 - acc: 0.9603KeyboardInterrupt
CPU times: user 48min 17s, sys: 8min 3s, total: 56min 20s
Wall time: 32min 14s

```

- 训练类型：coat  
- 前提条件：
- 训练参数和模型 xcetption
- 环境：google GPU
- keyWords ：InceptionResNetV2 padding + contrast + flip_his
399*399图片
lr=0.001 coat_augment_399.best0527_Xception.h5 效果比较差
```
Train on 39024 samples, validate on 1446 samples
Epoch 1/3
39024/39024 [==============================] - 1699s 44ms/step - loss: 1.2505 - acc: 0.5049 - val_loss: 1.1788 - val_acc: 0.5380

Epoch 00001: val_loss improved from inf to 1.17878, saving model to models/coat_augment_399.best0527_Xception.h5
Epoch 2/3
39024/39024 [==============================] - 1746s 45ms/step - loss: 0.9061 - acc: 0.6398 - val_loss: 1.1541 - val_acc: 0.5380

Epoch 00002: val_loss improved from 1.17878 to 1.15411, saving model to models/coat_augment_399.best0527_Xception.h5
Epoch 3/3
 2448/39024 [>.............................] - ETA: 26:58 - loss: 0.7231 - acc: 0.7030

```
lr=0.0001 coat2_augment_399.best0527_Xception.h5
```
Train on 39024 samples, validate on 1446 samples
Epoch 1/3
39024/39024 [==============================] - 1710s 44ms/step - loss: 0.4688 - acc: 0.8102 - val_loss: 1.0383 - val_acc: 0.6487

Epoch 00001: val_loss improved from inf to 1.03833, saving model to models/coat2_augment_399.best0527_Xception.h5
Epoch 2/3
39024/39024 [==============================] - 1694s 43ms/step - loss: 0.2506 - acc: 0.9010 - val_loss: 1.2389 - val_acc: 0.6556

Epoch 00002: val_loss did not improve from 1.03833
Epoch 3/3
 3920/39024 [==>...........................] - ETA: 25:08 - loss: 0.1367 - acc: 0.9482KeyboardInterrupt

```

- 训练类型：coat  
- 前提条件：
- 训练参数和模型 xcetption
- 环境：google GPU
- keyWords ：InceptionResNetV2 padding + flip + contrast + flip_his
Woody0527_coat_augment_xception
=>coat_0527_Xception_066_tta.csv
```
#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,6,8,5]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    for j in range(len(trans_style)):
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1

df_valid.reset_index(inplace=True)
del df_valid['index']
n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    img=cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_valid[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    y_valid[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))
x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

epochs = 3
batch_size = 32

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True)

h = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, y_valid))                         

```
lr=0.001 coat_augment.best0527_Xception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1295s 25ms/step - loss: 1.0346 - acc: 0.5913 - val_loss: 1.0781 - val_acc: 0.5913

Epoch 00001: val_loss improved from inf to 1.07813, saving model to models/coat_augment.best0527_Xception.h5
Epoch 2/3
52032/52032 [==============================] - 1288s 25ms/step - loss: 0.6498 - acc: 0.7396 - val_loss: 1.1648 - val_acc: 0.6030

Epoch 00002: val_loss did not improve from 1.07813
Epoch 3/3
 4192/52032 [=>............................] - ETA: 19:40 - loss: 0.4472 - acc: 0.8206

```

lr=0.0001 coat2_augment.best0527_Xception.h5
=>coat_0527_Xception_066_tta.csv
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1298s 25ms/step - loss: 0.4128 - acc: 0.8331 - val_loss: 1.0838 - val_acc: 0.6556

Epoch 00001: val_loss improved from inf to 1.08377, saving model to models/coat2_augment.best0527_Xception.h5
Epoch 2/3
52032/52032 [==============================] - 1285s 25ms/step - loss: 0.1584 - acc: 0.9345 - val_loss: 1.4973 - val_acc: 0.6494

Epoch 00002: val_loss did not improve from 1.08377
Epoch 3/3
 2784/52032 [>.............................] - ETA: 20:05 - loss: 0.0895 - acc: 0.9648KeyboardInterrupt
```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
Woody0527_transfer_learning_neck_clipnew_xception.ipynb
=>neck_0527_84_xception_clip_w_h.csv
```
crip_high=int(width*0.5)
crip_width_start=int(width*0.25)
crip_width_end=int(width*0.75)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])), (width, width))
    X[i] = cv2.resize(X[i][:crip_high], (width, width))
    X[i] = cv2.resize(X[i][:][:,crip_width_start:crip_width_end], (width, width))
    y[i][tmp_label.find('y')] = 1

cnn_model =Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

x_train_path='data-raw/train_2/{}X_train_299_clip_w_h.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_299_clip_w_h.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_299_clip_w_h.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_299_clip_w_h.bc'.format(prefix_cls)

save_array(x_train_path,X_train)
save_array(x_valid_path,X_valid)
save_array(y_train_path,y_train)
save_array(y_valid_path,y_valid)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_xception_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 neck_clip_w_h_resnet_all_mem_weights.h5
```
Epoch 1/32
229/229 [==============================] - 194s 848ms/step - loss: 1.0509 - acc: 0.5805 - val_loss: 0.9429 - val_acc: 0.6850

Epoch 00001: val_loss improved from inf to 0.94294, saving model to models/neck_clip_w_h_xception_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 181s 789ms/step - loss: 0.7430 - acc: 0.7184 - val_loss: 0.9189 - val_acc: 0.6814

Epoch 00002: val_loss improved from 0.94294 to 0.91886, saving model to models/neck_clip_w_h_xception_all_mem_weights.h5
Epoch 3/32
229/229 [==============================] - 181s 789ms/step - loss: 0.6124 - acc: 0.7760 - val_loss: 0.7549 - val_acc: 0.7279

Epoch 00003: val_loss improved from 0.91886 to 0.75491, saving model to models/neck_clip_w_h_xception_all_mem_weights.h5
Epoch 4/32
229/229 [==============================] - 180s 788ms/step - loss: 0.5526 - acc: 0.8008 - val_loss: 0.9175 - val_acc: 0.7365

Epoch 00004: val_loss did not improve from 0.75491
Epoch 5/32
229/229 [==============================] - 181s 789ms/step - loss: 0.4852 - acc: 0.8275 - val_loss: 1.3567 - val_acc: 0.6777

Epoch 00005: val_loss did not improve from 0.75491
Epoch 6/32
229/229 [==============================] - 181s 789ms/step - loss: 0.4354 - acc: 0.8456 - val_loss: 0.8137 - val_acc: 0.7328

Epoch 00006: val_loss did not improve from 0.75491
Epoch 7/32
229/229 [==============================] - 181s 788ms/step - loss: 0.3886 - acc: 0.8575 - val_loss: 0.7361 - val_acc: 0.7328

Epoch 00007: val_loss improved from 0.75491 to 0.73614, saving model to models/neck_clip_w_h_xception_all_mem_weights.h5
Epoch 8/32
229/229 [==============================] - 181s 791ms/step - loss: 0.3558 - acc: 0.8780 - val_loss: 0.7708 - val_acc: 0.7537

Epoch 00008: val_loss did not improve from 0.73614
Epoch 9/32
229/229 [==============================] - 180s 786ms/step - loss: 0.3356 - acc: 0.8809 - val_loss: 1.1195 - val_acc: 0.7365

Epoch 00009: val_loss did not improve from 0.73614
Epoch 10/32
229/229 [==============================] - 181s 789ms/step - loss: 0.2834 - acc: 0.9023 - val_loss: 0.6902 - val_acc: 0.7966

Epoch 00010: val_loss improved from 0.73614 to 0.69016, saving model to models/neck_clip_w_h_xception_all_mem_weights.h5
Epoch 11/32
229/229 [==============================] - 181s 789ms/step - loss: 0.2788 - acc: 0.8995 - val_loss: 0.6066 - val_acc: 0.8113

Epoch 00011: val_loss improved from 0.69016 to 0.60661, saving model to models/neck_clip_w_h_xception_all_mem_weights.h5
Epoch 12/32
229/229 [==============================] - 181s 788ms/step - loss: 0.2549 - acc: 0.9106 - val_loss: 0.6305 - val_acc: 0.7953

Epoch 00012: val_loss did not improve from 0.60661
Epoch 13/32
 27/229 [==>...........................] - ETA: 2:34 - loss: 0.2506 - acc: 0.9016KeyboardInterrupt
CPU times: user 54min 56s, sys: 10min 16s, total: 1h 5min 12s
Wall time: 36min 54s

```

lr=0.0001 neck2_clip_w_h_xception_all_mem_weights.h5
=>neck_0527_84_xception_clip_w_h.csv
```
Epoch 1/32
229/229 [==============================] - 191s 836ms/step - loss: 0.1521 - acc: 0.9470 - val_loss: 0.5117 - val_acc: 0.8419

Epoch 00001: val_loss improved from inf to 0.51166, saving model to models/neck2_clip_w_h_xception_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 181s 791ms/step - loss: 0.0939 - acc: 0.9687 - val_loss: 0.5374 - val_acc: 0.8431

Epoch 00002: val_loss did not improve from 0.51166
Epoch 3/32
229/229 [==============================] - 181s 790ms/step - loss: 0.0634 - acc: 0.9791 - val_loss: 0.5987 - val_acc: 0.8370

Epoch 00003: val_loss did not improve from 0.51166
Epoch 4/32
106/229 [============>.................] - ETA: 1:34 - loss: 0.0568 - acc: 0.9802KeyboardInterrupt
CPU times: user 15min 59s, sys: 2min 57s, total: 18min 57s
Wall time: 10min 46s
```

lr=0.00001 neck3_clip_w_h_xception_all_mem_weights.h5
```
Epoch 1/32
229/229 [==============================] - 193s 842ms/step - loss: 0.0943 - acc: 0.9698 - val_loss: 0.5170 - val_acc: 0.8395

Epoch 00001: val_loss improved from inf to 0.51702, saving model to models/neck3_clip_w_h_xception_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 182s 796ms/step - loss: 0.0848 - acc: 0.9722 - val_loss: 0.5211 - val_acc: 0.8419

Epoch 00002: val_loss did not improve from 0.51702
Epoch 3/32
 27/229 [==>...........................] - ETA: 2:35 - loss: 0.0705 - acc: 0.9792KeyboardInterrupt
CPU times: user 9min 55s, sys: 1min 51s, total: 11min 47s

```



2018.5.25

- 训练类型：coat  
- 前提条件：coat2_augment.best0525_Inception.h5
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 pseudo sleeve数据集
- 文件名：Woody0525_IceptionV2_coat_augment-pseudo-back
利用上一轮提升了成绩的模型，再次预测sleeve数据集，形成新的label，再次加入训练集
回到之前模型，重新再次训练一遍
效果也不好，因为再次对sleeve进行预测，其实已经没有多少意义，因为sleeve的数据相当于
被见过了
lr=0.0001 coat5_augment.best0525_Inception.h5
```


```

- 训练类型：coat  
- 前提条件：coat3_augment.best0525_Inception.h5
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 pseudo sleeve数据集
- 文件名：Woody0525_IceptionV2_coat_augment-pseudo_circle
利用上一轮提升了成绩的模型，再次预测sleeve数据集，形成新的label，再次加入训练集
```
#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,5]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)

```
lr=0.0001 coat4_augment.best0525_Inception.h5
```
Train on 43301 samples, validate on 1446 samples
Epoch 1/3
43301/43301 [==============================] - 1076s 25ms/step - loss: 0.1401 - acc: 0.9473 - val_loss: 1.7232 - val_acc: 0.6653

Epoch 00001: val_loss improved from inf to 1.72316, saving model to models/coat4_augment.best0525_Inception.h5
Epoch 2/3
43301/43301 [==============================] - 1018s 24ms/step - loss: 0.0752 - acc: 0.9738 - val_loss: 1.9934 - val_acc: 0.6473

Epoch 00002: val_loss did not improve from 1.72316
```


- 训练类型：coat  
- 前提条件：coat2_augment.best0525_Inception.h5
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 pseudo sleeve数据集

- 文件名：Woody0525_IceptionV2_coat_augment-pseudo
```
cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))
x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
save_path='models/{0}2_augment.best0525_Inception.h5'.format(prefix_cls)

model.load_weights(save_path)

adam = Adam(lr=0.0001)
model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

cur_class_psdo = classes[3]
df_load_psdo = df_train[(df_train['class'] == cur_class_psdo)].copy()
df_load_psdo.reset_index(inplace=True)
del df_load_psdo['index']

print('{0}: {1}'.format(cur_class_psdo, len(df_load_psdo)))
df_load_psdo.head()

n_psdo=len(df_load_psdo)
X_psdo = np.zeros((n_psdo, width, width, 3), dtype=np.uint8)
y_psdo = np.zeros((n_psdo, n_class), dtype=np.uint8)
for i in tqdm(range(n_psdo)):
    img=cv2.imread('data-raw/train_2/{0}'.format(df_load_psdo['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_psdo[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    
y_psdo_pred=model.predict(X_psdo)

for i in range(len(y_psdo_pred)):
    y_psdo[i][y_psdo_pred[i].argmax()]=1

#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,8]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)


for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    for j in range(len(trans_style)):
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1

comb_pseudo_x = np.concatenate([X_train, X_psdo])
comb_pseudo_y = np.concatenate([y_train, y_psdo])       


save_path='models/{0}3_augment.best0525_Inception.h5'.format(prefix_cls)
load_path='models/{0}2_augment.best0525_Inception.h5'.format(prefix_cls)

adam = Adam(lr=0.0001)
model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True)
model.load_weights(load_path)

h = model.fit(comb_pseudo_x, comb_pseudo_y, batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, y_valid))              

```
lr=0.0001 coat3_augment.best0525_Inception.h5
```
Train on 43301 samples, validate on 1446 samples
Epoch 1/3
43301/43301 [==============================] - 1080s 25ms/step - loss: 0.2392 - acc: 0.9061 - val_loss: 1.4333 - val_acc: 0.6632

Epoch 00001: val_loss improved from inf to 1.43329, saving model to models/coat3_augment.best0525_Inception.h5
Epoch 2/3
43301/43301 [==============================] - 1024s 24ms/step - loss: 0.1360 - acc: 0.9487 - val_loss: 1.7818 - val_acc: 0.6342

```

- 训练类型：coat  
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 padding + flip + contrast + flip_his
```
#变换的样式
#{0:"299X299",1:"顺时针30度",2:"逆时针30度",3:"高斯噪声",4:"直方图",5:"对比拉伸",6:"水平翻转",7:"水平翻转+高斯噪声",8:"水平翻转+直方图"}
trans_style=[0,6,8,5]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    for j in range(len(trans_style)):
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1

df_valid.reset_index(inplace=True)
del df_valid['index']
n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    img=cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X_valid[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    y_valid[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))
x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

epochs = 3
batch_size = 32

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True)

h = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, y_valid))                         

```
lr=0.001 coat_augment.best0525_Inception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1301s 25ms/step - loss: 1.1789 - acc: 0.5312 - val_loss: 1.2793 - val_acc: 0.5104

Epoch 00001: val_loss improved from inf to 1.27928, saving model to models/coat_augment.best0525_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1232s 24ms/step - loss: 0.8105 - acc: 0.6788 - val_loss: 1.1055 - val_acc: 0.5844

Epoch 00002: val_loss improved from 1.27928 to 1.10553, saving model to models/coat_augment.best0525_Inception.h5


```

lr=0.0001 coat2_augment.best0525_Inception.h5
```
Train on 52032 samples, validate on 1446 samples
Epoch 1/3
52032/52032 [==============================] - 1239s 24ms/step - loss: 0.3396 - acc: 0.8649 - val_loss: 1.2272 - val_acc: 0.6535

Epoch 00001: val_loss improved from inf to 1.22721, saving model to models/coat2_augment.best0525_Inception.h5
Epoch 2/3
52032/52032 [==============================] - 1183s 23ms/step - loss: 0.1293 - acc: 0.9507 - val_loss: 1.6101 - val_acc: 0.6528

Epoch 00002: val_loss did not improve from 1.22721
Epoch 3/3
  480/52032 [..............................] - ETA: 19:22 - loss: 0.0538 - acc: 0.9812KeyboardInterrupt
```


- 训练类型：neck  没有效果
- 前提条件：neck2_alldata_clip_w_h_weights.h5
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd finetune
```
cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

model.load_weights('models/{0}2_alldata_clip_w_h_weights.h5'.format(prefix_cls))

for layer in model.layers[2].layers:
    layer.trainable=False
fine
x_train_path='data-raw/train_1/{}X_train_299_clip_w_h.bc'.format(prefix_cls)
x_valid_path='data-raw/train_1/{}X_valid_299_clip_w_h.bc'.format(prefix_cls)
y_train_path='data-raw/train_1/{}y_train_299_clip_w_h.bc'.format(prefix_cls)
y_valid_path='data-raw/train_1/{}y_vaild_299_clip_w_h.bc'.format(prefix_cls)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)


# Compile the model
adam = Adam(lr=1e-4)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}4_alldata_clip_finetue_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}2_alldata_clip_w_h_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')



```
lr=1e-4 neck4_alldata_clip_finetue_weights.h5 全部冻结
```
Epoch 1/32
407/407 [==============================] - 270s 664ms/step - loss: 0.2453 - acc: 0.9151 - val_loss: 0.4818 - val_acc: 0.8438

Epoch 00001: val_loss improved from inf to 0.48177, saving model to models/neck4_alldata_clip_finetue_weights.h5
Epoch 2/32
407/407 [==============================] - 247s 606ms/step - loss: 0.2511 - acc: 0.9120 - val_loss: 0.4836 - val_acc: 0.8462

Epoch 00002: val_loss did not improve from 0.48177
Epoch 3/32
407/407 [==============================] - 249s 612ms/step - loss: 0.2478 - acc: 0.9126 - val_loss: 0.4833 - val_acc: 0.8438

Epoch 00003: val_loss did not improve from 0.48177
Epoch 4/32
 20/407 [>.............................] - ETA: 3:51 - loss: 0.2144 - acc: 0.9125KeyboardInterrupt
CPU times: user 18min 59s, sys: 2min 25s, total: 21min 25s
Wall time: 13min 7s

```
lr=1e-4 neck4_alldata_clip_finetue_weights.h5 冻结595层之前
```
LAYERS_TO_FREEZE = 595

for layer in model.layers[2].layers[:LAYERS_TO_FREEZE]:
      layer.trainable = False
for layer in  model.layers[2].layers[LAYERS_TO_FREEZE:]:
      layer.trainable = True
```

```
Epoch 1/32
407/407 [==============================] - 301s 739ms/step - loss: 0.2505 - acc: 0.9132 - val_loss: 0.4834 - val_acc: 0.8400

Epoch 00001: val_loss improved from inf to 0.48342, saving model to models/neck4_alldata_clip_finetue_weights.h5
Epoch 2/32
407/407 [==============================] - 278s 683ms/step - loss: 0.2483 - acc: 0.9129 - val_loss: 0.4865 - val_acc: 0.8425

Epoch 00002: val_loss did not improve from 0.48342
Epoch 3/32
407/407 [==============================] - 277s 682ms/step - loss: 0.2437 - acc: 0.9137 - val_loss: 0.4923 - val_acc: 0.8375

Epoch 00003: val_loss did not improve from 0.48342
Epoch 4/32
407/407 [==============================] - 276s 679ms/step - loss: 0.2378 - acc: 0.9186 - val_loss: 0.5001 - val_acc: 0.8400

Epoch 00004: val_loss did not improve from 0.48342
Epoch 5/32
407/407 [==============================] - 277s 680ms/step - loss: 0.2358 - acc: 0.9174 - val_loss: 0.4980 - val_acc: 0.8413

Epoch 00005: val_loss did not improve from 0.48342
Epoch 6/32
407/407 [==============================] - 276s 678ms/step - loss: 0.2259 - acc: 0.9209 - val_loss: 0.4984 - val_acc: 0.8325

Epoch 00006: val_loss did not improve from 0.48342
Epoch 7/32
407/407 [==============================] - 277s 680ms/step - loss: 0.2259 - acc: 0.9213 - val_loss: 0.5058 - val_acc: 0.8350

Epoch 00007: val_loss did not improve from 0.48342
Epoch 8/32
144/407 [=========>....................] - ETA: 2:50 - loss: 0.2186 - acc: 0.9219KeyboardInterrupt
CPU times: user 54min 25s, sys: 6min 1s, total: 1h 27s
Wall time: 34min 30s
```

2018.5.24

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 299*299 round1+round2
```
#将round2的图片复制到round1文件夹内
def coverFiles(sourceDir,  targetDir): 
    for file in os.listdir(sourceDir): 
        sourceFile = os.path.join(sourceDir,  file) 
        targetFile = os.path.join(targetDir,  file) 
        #cover the files 
        if os.path.isfile(sourceFile): 
            open(targetFile, "wb").write(open(sourceFile, "rb").read())
source='data-raw/train_2/Images/neck_design_labels'
target='data-raw/train_1/Images/neck_design_labels'

coverFiles(source,target)

df_train = pd.read_csv('data-raw/train_1/Annotations/label_all.csv')
df_train.columns = ['image_id', 'class', 'label']
df_train.head()

classes = ['collar_design_labels', 'neckline_design_labels', 'skirt_length_labels',
          'sleeve_length_labels', 'neck_design_labels', 'coat_length_labels', 'lapel_design_labels',
          'pant_length_labels']

cur_class = classes[4]
df_load = df_train[(df_train['class'] == cur_class)].copy()
df_load.reset_index(inplace=True)
del df_load['index']


n = len(df_load)
n_class = len(df_load['label'][0])
width = 299 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
#X_half = np.zeros((n, 150, 150, 3), dtype=np.uint8)
y = np.zeros((n, n_class), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]

crip_high=int(width*0.5)
crip_width_start=int(width*0.25)
crip_width_end=int(width*0.75)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_1/{0}'.format(df_load['image_id'][i])), (width, width))
    X[i] = cv2.resize(X[i][:crip_high], (width, width))
    X[i] = cv2.resize(X[i][:][:,crip_width_start:crip_width_end], (width, width))
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train=X_half[:-800]
X_valid=X_half[-800:]
y_train=y[:-800]
y_valid=y[-800:]
X_train.shape


x_train_path='data-raw/train_1/{}X_train_299_clip_w_h.bc'.format(prefix_cls)
x_valid_path='data-raw/train_1/{}X_valid_299_clip_w_h.bc'.format(prefix_cls)
y_train_path='data-raw/train_1/{}y_train_299_clip_w_h.bc'.format(prefix_cls)
y_valid_path='data-raw/train_1/{}y_vaild_299_clip_w_h.bc'.format(prefix_cls)

save_array(x_train_path,X_train)
save_array(x_valid_path,X_valid)
save_array(y_train_path,y_train)
save_array(y_valid_path,y_valid)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_alldata_clip_w_h_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```
lr=0.001 neck_alldata_clip_w_h_weights.h5
```
Epoch 1/32
407/407 [==============================] - 371s 911ms/step - loss: 1.0230 - acc: 0.6052 - val_loss: 0.9573 - val_acc: 0.6438

Epoch 00001: val_loss improved from inf to 0.95734, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 2/32
407/407 [==============================] - 303s 746ms/step - loss: 0.7530 - acc: 0.7236 - val_loss: 1.0087 - val_acc: 0.6475

Epoch 00002: val_loss did not improve from 0.95734
Epoch 3/32
407/407 [==============================] - 303s 746ms/step - loss: 0.6687 - acc: 0.7532 - val_loss: 0.7341 - val_acc: 0.7388

Epoch 00003: val_loss improved from 0.95734 to 0.73410, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 4/32
407/407 [==============================] - 303s 745ms/step - loss: 0.5953 - acc: 0.7839 - val_loss: 0.5954 - val_acc: 0.7863

Epoch 00004: val_loss improved from 0.73410 to 0.59536, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 5/32
407/407 [==============================] - 303s 745ms/step - loss: 0.5463 - acc: 0.7970 - val_loss: 0.6211 - val_acc: 0.7875

Epoch 00005: val_loss did not improve from 0.59536
Epoch 6/32
407/407 [==============================] - 303s 745ms/step - loss: 0.5009 - acc: 0.8184 - val_loss: 1.0189 - val_acc: 0.6863

Epoch 00006: val_loss did not improve from 0.59536
Epoch 7/32
 13/407 [..............................] - ETA: 4:47 - loss: 0.4734 - acc: 0.8365KeyboardInterrupt
CPU times: user 1h 2min 26s, sys: 5min 30s, total: 1h 7min 57s
Wall time: 32min 8s


```
lr=0.0001 neck2_alldata_clip_w_h_weights.h5
```



```

lr=0.00001 neck3_alldata_clip_w_h_weights.h5
```



```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 round1+round2
```
#将round2的图片复制到round1文件夹内
def coverFiles(sourceDir,  targetDir): 
    for file in os.listdir(sourceDir): 
        sourceFile = os.path.join(sourceDir,  file) 
        targetFile = os.path.join(targetDir,  file) 
        #cover the files 
        if os.path.isfile(sourceFile): 
            open(targetFile, "wb").write(open(sourceFile, "rb").read())
source='data-raw/train_2/Images/neck_design_labels'
target='data-raw/train_1/Images/neck_design_labels'

coverFiles(source,target)

df_train = pd.read_csv('data-raw/train_1/Annotations/label_all.csv')
df_train.columns = ['image_id', 'class', 'label']
df_train.head()

classes = ['collar_design_labels', 'neckline_design_labels', 'skirt_length_labels',
          'sleeve_length_labels', 'neck_design_labels', 'coat_length_labels', 'lapel_design_labels',
          'pant_length_labels']

cur_class = classes[4]
df_load = df_train[(df_train['class'] == cur_class)].copy()
df_load.reset_index(inplace=True)
del df_load['index']


n = len(df_load)
n_class = len(df_load['label'][0])
width = 299 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
X_half = np.zeros((n, 150, 150, 3), dtype=np.uint8)
y = np.zeros((n, n_class), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]

crip_high=int(width*0.5)
crip_width_start=int(width*0.25)
crip_width_end=int(width*0.75)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_1/{0}'.format(df_load['image_id'][i])), (width, width))
    X[i] = cv2.resize(X[i][:crip_high], (width, width))
    X_half[i] = cv2.resize(X[i][:][:,crip_width_start:crip_width_end], (150, 150))
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(150, 150, 3), weights='imagenet')

inputs = Input((150, 150, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train=X_half[:-800]
X_valid=X_half[-800:]
y_train=y[:-800]
y_valid=y[-800:]
X_train.shape


x_train_path='data-raw/train_1/{}X_train_150_clip_w_h.bc'.format(prefix_cls)
x_valid_path='data-raw/train_1/{}X_valid_150_clip_w_h.bc'.format(prefix_cls)
y_train_path='data-raw/train_1/{}y_train_150_clip_w_h.bc'.format(prefix_cls)
y_valid_path='data-raw/train_1/{}y_vaild_150_clip_w_h.bc'.format(prefix_cls)

save_array(x_train_path,X_train)
save_array(x_valid_path,X_valid)
save_array(y_train_path,y_train)
save_array(y_valid_path,y_valid)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 64

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_alldata_clip_w_h_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```
lr=0.001 neck_alldata_clip_w_h_weights.h5
```
Epoch 1/32
203/203 [==============================] - 144s 711ms/step - loss: 1.1502 - acc: 0.5598 - val_loss: 1.7737 - val_acc: 0.5700

Epoch 00001: val_loss improved from inf to 1.77366, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 2/32
203/203 [==============================] - 87s 430ms/step - loss: 0.8848 - acc: 0.6723 - val_loss: 0.9445 - val_acc: 0.6362

Epoch 00002: val_loss improved from 1.77366 to 0.94445, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 3/32
203/203 [==============================] - 87s 430ms/step - loss: 0.7320 - acc: 0.7229 - val_loss: 1.0783 - val_acc: 0.6200

Epoch 00003: val_loss did not improve from 0.94445
Epoch 4/32
203/203 [==============================] - 87s 430ms/step - loss: 0.6604 - acc: 0.7514 - val_loss: 0.8350 - val_acc: 0.6512

Epoch 00004: val_loss improved from 0.94445 to 0.83497, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 5/32
203/203 [==============================] - 87s 430ms/step - loss: 0.6863 - acc: 0.7504 - val_loss: 0.9143 - val_acc: 0.6625

Epoch 00005: val_loss did not improve from 0.83497
Epoch 6/32
203/203 [==============================] - 87s 430ms/step - loss: 0.5955 - acc: 0.7812 - val_loss: 0.8301 - val_acc: 0.7013

Epoch 00006: val_loss improved from 0.83497 to 0.83013, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 7/32
203/203 [==============================] - 87s 429ms/step - loss: 0.5485 - acc: 0.7971 - val_loss: 0.8015 - val_acc: 0.6963

Epoch 00007: val_loss improved from 0.83013 to 0.80148, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 8/32
203/203 [==============================] - 87s 429ms/step - loss: 0.4906 - acc: 0.8133 - val_loss: 0.8204 - val_acc: 0.7212

Epoch 00008: val_loss did not improve from 0.80148
Epoch 9/32
203/203 [==============================] - 87s 429ms/step - loss: 0.4665 - acc: 0.8301 - val_loss: 0.9266 - val_acc: 0.6987

Epoch 00009: val_loss did not improve from 0.80148
Epoch 10/32
203/203 [==============================] - 87s 430ms/step - loss: 0.4333 - acc: 0.8433 - val_loss: 0.7947 - val_acc: 0.7300

Epoch 00010: val_loss improved from 0.80148 to 0.79466, saving model to models/neck_alldata_clip_w_h_weights.h5
Epoch 11/32
203/203 [==============================] - 87s 429ms/step - loss: 0.3831 - acc: 0.8585 - val_loss: 0.8707 - val_acc: 0.7037

Epoch 00011: val_loss did not improve from 0.79466
Epoch 12/32
203/203 [==============================] - 87s 429ms/step - loss: 0.4920 - acc: 0.8212 - val_loss: 1.1450 - val_acc: 0.6837

Epoch 00012: val_loss did not improve from 0.79466
Epoch 13/32
 73/203 [=========>....................] - ETA: 54s - loss: 0.4604 - acc: 0.8416KeyboardInterrupt
CPU times: user 38min 50s, sys: 3min 33s, total: 42min 23s
Wall time: 19min 23s
```
lr=0.0001 neck2_alldata_clip_w_h_weights.h5
```
Epoch 1/32
203/203 [==============================] - 142s 698ms/step - loss: 0.2809 - acc: 0.8974 - val_loss: 0.6231 - val_acc: 0.7987

Epoch 00001: val_loss improved from inf to 0.62307, saving model to models/neck2_alldata_clip_w_h_weights.h5
Epoch 2/32
203/203 [==============================] - 88s 433ms/step - loss: 0.2049 - acc: 0.9251 - val_loss: 0.7193 - val_acc: 0.7937

Epoch 00002: val_loss did not improve from 0.62307
Epoch 3/32
203/203 [==============================] - 87s 430ms/step - loss: 0.1751 - acc: 0.9362 - val_loss: 0.7550 - val_acc: 0.8000

Epoch 00003: val_loss did not improve from 0.62307
Epoch 4/32
203/203 [==============================] - 88s 432ms/step - loss: 0.1500 - acc: 0.9462 - val_loss: 0.7942 - val_acc: 0.7937

Epoch 00004: val_loss did not improve from 0.62307
Epoch 5/32
203/203 [==============================] - 87s 429ms/step - loss: 0.1329 - acc: 0.9523 - val_loss: 0.8000 - val_acc: 0.8025

Epoch 00005: val_loss did not improve from 0.62307
Epoch 6/32
203/203 [==============================] - 87s 430ms/step - loss: 0.1108 - acc: 0.9609 - val_loss: 0.8428 - val_acc: 0.8025

Epoch 00006: val_loss did not improve from 0.62307
Epoch 7/32
 16/203 [=>............................] - ETA: 1:18 - loss: 0.1086 - acc: 0.9648KeyboardInterrupt
```
lr=0.0001 neck3_alldata_clip_w_h_weights.h5
```
Epoch 1/32
203/203 [==============================] - 146s 719ms/step - loss: 0.2052 - acc: 0.9255 - val_loss: 0.7150 - val_acc: 0.7950

Epoch 00001: val_loss improved from inf to 0.71502, saving model to models/neck3_alldata_clip_w_h_weights.h5
Epoch 2/32
203/203 [==============================] - 88s 433ms/step - loss: 0.2071 - acc: 0.9272 - val_loss: 0.7128 - val_acc: 0.7987

Epoch 00002: val_loss improved from 0.71502 to 0.71277, saving model to models/neck3_alldata_clip_w_h_weights.h5
Epoch 3/32
 17/203 [=>............................] - ETA: 1:19 - loss: 0.2280 - acc: 0.9127KeyboardInterrupt
CPU times: user 7min 49s, sys: 38 s, total: 8min 27s
Wall time: 4min 33s

```


- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 有了预处理 l2 1
```
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (width, width), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_


n = len(df_load)
n_class = len(df_load['label'][0])
width = 150 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
y = np.zeros((n, n_class), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',
         kernel_regularizer=regularizers.l2(1),
          bias_regularizer=regularizers.l2(1)
         )(x)

model = Model(inputs, x)



# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 64

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'nearest',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_reg_batch64_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_batch32_ssd_crop_label.h5'.format(prefix_cls),)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
lr=0.001 coat_regx_batch64_label.h5
```

Epoch 1/32
203/203 [==============================] - 161s 793ms/step - loss: 3.1224 - acc: 0.3878 - val_loss: 1.6555 - val_acc: 0.3824

Epoch 00001: val_loss improved from inf to 1.65553, saving model to models/coat_regx_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 100s 495ms/step - loss: 1.5095 - acc: 0.4583 - val_loss: 1.5499 - val_acc: 0.4627

Epoch 00002: val_loss improved from 1.65553 to 1.54993, saving model to models/coat_regx_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 100s 494ms/step - loss: 1.4164 - acc: 0.4945 - val_loss: 1.5040 - val_acc: 0.4696

Epoch 00003: val_loss improved from 1.54993 to 1.50395, saving model to models/coat_regx_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 100s 493ms/step - loss: 1.3802 - acc: 0.5160 - val_loss: 1.3842 - val_acc: 0.5263

Epoch 00004: val_loss improved from 1.50395 to 1.38425, saving model to models/coat_regx_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 100s 492ms/step - loss: 1.3370 - acc: 0.5371 - val_loss: 1.4275 - val_acc: 0.5083

Epoch 00005: val_loss did not improve from 1.38425
Epoch 6/32
203/203 [==============================] - 100s 493ms/step - loss: 1.2870 - acc: 0.5599 - val_loss: 1.5706 - val_acc: 0.4544

Epoch 00006: val_loss did not improve from 1.38425
Epoch 7/32
203/203 [==============================] - 100s 491ms/step - loss: 1.2613 - acc: 0.5722 - val_loss: 1.3527 - val_acc: 0.5304

Epoch 00007: val_loss improved from 1.38425 to 1.35274, saving model to models/coat_regx_batch64_label.h5
Epoch 8/32
203/203 [==============================] - 100s 495ms/step - loss: 1.2189 - acc: 0.5899 - val_loss: 1.3715 - val_acc: 0.5297

Epoch 00008: val_loss did not improve from 1.35274
Epoch 9/32
203/203 [==============================] - 100s 493ms/step - loss: 1.1921 - acc: 0.6024 - val_loss: 1.4657 - val_acc: 0.5076

Epoch 00009: val_loss did not improve from 1.35274
Epoch 10/32
 64/203 [========>.....................] - ETA: 1:06 - loss: 1.1160 - acc: 0.6343KeyboardInterrupt
CPU times: user 34min 40s, sys: 2min 50s, total: 37min 30s
Wall time: 17min 3s
```
lr=0.001  coat_regy_batch64_label.h5    droupout=0.8 l2=1
```
Epoch 1/32
203/203 [==============================] - 167s 823ms/step - loss: 3.3175 - acc: 0.3520 - val_loss: 1.7018 - val_acc: 0.3893

Epoch 00001: val_loss improved from inf to 1.70180, saving model to models/coat_regy_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 102s 501ms/step - loss: 1.6027 - acc: 0.4322 - val_loss: 1.6826 - val_acc: 0.4136

Epoch 00002: val_loss improved from 1.70180 to 1.68258, saving model to models/coat_regy_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 101s 500ms/step - loss: 1.5221 - acc: 0.4763 - val_loss: 1.6722 - val_acc: 0.4336

Epoch 00003: val_loss improved from 1.68258 to 1.67223, saving model to models/coat_regy_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 102s 501ms/step - loss: 1.4844 - acc: 0.4997 - val_loss: 1.5150 - val_acc: 0.4993

Epoch 00004: val_loss improved from 1.67223 to 1.51496, saving model to models/coat_regy_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 102s 503ms/step - loss: 1.4492 - acc: 0.5096 - val_loss: 1.5721 - val_acc: 0.4799

Epoch 00005: val_loss did not improve from 1.51496
Epoch 6/32
203/203 [==============================] - 102s 501ms/step - loss: 1.3979 - acc: 0.5294 - val_loss: 1.5171 - val_acc: 0.5035

Epoch 00006: val_loss did not improve from 1.51496
Epoch 7/32
203/203 [==============================] - 102s 501ms/step - loss: 1.3715 - acc: 0.5444 - val_loss: 1.3789 - val_acc: 0.5491

Epoch 00007: val_loss improved from 1.51496 to 1.37895, saving model to models/coat_regy_batch64_label.h5
Epoch 8/32
203/203 [==============================] - 102s 502ms/step - loss: 1.3286 - acc: 0.5626 - val_loss: 1.4132 - val_acc: 0.5553

Epoch 00008: val_loss did not improve from 1.37895
Epoch 9/32
203/203 [==============================] - 103s 507ms/step - loss: 1.3001 - acc: 0.5780 - val_loss: 1.3755 - val_acc: 0.5505

Epoch 00009: val_loss improved from 1.37895 to 1.37547, saving model to models/coat_regy_batch64_label.h5
Epoch 10/32
203/203 [==============================] - 103s 508ms/step - loss: 1.2824 - acc: 0.5875 - val_loss: 1.4568 - val_acc: 0.5290

Epoch 00010: val_loss did not improve from 1.37547
Epoch 11/32
203/203 [==============================] - 104s 510ms/step - loss: 1.2410 - acc: 0.6008 - val_loss: 1.3806 - val_acc: 0.5484

Epoch 00011: val_loss did not improve from 1.37547
Epoch 12/32
203/203 [==============================] - 103s 508ms/step - loss: 1.2069 - acc: 0.6228 - val_loss: 1.5186 - val_acc: 0.5104

Epoch 00012: val_loss did not improve from 1.37547
Epoch 13/32
 49/203 [======>.......................] - ETA: 1:15 - loss: 1.1883 - acc: 0.6247KeyboardInterrupt
CPU times: user 46min 29s, sys: 3min 40s, total: 50min 10s
Wall time: 22min 29s

```
lr=0.001  coat_regy_batch64_label.h5    droupout=0.8 l2=0.1
```
Epoch 1/32
203/203 [==============================] - 171s 844ms/step - loss: 2.1389 - acc: 0.3622 - val_loss: 2.1756 - val_acc: 0.3887

Epoch 00001: val_loss improved from inf to 2.17559, saving model to models/coat_regz_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 102s 501ms/step - loss: 1.4962 - acc: 0.4447 - val_loss: 1.7927 - val_acc: 0.4232

Epoch 00002: val_loss improved from 2.17559 to 1.79270, saving model to models/coat_regz_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 102s 500ms/step - loss: 1.3839 - acc: 0.4836 - val_loss: 1.5852 - val_acc: 0.4474

Epoch 00003: val_loss improved from 1.79270 to 1.58516, saving model to models/coat_regz_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 102s 502ms/step - loss: 1.3097 - acc: 0.5125 - val_loss: 1.4863 - val_acc: 0.4723

Epoch 00004: val_loss improved from 1.58516 to 1.48634, saving model to models/coat_regz_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 102s 501ms/step - loss: 1.2703 - acc: 0.5290 - val_loss: 1.2835 - val_acc: 0.5498

Epoch 00005: val_loss improved from 1.48634 to 1.28349, saving model to models/coat_regz_batch64_label.h5
Epoch 6/32
203/203 [==============================] - 102s 501ms/step - loss: 1.2153 - acc: 0.5563 - val_loss: 1.3685 - val_acc: 0.5118

Epoch 00006: val_loss did not improve from 1.28349
Epoch 7/32
203/203 [==============================] - 102s 505ms/step - loss: 1.1731 - acc: 0.5745 - val_loss: 1.4476 - val_acc: 0.4869

Epoch 00007: val_loss did not improve from 1.28349
Epoch 8/32
203/203 [==============================] - 103s 506ms/step - loss: 1.1340 - acc: 0.5854 - val_loss: 1.3033 - val_acc: 0.5201

Epoch 00008: val_loss did not improve from 1.28349
Epoch 9/32
203/203 [==============================] - 102s 502ms/step - loss: 1.0874 - acc: 0.6099 - val_loss: 1.2453 - val_acc: 0.5539

Epoch 00009: val_loss improved from 1.28349 to 1.24531, saving model to models/coat_regz_batch64_label.h5
Epoch 10/32
203/203 [==============================] - 102s 500ms/step - loss: 1.0631 - acc: 0.6195 - val_loss: 1.3346 - val_acc: 0.5450

Epoch 00010: val_loss did not improve from 1.24531
Epoch 11/32
203/203 [==============================] - 102s 501ms/step - loss: 1.0145 - acc: 0.6371 - val_loss: 1.2797 - val_acc: 0.5657

Epoch 00011: val_loss did not improve from 1.24531
Epoch 12/32
203/203 [==============================] - 102s 501ms/step - loss: 1.0070 - acc: 0.6410 - val_loss: 1.2704 - val_acc: 0.5588

Epoch 00012: val_loss did not improve from 1.24531
Epoch 13/32
 72/203 [=========>....................] - ETA: 1:03 - loss: 0.8976 - acc: 0.6860KeyboardInterrupt
CPU times: user 47min 9s, sys: 3min 38s, total: 50min 48s
Wall time: 22min 39s
```

lr=0.001 l2=0.001 dropout=0.5
```
Epoch 1/32
203/203 [==============================] - 174s 856ms/step - loss: 1.6048 - acc: 0.3836 - val_loss: 8.9149 - val_acc: 0.2649

Epoch 00001: val_loss improved from inf to 8.91488, saving model to models/coat_rega_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 103s 509ms/step - loss: 1.3993 - acc: 0.4514 - val_loss: 1.6675 - val_acc: 0.4073

Epoch 00002: val_loss improved from 8.91488 to 1.66751, saving model to models/coat_rega_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 103s 507ms/step - loss: 1.3071 - acc: 0.4808 - val_loss: 1.2985 - val_acc: 0.4855

Epoch 00003: val_loss improved from 1.66751 to 1.29850, saving model to models/coat_rega_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 103s 509ms/step - loss: 1.2494 - acc: 0.5029 - val_loss: 1.2446 - val_acc: 0.5180

Epoch 00004: val_loss improved from 1.29850 to 1.24458, saving model to models/coat_rega_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 103s 508ms/step - loss: 1.1494 - acc: 0.5418 - val_loss: 1.3182 - val_acc: 0.5118

Epoch 00005: val_loss did not improve from 1.24458
Epoch 6/32
203/203 [==============================] - 103s 508ms/step - loss: 1.0933 - acc: 0.5632 - val_loss: 1.2323 - val_acc: 0.5277

Epoch 00006: val_loss improved from 1.24458 to 1.23235, saving model to models/coat_rega_batch64_label.h5
Epoch 7/32
203/203 [==============================] - 103s 509ms/step - loss: 1.0384 - acc: 0.5857 - val_loss: 1.1710 - val_acc: 0.5443

Epoch 00007: val_loss improved from 1.23235 to 1.17100, saving model to models/coat_rega_batch64_label.h5
Epoch 8/32
203/203 [==============================] - 103s 508ms/step - loss: 0.9852 - acc: 0.6109 - val_loss: 1.2295 - val_acc: 0.5221

Epoch 00008: val_loss did not improve from 1.17100
Epoch 9/32
203/203 [==============================] - 103s 506ms/step - loss: 0.9437 - acc: 0.6244 - val_loss: 1.1600 - val_acc: 0.5491

Epoch 00009: val_loss improved from 1.17100 to 1.16000, saving model to models/coat_rega_batch64_label.h5
Epoch 10/32
203/203 [==============================] - 103s 507ms/step - loss: 0.9131 - acc: 0.6359 - val_loss: 1.2832 - val_acc: 0.5207

Epoch 00010: val_loss did not improve from 1.16000
Epoch 11/32
203/203 [==============================] - 103s 507ms/step - loss: 0.8622 - acc: 0.6578 - val_loss: 1.3108 - val_acc: 0.5318

Epoch 00011: val_loss did not improve from 1.16000
Epoch 12/32
203/203 [==============================] - 103s 508ms/step - loss: 0.8037 - acc: 0.6776 - val_loss: 1.2751 - val_acc: 0.5346

Epoch 00012: val_loss did not improve from 1.16000
Epoch 13/32
203/203 [==============================] - 103s 508ms/step - loss: 0.7814 - acc: 0.6883 - val_loss: 1.2854 - val_acc: 0.5422

Epoch 00013: val_loss did not improve from 1.16000
Epoch 14/32
203/203 [==============================] - 103s 509ms/step - loss: 0.7262 - acc: 0.7134 - val_loss: 1.3693 - val_acc: 0.5297

Epoch 00014: val_loss did not improve from 1.16000
Epoch 15/32
203/203 [==============================] - 103s 508ms/step - loss: 0.6722 - acc: 0.7368 - val_loss: 1.4534 - val_acc: 0.5263

Epoch 00015: val_loss did not improve from 1.16000
Epoch 16/32
163/203 [=======================>......] - ETA: 19s - loss: 0.6314 - acc: 0.7531KeyboardInterrupt
CPU times: user 1h 33s, sys: 4min 37s, total: 1h 5min 11s
Wall time: 28min 52s

```

lr=0.001 l2=0.1 dropout=0.5 随机旋转30度
```
Epoch 1/32
203/203 [==============================] - 152s 747ms/step - loss: 2.0126 - acc: 0.3687 - val_loss: 1.4548 - val_acc: 0.4467

Epoch 00001: val_loss improved from inf to 1.45477, saving model to models/coat_regx_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 89s 438ms/step - loss: 1.4465 - acc: 0.4514 - val_loss: 1.4392 - val_acc: 0.4841

Epoch 00002: val_loss improved from 1.45477 to 1.43915, saving model to models/coat_regx_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 89s 439ms/step - loss: 1.3580 - acc: 0.4856 - val_loss: 1.4199 - val_acc: 0.4799

Epoch 00003: val_loss improved from 1.43915 to 1.41986, saving model to models/coat_regx_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 89s 439ms/step - loss: 1.2994 - acc: 0.5069 - val_loss: 1.4656 - val_acc: 0.4779

Epoch 00004: val_loss did not improve from 1.41986
Epoch 5/32
203/203 [==============================] - 89s 438ms/step - loss: 1.2601 - acc: 0.5218 - val_loss: 1.3230 - val_acc: 0.5145

Epoch 00005: val_loss improved from 1.41986 to 1.32297, saving model to models/coat_regx_batch64_label.h5
Epoch 6/32
203/203 [==============================] - 89s 437ms/step - loss: 1.2148 - acc: 0.5406 - val_loss: 1.2626 - val_acc: 0.5118

Epoch 00006: val_loss improved from 1.32297 to 1.26258, saving model to models/coat_regx_batch64_label.h5
Epoch 7/32
203/203 [==============================] - 89s 439ms/step - loss: 1.1795 - acc: 0.5553 - val_loss: 1.2340 - val_acc: 0.5284

Epoch 00007: val_loss improved from 1.26258 to 1.23396, saving model to models/coat_regx_batch64_label.h5
Epoch 8/32
203/203 [==============================] - 89s 438ms/step - loss: 1.1266 - acc: 0.5776 - val_loss: 1.2446 - val_acc: 0.5422

Epoch 00008: val_loss did not improve from 1.23396
Epoch 9/32
203/203 [==============================] - 89s 438ms/step - loss: 1.0956 - acc: 0.5837 - val_loss: 1.3094 - val_acc: 0.5526

Epoch 00009: val_loss did not improve from 1.23396
Epoch 10/32
203/203 [==============================] - 89s 437ms/step - loss: 1.0733 - acc: 0.6021 - val_loss: 1.3805 - val_acc: 0.4965

Epoch 00010: val_loss did not improve from 1.23396
Epoch 11/32
203/203 [==============================] - 89s 439ms/step - loss: 1.0415 - acc: 0.6125 - val_loss: 1.3332 - val_acc: 0.5152

Epoch 00011: val_loss did not improve from 1.23396
Epoch 12/32
203/203 [==============================] - 89s 437ms/step - loss: 1.0061 - acc: 0.6275 - val_loss: 1.2528 - val_acc: 0.5353

Epoch 00012: val_loss did not improve from 1.23396
Epoch 13/32
 31/203 [===>..........................] - ETA: 1:12 - loss: 0.9246 - acc: 0.6588KeyboardInterrupt
CPU times: user 39min 20s, sys: 3min 40s, total: 43min 1s
Wall time: 19min 31s


```
lr=0.001 l2=0.1 dropout=0.5 随机旋转30度 nadam 优化器
```
Epoch 1/32
203/203 [==============================] - 160s 788ms/step - loss: 1.9984 - acc: 0.3778 - val_loss: 1.5378 - val_acc: 0.4025

Epoch 00001: val_loss improved from inf to 1.53783, saving model to models/coat_regy_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 92s 451ms/step - loss: 1.4040 - acc: 0.4687 - val_loss: 1.4265 - val_acc: 0.4461

Epoch 00002: val_loss improved from 1.53783 to 1.42651, saving model to models/coat_regy_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 91s 450ms/step - loss: 1.3187 - acc: 0.4920 - val_loss: 1.3963 - val_acc: 0.4454

Epoch 00003: val_loss improved from 1.42651 to 1.39635, saving model to models/coat_regy_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 92s 451ms/step - loss: 1.2635 - acc: 0.5199 - val_loss: 1.2776 - val_acc: 0.4931

Epoch 00004: val_loss improved from 1.39635 to 1.27756, saving model to models/coat_regy_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 91s 450ms/step - loss: 1.2101 - acc: 0.5462 - val_loss: 1.3088 - val_acc: 0.4910

Epoch 00005: val_loss did not improve from 1.27756
Epoch 6/32
203/203 [==============================] - 91s 449ms/step - loss: 1.1798 - acc: 0.5551 - val_loss: 1.3057 - val_acc: 0.5166

Epoch 00006: val_loss did not improve from 1.27756
Epoch 7/32
203/203 [==============================] - 91s 449ms/step - loss: 1.1285 - acc: 0.5763 - val_loss: 2.3191 - val_acc: 0.4869

Epoch 00007: val_loss did not improve from 1.27756
Epoch 8/32
 63/203 [========>.....................] - ETA: 1:01 - loss: 1.0694 - acc: 0.6000KeyboardInterrupt
CPU times: user 24min 47s, sys: 2min 7s, total: 26min 55s
Wall time: 12min 43s
```
lr=0.0001 l2=0.1 dropout=0.5 随机旋转30度 nadam 优化器
```
Epoch 1/32
203/203 [==============================] - 159s 785ms/step - loss: 1.0533 - acc: 0.6049 - val_loss: 2.1330 - val_acc: 0.1563

Epoch 00001: val_loss improved from inf to 2.13302, saving model to models/coat2_regy_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 92s 455ms/step - loss: 0.9565 - acc: 0.6439 - val_loss: 2.1129 - val_acc: 0.1342

Epoch 00002: val_loss improved from 2.13302 to 2.11292, saving model to models/coat2_regy_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 93s 456ms/step - loss: 0.9120 - acc: 0.6575 - val_loss: 1.7865 - val_acc: 0.4509

Epoch 00003: val_loss improved from 2.11292 to 1.78648, saving model to models/coat2_regy_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 92s 454ms/step - loss: 0.8611 - acc: 0.6773 - val_loss: 1.2108 - val_acc: 0.5533

Epoch 00004: val_loss improved from 1.78648 to 1.21078, saving model to models/coat2_regy_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 92s 455ms/step - loss: 0.8103 - acc: 0.6967 - val_loss: 1.0895 - val_acc: 0.5989

Epoch 00005: val_loss improved from 1.21078 to 1.08947, saving model to models/coat2_regy_batch64_label.h5
Epoch 6/32
203/203 [==============================] - 92s 455ms/step - loss: 0.7504 - acc: 0.7221 - val_loss: 1.1520 - val_acc: 0.5775

Epoch 00006: val_loss did not improve from 1.08947
Epoch 7/32
203/203 [==============================] - 93s 457ms/step - loss: 0.6988 - acc: 0.7463 - val_loss: 1.2084 - val_acc: 0.5871

Epoch 00007: val_loss did not improve from 1.08947
Epoch 8/32

```
- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 299*299  l2 0.01 多GPU

```
失败，keras有问题
```

```

```



2018.5.23

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 有了预处理 l2 0.01
```
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (width, width), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_


n = len(df_load)
n_class = len(df_load['label'][0])
width = 150 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
y = np.zeros((n, n_class), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',
         kernel_regularizer=regularizers.l2(0.01),
          bias_regularizer=regularizers.l2(0.01)
         )(x)

model = Model(inputs, x)



# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 64

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'nearest',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_reg_batch64_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_batch32_ssd_crop_label.h5'.format(prefix_cls),)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
lr=0.001 coat_reg_batch64_label.h5

```
Epoch 1/32
203/203 [==============================] - 150s 737ms/step - loss: 1.9645 - acc: 0.3934 - val_loss: 1.5510 - val_acc: 0.4281

Epoch 00001: val_loss improved from inf to 1.55104, saving model to models/coat_reg_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 88s 434ms/step - loss: 1.3704 - acc: 0.4757 - val_loss: 1.4521 - val_acc: 0.4544

Epoch 00002: val_loss improved from 1.55104 to 1.45214, saving model to models/coat_reg_batch64_label.h5
Epoch 3/32
203/203 [==============================] - 88s 436ms/step - loss: 1.2688 - acc: 0.5160 - val_loss: 1.2884 - val_acc: 0.5035

Epoch 00003: val_loss improved from 1.45214 to 1.28836, saving model to models/coat_reg_batch64_label.h5
Epoch 4/32
203/203 [==============================] - 88s 436ms/step - loss: 1.2085 - acc: 0.5421 - val_loss: 1.2857 - val_acc: 0.5166

Epoch 00004: val_loss improved from 1.28836 to 1.28566, saving model to models/coat_reg_batch64_label.h5
Epoch 5/32
203/203 [==============================] - 89s 437ms/step - loss: 1.1631 - acc: 0.5556 - val_loss: 1.4095 - val_acc: 0.4668

Epoch 00005: val_loss did not improve from 1.28566
Epoch 6/32
203/203 [==============================] - 89s 436ms/step - loss: 1.0920 - acc: 0.5925 - val_loss: 1.2254 - val_acc: 0.5422

Epoch 00006: val_loss improved from 1.28566 to 1.22536, saving model to models/coat_reg_batch64_label.h5
Epoch 7/32
203/203 [==============================] - 89s 437ms/step - loss: 1.0548 - acc: 0.6044 - val_loss: 1.2266 - val_acc: 0.5588

Epoch 00007: val_loss did not improve from 1.22536
Epoch 8/32
203/203 [==============================] - 89s 436ms/step - loss: 1.0232 - acc: 0.6220 - val_loss: 1.2420 - val_acc: 0.5491

Epoch 00008: val_loss did not improve from 1.22536
Epoch 9/32
203/203 [==============================] - 88s 436ms/step - loss: 0.9812 - acc: 0.6402 - val_loss: 1.2168 - val_acc: 0.5443

Epoch 00009: val_loss improved from 1.22536 to 1.21676, saving model to models/coat_reg_batch64_label.h5
Epoch 10/32
203/203 [==============================] - 89s 436ms/step - loss: 0.9344 - acc: 0.6559 - val_loss: 1.2485 - val_acc: 0.5422

Epoch 00010: val_loss did not improve from 1.21676
Epoch 11/32
203/203 [==============================] - 89s 437ms/step - loss: 0.9010 - acc: 0.6733 - val_loss: 1.2838 - val_acc: 0.5588

Epoch 00011: val_loss did not improve from 1.21676
Epoch 12/32
203/203 [==============================] - 89s 436ms/step - loss: 0.8435 - acc: 0.6930 - val_loss: 1.2979 - val_acc: 0.5373

Epoch 00012: val_loss did not improve from 1.21676
Epoch 13/32
203/203 [==============================] - 88s 436ms/step - loss: 0.8155 - acc: 0.7047 - val_loss: 1.2952 - val_acc: 0.5456

Epoch 00013: val_loss did not improve from 1.21676
Epoch 14/32
 18/203 [=>............................] - ETA: 1:17 - loss: 0.8078 - acc: 0.7144KeyboardInterrupt
CPU times: user 42min 12s, sys: 3min 54s, total: 46min 6s
Wall time: 20min 49s

```

lr=0.0001 coat2_reg_batch64_label.h5
```
Epoch 1/32
203/203 [==============================] - 148s 731ms/step - loss: 0.7469 - acc: 0.7336 - val_loss: 1.0819 - val_acc: 0.5996

Epoch 00001: val_loss improved from inf to 1.08195, saving model to models/coat2_reg_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 89s 437ms/step - loss: 0.6472 - acc: 0.7700 - val_loss: 1.1117 - val_acc: 0.6058

Epoch 00002: val_loss did not improve from 1.08195
Epoch 3/32
203/203 [==============================] - 89s 439ms/step - loss: 0.5851 - acc: 0.7928 - val_loss: 1.1360 - val_acc: 0.6107

Epoch 00003: val_loss did not improve from 1.08195
Epoch 4/32
203/203 [==============================] - 89s 439ms/step - loss: 0.5316 - acc: 0.8141 - val_loss: 1.2133 - val_acc: 0.6107

Epoch 00004: val_loss did not improve from 1.08195
Epoch 5/32
 36/203 [====>.........................] - ETA: 1:10 - loss: 0.4956 - acc: 0.8277KeyboardInterrupt
CPU times: user 14min 31s, sys: 1min 15s, total: 15min 46s
Wall time: 7min 40s

```
lr=0.00001 coat3_reg_batch64_label.h5
```
Epoch 1/32
203/203 [==============================] - 150s 738ms/step - loss: 0.6462 - acc: 0.7702 - val_loss: 1.0865 - val_acc: 0.6127

Epoch 00001: val_loss improved from inf to 1.08650, saving model to models/coat3_reg_batch64_label.h5
Epoch 2/32
203/203 [==============================] - 89s 439ms/step - loss: 0.6353 - acc: 0.7752 - val_loss: 1.0892 - val_acc: 0.6162

Epoch 00002: val_loss did not improve from 1.08650
Epoch 3/32
203/203 [==============================] - 89s 440ms/step - loss: 0.6216 - acc: 0.7808 - val_loss: 1.0962 - val_acc: 0.6141

Epoch 00003: val_loss did not improve from 1.08650
Epoch 4/32
203/203 [==============================] - 89s 438ms/step - loss: 0.6207 - acc: 0.7789 - val_loss: 1.0972 - val_acc: 0.6162

Epoch 00004: val_loss did not improve from 1.08650
Epoch 5/32
203/203 [==============================] - 89s 438ms/step - loss: 0.6048 - acc: 0.7830 - val_loss: 1.1070 - val_acc: 0.6134

Epoch 00005: val_loss did not improve from 1.08650
Epoch 6/32
203/203 [==============================] - 89s 440ms/step - loss: 0.5915 - acc: 0.7914 - val_loss: 1.1136 - val_acc: 0.6093

Epoch 00006: val_loss did not improve from 1.08650
Epoch 7/32
126/203 [=================>............] - ETA: 32s - loss: 0.5883 - acc: 0.7977KeyboardInterrupt
CPU times: user 22min 13s, sys: 1min 56s, total: 24min 9s
Wall time: 11min 19s

```

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 有了预处理 l2 0.01
```
n = len(df_load)
n_class = len(df_load['label'][0])
width = 150 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
y = np.zeros((n, n_class), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',
         kernel_regularizer=regularizers.l2(0.01),
          bias_regularizer=regularizers.l2(0.01)
         )(x)

model = Model(inputs, x)

x_train_path='data-raw/train_2/{}X_train_150_clip_w.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_150_clip_w.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_150_clip_w.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_150_clip_w.bc'.format(prefix_cls)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 64

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'nearest',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}reg_batch64_ssd_crop_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_batch32_ssd_crop_label.h5'.format(prefix_cls),)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
lr=0.001 coatreg_batch64_ssd_crop_label.h5 l2=0.01
```
Epoch 1/32
203/203 [==============================] - 155s 764ms/step - loss: 1.7166 - acc: 0.3735 - val_loss: 2.6654 - val_acc: 0.3520

Epoch 00001: val_loss improved from inf to 2.66540, saving model to models/coatreg_batch64_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 439ms/step - loss: 1.4292 - acc: 0.4576 - val_loss: 2.7308 - val_acc: 0.4246

Epoch 00002: val_loss did not improve from 2.66540
Epoch 3/32
203/203 [==============================] - 89s 437ms/step - loss: 1.3140 - acc: 0.4921 - val_loss: 1.3161 - val_acc: 0.4889

Epoch 00003: val_loss improved from 2.66540 to 1.31614, saving model to models/coatreg_batch64_ssd_crop_label.h5
Epoch 4/32
203/203 [==============================] - 89s 438ms/step - loss: 1.2313 - acc: 0.5245 - val_loss: 1.2737 - val_acc: 0.5436

Epoch 00004: val_loss improved from 1.31614 to 1.27374, saving model to models/coatreg_batch64_ssd_crop_label.h5
Epoch 5/32
203/203 [==============================] - 89s 438ms/step - loss: 1.1698 - acc: 0.5451 - val_loss: 1.4081 - val_acc: 0.5104

Epoch 00005: val_loss did not improve from 1.27374
Epoch 6/32
203/203 [==============================] - 89s 440ms/step - loss: 1.1113 - acc: 0.5660 - val_loss: 1.2747 - val_acc: 0.4710

Epoch 00006: val_loss did not improve from 1.27374
Epoch 7/32
203/203 [==============================] - 89s 439ms/step - loss: 1.0565 - acc: 0.5874 - val_loss: 1.2746 - val_acc: 0.5443

Epoch 00007: val_loss did not improve from 1.27374
Epoch 8/32
203/203 [==============================] - 89s 438ms/step - loss: 1.0147 - acc: 0.6055 - val_loss: 1.4634 - val_acc: 0.5028

Epoch 00008: val_loss did not improve from 1.27374
Epoch 9/32
203/203 [==============================] - 89s 439ms/step - loss: 0.9706 - acc: 0.6178 - val_loss: 1.2368 - val_acc: 0.5394

Epoch 00009: val_loss improved from 1.27374 to 1.23676, saving model to models/coatreg_batch64_ssd_crop_label.h5
Epoch 10/32
203/203 [==============================] - 89s 438ms/step - loss: 0.9369 - acc: 0.6336 - val_loss: 1.2116 - val_acc: 0.5512

Epoch 00010: val_loss improved from 1.23676 to 1.21165, saving model to models/coatreg_batch64_ssd_crop_label.h5
Epoch 11/32
203/203 [==============================] - 89s 439ms/step - loss: 0.8913 - acc: 0.6567 - val_loss: 1.3334 - val_acc: 0.5104

Epoch 00011: val_loss did not improve from 1.21165
Epoch 12/32
 32/203 [===>..........................] - ETA: 1:11 - loss: 0.8358 - acc: 0.6846KeyboardInterrupt
CPU times: user 36min 28s, sys: 3min 19s, total: 39min 48s
Wall time: 18min 6s

```
lr=0.0001 coatreg2_batch64_ssd_crop_label.h5 l2=0.01
```
Epoch 1/32
203/203 [==============================] - 147s 722ms/step - loss: 0.7350 - acc: 0.7144 - val_loss: 1.1077 - val_acc: 0.5989

Epoch 00001: val_loss improved from inf to 1.10771, saving model to models/coatreg2_batch64_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 440ms/step - loss: 0.6244 - acc: 0.7571 - val_loss: 1.1934 - val_acc: 0.5906

Epoch 00002: val_loss did not improve from 1.10771
Epoch 3/32
203/203 [==============================] - 89s 440ms/step - loss: 0.5724 - acc: 0.7788 - val_loss: 1.2619 - val_acc: 0.5761

Epoch 00003: val_loss did not improve from 1.10771
Epoch 4/32
203/203 [==============================] - 89s 440ms/step - loss: 0.5180 - acc: 0.7995 - val_loss: 1.3223 - val_acc: 0.5892

Epoch 00004: val_loss did not improve from 1.10771
Epoch 5/32
203/203 [==============================] - 89s 438ms/step - loss: 0.4720 - acc: 0.8179 - val_loss: 1.3878 - val_acc: 0.5802

Epoch 00005: val_loss did not improve from 1.10771
Epoch 6/32
203/203 [==============================] - 89s 440ms/step - loss: 0.4319 - acc: 0.8359 - val_loss: 1.4443 - val_acc: 0.5740

Epoch 00006: val_loss did not improve from 1.10771
Epoch 7/32
203/203 [==============================] - 89s 441ms/step - loss: 0.3887 - acc: 0.8548 - val_loss: 1.4806 - val_acc: 0.5864

Epoch 00007: val_loss did not improve from 1.10771
Epoch 8/32
 98/203 [=============>................] - ETA: 44s - loss: 0.3465 - acc: 0.8732KeyboardInterrupt
CPU times: user 24min 58s, sys: 2min 11s, total: 27min 10s
Wall time: 12min 35s
```

lr=0.001 coatreg3_batch64_ssd_crop_label.h5 l2=0.1
```
Epoch 1/32
203/203 [==============================] - 156s 768ms/step - loss: 1.9802 - acc: 0.3907 - val_loss: 1.6193 - val_acc: 0.4170

Epoch 00001: val_loss improved from inf to 1.61933, saving model to models/coatreg3_batch64_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 438ms/step - loss: 1.4021 - acc: 0.4672 - val_loss: 1.3841 - val_acc: 0.4799

Epoch 00002: val_loss improved from 1.61933 to 1.38407, saving model to models/coatreg3_batch64_ssd_crop_label.h5
Epoch 3/32
203/203 [==============================] - 88s 434ms/step - loss: 1.2999 - acc: 0.5009 - val_loss: 1.2873 - val_acc: 0.4986

Epoch 00003: val_loss improved from 1.38407 to 1.28733, saving model to models/coatreg3_batch64_ssd_crop_label.h5
Epoch 4/32
203/203 [==============================] - 88s 434ms/step - loss: 1.2191 - acc: 0.5400 - val_loss: 1.4767 - val_acc: 0.4592

Epoch 00004: val_loss did not improve from 1.28733
Epoch 5/32
203/203 [==============================] - 89s 437ms/step - loss: 1.1859 - acc: 0.5519 - val_loss: 1.3980 - val_acc: 0.5014

Epoch 00005: val_loss did not improve from 1.28733
Epoch 6/32
203/203 [==============================] - 88s 436ms/step - loss: 1.1337 - acc: 0.5730 - val_loss: 1.2579 - val_acc: 0.5297

Epoch 00006: val_loss improved from 1.28733 to 1.25786, saving model to models/coatreg3_batch64_ssd_crop_label.h5
Epoch 7/32
203/203 [==============================] - 88s 435ms/step - loss: 1.0925 - acc: 0.5928 - val_loss: 1.2249 - val_acc: 0.5450

Epoch 00007: val_loss improved from 1.25786 to 1.22486, saving model to models/coatreg3_batch64_ssd_crop_label.h5
Epoch 8/32
203/203 [==============================] - 88s 436ms/step - loss: 1.0486 - acc: 0.6147 - val_loss: 1.3216 - val_acc: 0.5270

Epoch 00008: val_loss did not improve from 1.22486
Epoch 9/32
203/203 [==============================] - 89s 437ms/step - loss: 0.9916 - acc: 0.6330 - val_loss: 1.3992 - val_acc: 0.4938

Epoch 00009: val_loss did not improve from 1.22486
Epoch 10/32
203/203 [==============================] - 89s 437ms/step - loss: 0.9804 - acc: 0.6351 - val_loss: 1.3540 - val_acc: 0.5221

Epoch 00010: val_loss did not improve from 1.22486
Epoch 11/32
203/203 [==============================] - 88s 436ms/step - loss: 0.9363 - acc: 0.6572 - val_loss: 1.2054 - val_acc: 0.5678

Epoch 00011: val_loss improved from 1.22486 to 1.20540, saving model to models/coatreg3_batch64_ssd_crop_label.h5
Epoch 12/32
203/203 [==============================] - 88s 435ms/step - loss: 0.8788 - acc: 0.6800 - val_loss: 1.2685 - val_acc: 0.5311

Epoch 00012: val_loss did not improve from 1.20540
Epoch 13/32
203/203 [==============================] - 88s 436ms/step - loss: 0.8291 - acc: 0.7029 - val_loss: 1.2922 - val_acc: 0.5622

Epoch 00013: val_loss did not improve from 1.20540
Epoch 14/32
203/203 [==============================] - 88s 436ms/step - loss: 0.8036 - acc: 0.7154 - val_loss: 1.4115 - val_acc: 0.5221

Epoch 00014: val_loss did not improve from 1.20540
Epoch 15/32
203/203 [==============================] - 89s 437ms/step - loss: 0.7577 - acc: 0.7357 - val_loss: 1.3479 - val_acc: 0.5443

Epoch 00015: val_loss did not improve from 1.20540
Epoch 16/32
203/203 [==============================] - 88s 436ms/step - loss: 0.7323 - acc: 0.7495 - val_loss: 1.3617 - val_acc: 0.5450

Epoch 00016: val_loss did not improve from 1.20540
Epoch 17/32
 89/203 [============>.................] - ETA: 48s - loss: 0.6730 - acc: 0.7749KeyboardInterrupt
CPU times: user 52min 50s, sys: 4min 50s, total: 57min 40s
Wall time: 25min 50s

```
lr=0.0001 coatreg4_batch64_ssd_crop_label.h5 l2=0.1
```

```


- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 有了预处理 没有padding直接resize 成150*150
- 
- 模型文件 ：
```
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2_copy/{0}'.format(df_load['image_id'][i])) 
    #row, column, channels = img.shape
    #max_value = max(row, column)
    #pad_img= cv2.copyMakeBorder(img,0,(max_value-row),0,(max_value-column),cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(img, (width, width))
    #X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',
         #kernel_regularizer=regularizers.l2(0.01),
          #bias_regularizer=regularizers.l2(0.01)
         )(x)



model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 64

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 45, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'nearest',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}nopadding_batch64_ssd_crop_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_batch32_ssd_crop_label.h5'.format(prefix_cls),)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')    

```
lr=0.001 coatnopadding_batch64_ssd_crop_label.h5
```
Epoch 1/32
203/203 [==============================] - 148s 727ms/step - loss: 1.7551 - acc: 0.3174 - val_loss: 1.9054 - val_acc: 0.2946

Epoch 00001: val_loss improved from inf to 1.90537, saving model to models/coatnopadding_batch64_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 436ms/step - loss: 1.5281 - acc: 0.3973 - val_loss: 3.8790 - val_acc: 0.2801

Epoch 00002: val_loss did not improve from 1.90537
Epoch 3/32
203/203 [==============================] - 89s 437ms/step - loss: 1.4185 - acc: 0.4407 - val_loss: 1.6936 - val_acc: 0.3741

Epoch 00003: val_loss improved from 1.90537 to 1.69355, saving model to models/coatnopadding_batch64_ssd_crop_label.h5
Epoch 4/32
203/203 [==============================] - 89s 438ms/step - loss: 1.3540 - acc: 0.4557 - val_loss: 1.5679 - val_acc: 0.3893

Epoch 00004: val_loss improved from 1.69355 to 1.56785, saving model to models/coatnopadding_batch64_ssd_crop_label.h5
Epoch 5/32
203/203 [==============================] - 89s 436ms/step - loss: 1.2697 - acc: 0.4909 - val_loss: 1.3192 - val_acc: 0.4661

Epoch 00005: val_loss improved from 1.56785 to 1.31920, saving model to models/coatnopadding_batch64_ssd_crop_label.h5
Epoch 6/32
203/203 [==============================] - 89s 437ms/step - loss: 1.2103 - acc: 0.5156 - val_loss: 1.5025 - val_acc: 0.4378

Epoch 00006: val_loss did not improve from 1.31920
Epoch 7/32
203/203 [==============================] - 89s 437ms/step - loss: 1.1713 - acc: 0.5344 - val_loss: 1.4578 - val_acc: 0.4288

Epoch 00007: val_loss did not improve from 1.31920
Epoch 8/32
203/203 [==============================] - 89s 438ms/step - loss: 1.1320 - acc: 0.5528 - val_loss: 1.3199 - val_acc: 0.4744

Epoch 00008: val_loss did not improve from 1.31920
Epoch 9/32
203/203 [==============================] - 89s 438ms/step - loss: 1.1061 - acc: 0.5584 - val_loss: 1.3565 - val_acc: 0.4710

Epoch 00009: val_loss did not improve from 1.31920
Epoch 10/32
203/203 [==============================] - 89s 437ms/step - loss: 1.0415 - acc: 0.5847 - val_loss: 2.1034 - val_acc: 0.3437

Epoch 00010: val_loss did not improve from 1.31920
Epoch 11/32
203/203 [==============================] - 89s 438ms/step - loss: 1.0245 - acc: 0.5945 - val_loss: 1.6449 - val_acc: 0.4454

Epoch 00011: val_loss did not improve from 1.31920
Epoch 12/32
203/203 [==============================] - 89s 438ms/step - loss: 0.9818 - acc: 0.6038 - val_loss: 1.3554 - val_acc: 0.4758

Epoch 00012: val_loss did not improve from 1.31920
Epoch 13/32
203/203 [==============================] - 89s 437ms/step - loss: 0.9696 - acc: 0.6168 - val_loss: 2.4876 - val_acc: 0.3845

Epoch 00013: val_loss did not improve from 1.31920
Epoch 14/32
203/203 [==============================] - 89s 438ms/step - loss: 0.9201 - acc: 0.6382 - val_loss: 1.5186 - val_acc: 0.4779

Epoch 00014: val_loss did not improve from 1.31920
Epoch 15/32
 18/203 [=>............................] - ETA: 1:17 - loss: 0.8896 - acc: 0.6215KeyboardInterrupt
CPU times: user 45min 34s, sys: 4min 5s, total: 49min 40s
Wall time: 22min 14s
```

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 上边和左边没有padding
- 
- 模型文件 ：
```
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2_copy/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,0,(max_value-row),0,(max_value-column),cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (width, width), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y[i][tmp_label.find('y')] = 1


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

x_train_path='data-raw/train_2/{}X_train_150_2_clip_w.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_150_2_clip_w.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_150_2_clip_w.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_150_2_clip_w.bc'.format(prefix_cls)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

epochs = 32
batch_size = 64

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 45, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.0,
        fill_mode = 'nearest',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}new_batch64_ssd_crop_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_batch32_ssd_crop_label.h5'.format(prefix_cls),)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

lr=0.001 neck_new_batch64_ssd_crop_label.h5

```
`Epoch 1/32
203/203 [==============================] - 93s 457ms/step - loss: 1.6384 - acc: 0.3605 - val_loss: 3.6001 - val_acc: 0.2234

Epoch 00001: val_loss improved from inf to 3.60009, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 437ms/step - loss: 1.5220 - acc: 0.4055 - val_loss: 1.4518 - val_acc: 0.4454

Epoch 00002: val_loss improved from 3.60009 to 1.45180, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 3/32
203/203 [==============================] - 89s 440ms/step - loss: 1.4213 - acc: 0.4365 - val_loss: 2.4564 - val_acc: 0.3970

Epoch 00003: val_loss did not improve from 1.45180
Epoch 4/32
203/203 [==============================] - 89s 438ms/step - loss: 1.3650 - acc: 0.4564 - val_loss: 1.4631 - val_acc: 0.4467

Epoch 00004: val_loss did not improve from 1.45180
Epoch 5/32
203/203 [==============================] - 89s 438ms/step - loss: 1.3222 - acc: 0.4671 - val_loss: 1.3427 - val_acc: 0.4661

Epoch 00005: val_loss improved from 1.45180 to 1.34266, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 6/32
203/203 [==============================] - 89s 440ms/step - loss: 1.3063 - acc: 0.4833 - val_loss: 1.2809 - val_acc: 0.5014

Epoch 00006: val_loss improved from 1.34266 to 1.28091, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 7/32
203/203 [==============================] - 89s 439ms/step - loss: 1.2601 - acc: 0.5074 - val_loss: 1.2652 - val_acc: 0.4889

Epoch 00007: val_loss improved from 1.28091 to 1.26524, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 8/32
203/203 [==============================] - 89s 438ms/step - loss: 1.2248 - acc: 0.5053 - val_loss: 1.3162 - val_acc: 0.4675

Epoch 00008: val_loss did not improve from 1.26524
Epoch 9/32
203/203 [==============================] - 89s 439ms/step - loss: 1.2097 - acc: 0.5188 - val_loss: 1.2676 - val_acc: 0.4972

Epoch 00009: val_loss did not improve from 1.26524
Epoch 10/32
203/203 [==============================] - 89s 440ms/step - loss: 1.1698 - acc: 0.5351 - val_loss: 1.2157 - val_acc: 0.5138

Epoch 00010: val_loss improved from 1.26524 to 1.21571, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 11/32
203/203 [==============================] - 89s 439ms/step - loss: 1.1376 - acc: 0.5403 - val_loss: 1.2634 - val_acc: 0.5041

Epoch 00011: val_loss did not improve from 1.21571
Epoch 12/32
203/203 [==============================] - 89s 440ms/step - loss: 1.1639 - acc: 0.5406 - val_loss: 1.1863 - val_acc: 0.5284

Epoch 00012: val_loss improved from 1.21571 to 1.18632, saving model to models/coatnew_batch64_ssd_crop_label.h5
Epoch 13/32
203/203 [==============================] - 89s 439ms/step - loss: 1.1780 - acc: 0.5335 - val_loss: 6.6606 - val_acc: 0.1591

Epoch 00013: val_loss did not improve from 1.18632
Epoch 14/32
203/203 [==============================] - 90s 442ms/step - loss: 1.4438 - acc: 0.4439 - val_loss: 1.3348 - val_acc: 0.4744

Epoch 00014: val_loss did not improve from 1.18632
Epoch 15/32
 21/203 [==>...........................] - ETA: 1:16 - loss: 1.3682 - acc: 0.4479KeyboardInterrupt
CPU times: user 44min 32s, sys: 4min 6s, total: 48min 38s
Wall time: 21min 12s``

```

lr=0.0001

```


```

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 ssd crop 150*150 有了预处理前提 

- 此处batch_size=32  or batch_size =64

- 模型文件 ：
```
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2_copy/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (width, width), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

x_train_path='data-raw/train_2/{}X_train_150_clip_w.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_150_clip_w.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_150_clip_w.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_150_clip_w.bc'.format(prefix_cls)

save_array(x_train_path,X_train)
save_array(x_valid_path,X_valid)
save_array(y_train_path,y_train)
save_array(y_valid_path,y_valid)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'nearest',
        cval = 0)


#datagen.fit(X_train,seed=123)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_ssd_crop_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

lr=0.001 coat_ssd_crop_label.h5
```
Epoch 1/32
406/406 [==============================] - 200s 494ms/step - loss: 1.6238 - acc: 0.3608 - val_loss: 1.6610 - val_acc: 0.4094

Epoch 00001: val_loss improved from inf to 1.66095, saving model to models/coat_ssd_crop_label.h5
Epoch 2/32
406/406 [==============================] - 143s 352ms/step - loss: 1.4373 - acc: 0.4252 - val_loss: 2.5731 - val_acc: 0.3513

Epoch 00002: val_loss did not improve from 1.66095
Epoch 3/32
406/406 [==============================] - 142s 351ms/step - loss: 1.3568 - acc: 0.4573 - val_loss: 1.6054 - val_acc: 0.4447

Epoch 00003: val_loss improved from 1.66095 to 1.60537, saving model to models/coat_ssd_crop_label.h5
Epoch 4/32
406/406 [==============================] - 143s 351ms/step - loss: 1.2806 - acc: 0.4884 - val_loss: 1.5784 - val_acc: 0.4502

Epoch 00004: val_loss improved from 1.60537 to 1.57842, saving model to models/coat_ssd_crop_label.h5
Epoch 5/32
406/406 [==============================] - 143s 352ms/step - loss: 1.2604 - acc: 0.5050 - val_loss: 1.4983 - val_acc: 0.4723

Epoch 00005: val_loss improved from 1.57842 to 1.49833, saving model to models/coat_ssd_crop_label.h5
Epoch 6/32
406/406 [==============================] - 143s 352ms/step - loss: 1.2090 - acc: 0.5186 - val_loss: 1.1309 - val_acc: 0.5463

Epoch 00006: val_loss improved from 1.49833 to 1.13095, saving model to models/coat_ssd_crop_label.h5
Epoch 7/32
406/406 [==============================] - 143s 351ms/step - loss: 1.1527 - acc: 0.5407 - val_loss: 1.2857 - val_acc: 0.5000

Epoch 00007: val_loss did not improve from 1.13095
Epoch 8/32
406/406 [==============================] - 143s 352ms/step - loss: 1.1420 - acc: 0.5436 - val_loss: 1.3260 - val_acc: 0.4820

Epoch 00008: val_loss did not improve from 1.13095
Epoch 9/32
406/406 [==============================] - 143s 352ms/step - loss: 1.0836 - acc: 0.5661 - val_loss: 1.1542 - val_acc: 0.5574

Epoch 00009: val_loss did not improve from 1.13095
Epoch 10/32
406/406 [==============================] - 143s 352ms/step - loss: 1.0452 - acc: 0.5800 - val_loss: 1.2947 - val_acc: 0.5242

Epoch 00010: val_loss did not improve from 1.13095
Epoch 11/32
406/406 [==============================] - 143s 352ms/step - loss: 1.0118 - acc: 0.5947 - val_loss: 1.2220 - val_acc: 0.5263

Epoch 00011: val_loss did not improve from 1.13095
Epoch 12/32
406/406 [==============================] - 143s 351ms/step - loss: 1.0132 - acc: 0.5954 - val_loss: 1.4186 - val_acc: 0.5228

Epoch 00012: val_loss did not improve from 1.13095
Epoch 13/32
222/406 [===============>..............] - ETA: 1:03 - loss: 1.0054 - acc: 0.6080KeyboardInterrupt
CPU times: user 1h 1min 53s, sys: 4min 33s, total: 1h 6min 26s
Wall time: 31min 13s

```

lr=1e-4 coat2_ssd_crop_label.h5
```
Epoch 1/32
406/406 [==============================] - 201s 494ms/step - loss: 1.0330 - acc: 0.5831 - val_loss: 1.0385 - val_acc: 0.5892

Epoch 00001: val_loss improved from inf to 1.03852, saving model to models/coat2_ssd_crop_label.h5
Epoch 2/32
406/406 [==============================] - 143s 353ms/step - loss: 0.9619 - acc: 0.6106 - val_loss: 1.0363 - val_acc: 0.5947

Epoch 00002: val_loss improved from 1.03852 to 1.03625, saving model to models/coat2_ssd_crop_label.h5
Epoch 3/32
406/406 [==============================] - 144s 354ms/step - loss: 0.9266 - acc: 0.6230 - val_loss: 1.0374 - val_acc: 0.5927

Epoch 00003: val_loss did not improve from 1.03625
Epoch 4/32
 93/406 [=====>........................] - ETA: 1:47 - loss: 0.8805 - acc: 0.6405KeyboardInterrupt
CPU times: user 17min 10s, sys: 1min 12s, total: 18min 22s
Wall time: 9min 12s


```

lr=1e-5 coat3_ssd_crop_label.h5
```



```

lr=0.001 coat_batch32_ssd_crop_label.h5

 batch_size=64

```
Epoch 1/32
203/203 [==============================] - 146s 722ms/step - loss: 1.5440 - acc: 0.3953 - val_loss: 1.6114 - val_acc: 0.4094

Epoch 00001: val_loss improved from inf to 1.61138, saving model to models/coat_batch32_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 436ms/step - loss: 1.3405 - acc: 0.4649 - val_loss: 1.4484 - val_acc: 0.4329

Epoch 00002: val_loss improved from 1.61138 to 1.44840, saving model to models/coat_batch32_ssd_crop_label.h5
Epoch 3/32
203/203 [==============================] - 89s 438ms/step - loss: 1.2435 - acc: 0.5018 - val_loss: 1.3078 - val_acc: 0.4993

Epoch 00003: val_loss improved from 1.44840 to 1.30777, saving model to models/coat_batch32_ssd_crop_label.h5
Epoch 4/32
203/203 [==============================] - 89s 437ms/step - loss: 1.1503 - acc: 0.5383 - val_loss: 1.2153 - val_acc: 0.5187

Epoch 00004: val_loss improved from 1.30777 to 1.21527, saving model to models/coat_batch32_ssd_crop_label.h5
Epoch 5/32
203/203 [==============================] - 88s 436ms/step - loss: 1.0846 - acc: 0.5633 - val_loss: 1.1373 - val_acc: 0.5380

Epoch 00005: val_loss improved from 1.21527 to 1.13734, saving model to models/coat_batch32_ssd_crop_label.h5
Epoch 6/32
203/203 [==============================] - 89s 438ms/step - loss: 1.0358 - acc: 0.5863 - val_loss: 1.2329 - val_acc: 0.5138

Epoch 00006: val_loss did not improve from 1.13734
Epoch 7/32
203/203 [==============================] - 89s 437ms/step - loss: 0.9965 - acc: 0.5978 - val_loss: 1.9376 - val_acc: 0.4640

Epoch 00007: val_loss did not improve from 1.13734
Epoch 8/32
203/203 [==============================] - 88s 436ms/step - loss: 0.9645 - acc: 0.6075 - val_loss: 1.1857 - val_acc: 0.5387

Epoch 00008: val_loss did not improve from 1.13734
Epoch 9/32
203/203 [==============================] - 89s 436ms/step - loss: 0.9053 - acc: 0.6384 - val_loss: 1.4856 - val_acc: 0.5256

Epoch 00009: val_loss did not improve from 1.13734
Epoch 10/32
203/203 [==============================] - 89s 436ms/step - loss: 0.8632 - acc: 0.6485 - val_loss: 1.1248 - val_acc: 0.5609

Epoch 00010: val_loss improved from 1.13734 to 1.12479, saving model to models/coat_batch32_ssd_crop_label.h5
Epoch 11/32
 33/203 [===>..........................] - ETA: 1:10 - loss: 0.7993 - acc: 0.6818KeyboardInterrupt
CPU times: user 33min, sys: 3min 3s, total: 36min 3s
Wall time: 16min 26s

```


lr=0.001 coat2__batch32_ssd_crop_label.h5 batch_size=64 目的训练准确率


```
Epoch 1/32
203/203 [==============================] - 89s 438ms/step - loss: 0.8739 - acc: 0.6472 - val_loss: 2.2107 - val_acc: 0.4827

Epoch 00001: val_loss improved from inf to 2.21066, saving model to models/coat2_batch64_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 88s 436ms/step - loss: 0.8202 - acc: 0.6670 - val_loss: 1.2524 - val_acc: 0.5456

Epoch 00002: val_loss improved from 2.21066 to 1.25240, saving model to models/coat2_batch64_ssd_crop_label.h5
Epoch 3/32
203/203 [==============================] - 89s 436ms/step - loss: 0.7339 - acc: 0.7036 - val_loss: 1.3285 - val_acc: 0.5491

Epoch 00003: val_loss did not improve from 1.25240
Epoch 4/32
203/203 [==============================] - 89s 437ms/step - loss: 0.6738 - acc: 0.7300 - val_loss: 1.3179 - val_acc: 0.5477

Epoch 00004: val_loss did not improve from 1.25240
Epoch 5/32
203/203 [==============================] - 89s 437ms/step - loss: 0.6325 - acc: 0.7480 - val_loss: 1.2761 - val_acc: 0.5602

Epoch 00005: val_loss did not improve from 1.25240
Epoch 6/32
203/203 [==============================] - 89s 437ms/step - loss: 0.5927 - acc: 0.7631 - val_loss: 1.3648 - val_acc: 0.5491

Epoch 00006: val_loss did not improve from 1.25240
Epoch 7/32
203/203 [==============================] - 88s 436ms/step - loss: 0.5545 - acc: 0.7797 - val_loss: 1.4112 - val_acc: 0.5360

Epoch 00007: val_loss did not improve from 1.25240
Epoch 8/32
203/203 [==============================] - 89s 437ms/step - loss: 0.5072 - acc: 0.8008 - val_loss: 1.4594 - val_acc: 0.5422

Epoch 00008: val_loss did not improve from 1.25240
Epoch 9/32
203/203 [==============================] - 88s 436ms/step - loss: 0.4720 - acc: 0.8117 - val_loss: 1.5870 - val_acc: 0.5394

Epoch 00009: val_loss did not improve from 1.25240
Epoch 10/32
203/203 [==============================] - 88s 436ms/step - loss: 0.4447 - acc: 0.8239 - val_loss: 1.5641 - val_acc: 0.5353

Epoch 00010: val_loss did not improve from 1.25240
Epoch 11/32
203/203 [==============================] - 89s 437ms/step - loss: 0.4264 - acc: 0.8392 - val_loss: 1.5385 - val_acc: 0.5408

Epoch 00011: val_loss did not improve from 1.25240
Epoch 12/32
203/203 [==============================] - 89s 438ms/step - loss: 0.3785 - acc: 0.8497 - val_loss: 1.6667 - val_acc: 0.5270

Epoch 00012: val_loss did not improve from 1.25240
Epoch 13/32
203/203 [==============================] - 89s 438ms/step - loss: 0.3643 - acc: 0.8605 - val_loss: 1.7057 - val_acc: 0.5304

Epoch 00013: val_loss did not improve from 1.25240
Epoch 14/32
203/203 [==============================] - 89s 437ms/step - loss: 0.3332 - acc: 0.8732 - val_loss: 1.7322 - val_acc: 0.5463

Epoch 00014: val_loss did not improve from 1.25240
Epoch 15/32
203/203 [==============================] - 89s 438ms/step - loss: 0.3211 - acc: 0.8803 - val_loss: 1.8168 - val_acc: 0.5277

Epoch 00015: val_loss did not improve from 1.25240
Epoch 16/32
203/203 [==============================] - 89s 438ms/step - loss: 0.3086 - acc: 0.8816 - val_loss: 1.8284 - val_acc: 0.5512

Epoch 00016: val_loss did not improve from 1.25240
Epoch 17/32
203/203 [==============================] - 89s 437ms/step - loss: 0.2781 - acc: 0.8942 - val_loss: 1.8943 - val_acc: 0.5277

Epoch 00017: val_loss did not improve from 1.25240
Epoch 18/32
203/203 [==============================] - 89s 436ms/step - loss: 0.2567 - acc: 0.9029 - val_loss: 2.6597 - val_acc: 0.4654

Epoch 00018: val_loss did not improve from 1.25240
Epoch 19/32
 54/203 [======>.......................] - ETA: 1:02 - loss: 0.3222 - acc: 0.8802KeyboardInterrupt
CPU times: user 56min 58s, sys: 5min 21s, total: 1h 2min 20s
Wall time: 27min 8s

```
lr=0.0001 coat3__batch64_ssd_crop_label.h5 batch_size=64 目的训练准确率
```
Epoch 1/32
203/203 [==============================] - 143s 707ms/step - loss: 0.1801 - acc: 0.9326 - val_loss: 1.9400 - val_acc: 0.5636

Epoch 00001: val_loss improved from inf to 1.93995, saving model to models/coat3_batch32_ssd_crop_label.h5
Epoch 2/32
203/203 [==============================] - 89s 437ms/step - loss: 0.1141 - acc: 0.9582 - val_loss: 2.2611 - val_acc: 0.5698

Epoch 00002: val_loss did not improve from 1.93995
Epoch 3/32
203/203 [==============================] - 89s 440ms/step - loss: 0.0970 - acc: 0.9620 - val_loss: 2.3796 - val_acc: 0.5602

Epoch 00003: val_loss did not improve from 1.93995
Epoch 4/32
203/203 [==============================] - 89s 441ms/step - loss: 0.0821 - acc: 0.9708 - val_loss: 2.4763 - val_acc: 0.5595

Epoch 00004: val_loss did not improve from 1.93995
Epoch 5/32
203/203 [==============================] - 89s 440ms/step - loss: 0.0654 - acc: 0.9756 - val_loss: 2.8305 - val_acc: 0.5505

Epoch 00005: val_loss did not improve from 1.93995
Epoch 6/32
203/203 [==============================] - 89s 438ms/step - loss: 0.0613 - acc: 0.9761 - val_loss: 2.8817 - val_acc: 0.5498

Epoch 00006: val_loss did not improve from 1.93995
Epoch 7/32
203/203 [==============================] - 89s 440ms/step - loss: 0.0557 - acc: 0.9789 - val_loss: 2.8695 - val_acc: 0.5526

Epoch 00007: val_loss did not improve from 1.93995
Epoch 8/32
203/203 [==============================] - 89s 440ms/step - loss: 0.0560 - acc: 0.9800 - val_loss: 2.9173 - val_acc: 0.5491

Epoch 00008: val_loss did not improve from 1.93995
Epoch 9/32
 17/203 [=>............................] - ETA: 1:18 - loss: 0.0364 - acc: 0.9853KeyboardInterrupt
CPU times: user 26min 29s, sys: 2min 25s, total: 28min 54s
Wall time: 13min 16s

```

lr=0.00001 coat4__batch64_ssd_crop_label.h5 batch_size=64 目的训练准确率

```
Epoch 1/32
229/229 [==============================] - 248s 1s/step - loss: 0.3065 - acc: 0.8920 - val_loss: 0.4593 - val_acc: 0.8431

Epoch 00001: val_loss improved from inf to 0.45926, saving model to models/neck4_clip_w_h_resnet_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 176s 769ms/step - loss: 0.3052 - acc: 0.8937 - val_loss: 0.4602 - val_acc: 0.8456

Epoch 00002: val_loss did not improve from 0.45926
Epoch 3/32
 62/229 [=======>......................] - ETA: 2:02 - loss: 0.3125 - acc: 0.8853KeyboardInterrupt
CPU times: user 15min 10s, sys: 1min 16s, total: 16min 27s
Wall time: 8min 31s
```

2018.5.22

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPUmu
- keyWords ：InceptionResNetV2  多标签 muti_label
- 
- 模型文件 ：

```
df_train = pd.read_csv('data-raw/train_2/Annotations/label_2.csv')
#df_train.columns = ['image_id', 'class', 'label']
df_train.head()

classes = ['collar_design_labels', 'neckline_design_labels', 'skirt_length_labels',
          'sleeve_length_labels', 'neck_design_labels', 'coat_length_labels', 'lapel_design_labels',
          'pant_length_labels']

cur_class = classes[5]
df_load = df_train[(df_train['class'] == cur_class)].copy()
df_load.reset_index(inplace=True)
del df_load['index']

print('{0}: {1}'.format(cur_class, len(df_load)))
df_load.head()

n = len(df_load)
n_class = len(df_load['label'][0])
width = 299 # 定义图片大小

X = np.zeros((n, width, width, 3), dtype=np.uint8)
y = np.zeros((n, n_class+1), dtype=np.uint8)
prefix_cls = cur_class.split('_')[0]

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    #X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y[i][tmp_label.find('y')] = 1
    y[i][-1]=int(df_load['human'][i])


cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class+1, activation='sigmoid', name='sigmoid')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_mult_label.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')



```
lr=0.001
```
Epoch 1/32
406/406 [==============================] - 397s 978ms/step - loss: 0.2885 - acc: 0.8836 - val_loss: 0.2639 - val_acc: 0.8887

Epoch 00001: val_loss improved from inf to 0.26392, saving model to models/coat_mult_label.h5
Epoch 2/32
406/406 [==============================] - 315s 776ms/step - loss: 0.2472 - acc: 0.8977 - val_loss: 0.2957 - val_acc: 0.8723

Epoch 00002: val_loss did not improve from 0.26392
Epoch 3/32
406/406 [==============================] - 315s 776ms/step - loss: 0.2298 - acc: 0.9025 - val_loss: 0.2516 - val_acc: 0.8981

Epoch 00003: val_loss improved from 0.26392 to 0.25161, saving model to models/coat_mult_label.h5
Epoch 4/32
 71/406 [====>.........................] - ETA: 4:09 - loss: 0.2245 - acc: 0.9052KeyboardInterrupt
CPU times: user 34min 53s, sys: 3min 11s, total: 38min 4s
Wall time: 18min 31s
```

lr=0.0001
```
Epoch 1/32
406/406 [==============================] - 389s 959ms/step - loss: 0.1873 - acc: 0.9185 - val_loss: 0.1915 - val_acc: 0.9183

Epoch 00001: val_loss improved from inf to 0.19153, saving model to models/coat2_mult_label.h5
Epoch 2/32
406/406 [==============================] - 318s 782ms/step - loss: 0.1688 - acc: 0.9261 - val_loss: 0.1894 - val_acc: 0.9189

Epoch 00002: val_loss improved from 0.19153 to 0.18938, saving model to models/coat2_mult_label.h5
Epoch 3/32
406/406 [==============================] - 316s 779ms/step - loss: 0.1564 - acc: 0.9317 - val_loss: 0.1920 - val_acc: 0.9195

Epoch 00003: val_loss did not improve from 0.18938
Epoch 4/32
406/406 [==============================] - 316s 779ms/step - loss: 0.1451 - acc: 0.9372 - val_loss: 0.1972 - val_acc: 0.9187

Epoch 00004: val_loss did not improve from 0.18938
Epoch 5/32
406/406 [==============================] - 317s 781ms/step - loss: 0.1332 - acc: 0.9425 - val_loss: 0.2071 - val_acc: 0.9182

Epoch 00005: val_loss did not improve from 0.18938
Epoch 6/32
 12/406 [..............................] - ETA: 4:53 - loss: 0.1273 - acc: 0.9413KeyboardInterrupt
CPU times: user 54min 49s, sys: 5min, total: 59min 49s
Wall time: 28min 15s

```
2018.5.21

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：

```
#首先进行填充，保持图片的比例不变，然后再进行缩放
crip_width_start=int(width*0.25)
crip_width_end=int(width*0.75)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    img=cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])) 
    row, column, channels = img.shape
    max_value = max(row, column)
    pad_img= cv2.copyMakeBorder(img,(max_value-row)//2,(max_value-row)//2,(max_value-column)//2,(max_value-column)//2,cv2.BORDER_CONSTANT)
    X[i] = cv2.resize(pad_img, (299, 299), interpolation=cv2.INTER_AREA)
    X_clip[i] = X[i][:,crip_width_start:crip_width_end]
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, 150, 3), weights='imagenet')

inputs = Input((width, 150, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X_clip, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

x_train_path='data-raw/train_2/{}X_train_299_150_clip_w.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_299_150_clip_w.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_299_150_clip_w.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_299_150_clip_w.bc'.format(prefix_cls)

save_array(x_train_path,X_train)
save_array(x_valid_path,X_valid)
save_array(y_train_path,y_train)
save_array(y_valid_path,y_valid)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_299_150_resnet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```
lr=0.001 coat_clip_299_150_resnet_all_mem_weights.h5
注意每个epcoh训练的时间变小了

```
Epoch 1/32
406/406 [==============================] - 252s 621ms/step - loss: 1.5539 - acc: 0.3835 - val_loss: 1.5044 - val_acc: 0.4011

Epoch 00001: val_loss improved from inf to 1.50442, saving model to models/coat_clip_299_150_resnet_all_mem_weights.h5
Epoch 2/32
406/406 [==============================] - 191s 471ms/step - loss: 1.3256 - acc: 0.4718 - val_loss: 1.2847 - val_acc: 0.4924

Epoch 00002: val_loss improved from 1.50442 to 1.28466, saving model to models/coat_clip_299_150_resnet_all_mem_weights.h5
Epoch 3/32
406/406 [==============================] - 191s 470ms/step - loss: 1.2161 - acc: 0.5109 - val_loss: 1.1916 - val_acc: 0.5242

Epoch 00003: val_loss improved from 1.28466 to 1.19157, saving model to models/coat_clip_299_150_resnet_all_mem_weights.h5
Epoch 4/32
406/406 [==============================] - 191s 472ms/step - loss: 1.1406 - acc: 0.5440 - val_loss: 1.8919 - val_acc: 0.4723

Epoch 00004: val_loss did not improve from 1.19157
Epoch 5/32
406/406 [==============================] - 191s 470ms/step - loss: 1.1022 - acc: 0.5575 - val_loss: 1.1221 - val_acc: 0.5422

Epoch 00005: val_loss improved from 1.19157 to 1.12210, saving model to models/coat_clip_299_150_resnet_all_mem_weights.h5
Epoch 6/32
406/406 [==============================] - 191s 470ms/step - loss: 1.0140 - acc: 0.5911 - val_loss: 1.0673 - val_acc: 0.5657

Epoch 00006: val_loss improved from 1.12210 to 1.06733, saving model to models/coat_clip_299_150_resnet_all_mem_weights.h5
Epoch 7/32
406/406 [==============================] - 191s 470ms/step - loss: 0.9700 - acc: 0.6115 - val_loss: 1.1235 - val_acc: 0.5546

Epoch 00007: val_loss did not improve from 1.06733
Epoch 8/32
406/406 [==============================] - 191s 472ms/step - loss: 0.9127 - acc: 0.6365 - val_loss: 1.0685 - val_acc: 0.5671

Epoch 00008: val_loss did not improve from 1.06733
Epoch 9/32
406/406 [==============================] - 190s 469ms/step - loss: 0.8652 - acc: 0.6527 - val_loss: 1.0568 - val_acc: 0.5809

Epoch 00009: val_loss improved from 1.06733 to 1.05684, saving model to models/coat_clip_299_150_resnet_all_mem_weights.h5
Epoch 10/32
406/406 [==============================] - 191s 470ms/step - loss: 0.8266 - acc: 0.6649 - val_loss: 1.1573 - val_acc: 0.5692

Epoch 00010: val_loss did not improve from 1.05684
Epoch 11/32
406/406 [==============================] - 190s 469ms/step - loss: 0.7818 - acc: 0.6850 - val_loss: 1.1354 - val_acc: 0.5539

Epoch 00011: val_loss did not improve from 1.05684
Epoch 12/32
406/406 [==============================] - 191s 470ms/step - loss: 0.7336 - acc: 0.7060 - val_loss: 1.1658 - val_acc: 0.5754

Epoch 00012: val_loss did not improve from 1.05684
Epoch 13/32
 61/406 [===>..........................] - ETA: 2:36 - loss: 0.6558 - acc: 0.7469KeyboardInterrupt
CPU times: user 1h 19min 22s, sys: 5min 17s, total: 1h 24min 40s
Wall time: 40min 9s

```
lr=0.001 coat2_clip_299_150_resnet_all_mem_weights.h5
```
Epoch 1/32
406/406 [==============================] - 268s 660ms/step - loss: 0.8242 - acc: 0.6671 - val_loss: 1.0993 - val_acc: 0.5712

Epoch 00001: val_loss improved from inf to 1.09932, saving model to models/coat2_clip_299_150_resnet_all_mem_weights.h5
Epoch 2/32
406/406 [==============================] - 205s 504ms/step - loss: 0.7946 - acc: 0.6752 - val_loss: 1.1124 - val_acc: 0.5719

Epoch 00002: val_loss did not improve from 1.09932
Epoch 3/32
406/406 [==============================] - 203s 500ms/step - loss: 0.7534 - acc: 0.6994 - val_loss: 1.7675 - val_acc: 0.5353

Epoch 00003: val_loss did not improve from 1.09932
Epoch 4/32
406/406 [==============================] - 202s 498ms/step - loss: 0.6921 - acc: 0.7207 - val_loss: 1.2604 - val_acc: 0.5560

Epoch 00004: val_loss did not improve from 1.09932
Epoch 5/32
406/406 [==============================] - 203s 500ms/step - loss: 0.6496 - acc: 0.7394 - val_loss: 1.1573 - val_acc: 0.5816

Epoch 00005: val_loss did not improve from 1.09932
Epoch 6/32
 31/406 [=>............................] - ETA: 3:00 - loss: 0.5748 - acc: 0.7621KeyboardInterrupt
CPU times: user 35min 59s, sys: 2min 33s, total: 38min 33s
Wall time: 18min 52s
```
lr=0.0001 coat3_clip_299_150_resnet_all_mem_weights.h5
```
Epoch 1/32
406/406 [==============================] - 256s 630ms/step - loss: 0.4241 - acc: 0.8347 - val_loss: 1.1892 - val_acc: 0.6183

Epoch 00001: val_loss improved from inf to 1.18924, saving model to models/coat3_clip_299_150_resnet_all_mem_weights.h5
Epoch 2/32
406/406 [==============================] - 205s 505ms/step - loss: 0.3392 - acc: 0.8655 - val_loss: 1.3260 - val_acc: 0.6169

Epoch 00002: val_loss did not improve from 1.18924
Epoch 3/32
406/406 [==============================] - 204s 502ms/step - loss: 0.2887 - acc: 0.8874 - val_loss: 1.4149 - val_acc: 0.6155

Epoch 00003: val_loss did not improve from 1.18924
Epoch 4/32
406/406 [==============================] - 203s 500ms/step - loss: 0.2540 - acc: 0.9015 - val_loss: 1.5270 - val_acc: 0.6093

Epoch 00004: val_loss did not improve from 1.18924
Epoch 5/32
 20/406 [>.............................] - ETA: 3:05 - loss: 0.2409 - acc: 0.9047KeyboardInterrupt
CPU times: user 28min 20s, sys: 2min 1s, total: 30min 22s
Wall time: 14min 59s

```


- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
=>

```
#crip_high=int(width*0.5)
crip_width_start=int(width*0.25)
crip_width_end=int(width*0.75)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])), (width, width))
    #X[i] = cv2.resize(X[i][:crip_high], (width, width))
    X[i] = cv2.resize(X[i][:,crip_width_start:crip_width_end], (width, width))
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

x_train_path='data-raw/train_2/{}X_train_299_clip_w.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_299_clip_w.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_299_clip_w.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_299_clip_w.bc'.format(prefix_cls)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

训练结果lr=0.001 neck_clip_w_h_resnet_all_mem_weights.h5
```
Epoch 1/32
406/406 [==============================] - 368s 906ms/step - loss: 1.5916 - acc: 0.3743 - val_loss: 1.7488 - val_acc: 0.3873

Epoch 00001: val_loss improved from inf to 1.74875, saving model to models/coat_clip_w_h_resnet_all_mem_weights.h5
Epoch 2/32
406/406 [==============================] - 308s 758ms/step - loss: 1.3797 - acc: 0.4477 - val_loss: 1.3567 - val_acc: 0.4467

Epoch 00002: val_loss improved from 1.74875 to 1.35670, saving model to models/coat_clip_w_h_resnet_all_mem_weights.h5
Epoch 3/32
406/406 [==============================] - 307s 756ms/step - loss: 1.2879 - acc: 0.4802 - val_loss: 1.3882 - val_acc: 0.4820

Epoch 00003: val_loss did not improve from 1.35670
Epoch 4/32
406/406 [==============================] - 307s 757ms/step - loss: 1.2050 - acc: 0.5180 - val_loss: 1.2494 - val_acc: 0.5000

Epoch 00004: val_loss improved from 1.35670 to 1.24938, saving model to models/coat_clip_w_h_resnet_all_mem_weights.h5
Epoch 5/32
406/406 [==============================] - 307s 756ms/step - loss: 1.1454 - acc: 0.5428 - val_loss: 1.1392 - val_acc: 0.5470

Epoch 00005: val_loss improved from 1.24938 to 1.13920, saving model to models/coat_clip_w_h_resnet_all_mem_weights.h5
Epoch 6/32
406/406 [==============================] - 307s 756ms/step - loss: 1.0848 - acc: 0.5668 - val_loss: 1.1810 - val_acc: 0.5512

Epoch 00006: val_loss did not improve from 1.13920
Epoch 7/32
406/406 [==============================] - 307s 756ms/step - loss: 1.0336 - acc: 0.5885 - val_loss: 1.4060 - val_acc: 0.4758

Epoch 00007: val_loss did not improve from 1.13920
Epoch 8/32
129/406 [========>.....................] - ETA: 3:20 - loss: 0.9766 - acc: 0.6105KeyboardInterrupt
CPU times: user 1h 15min 3s, sys: 6min 54s, total: 1h 21min 58s
Wall time: 38min 54s

```
lr=0.0001 neck2_clip_w_h_resnet_all_mem_weights.h5
```
效果不好，不继续探索
```




- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
=>neck_0521_84_resnet_clip_w_h.csv
**注意模型3的val_loss 比模型4的低，可能跟靠谱，之后可以尝试用模型3预测**
```
crip_high=int(width*0.5)
crip_width_start=int(width*0.25)
crip_width_end=int(width*0.75)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])), (width, width))
    X[i] = cv2.resize(X[i][:crip_high], (width, width))
    X[i] = cv2.resize(X[i][:][:,crip_width_start:crip_width_end], (width, width))
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

x_train_path='data-raw/train_2/{}X_train_299_clip_w_h.bc'.format(prefix_cls)
x_valid_path='data-raw/train_2/{}X_valid_299_clip_w_h.bc'.format(prefix_cls)
y_train_path='data-raw/train_2/{}y_train_299_clip_w_h.bc'.format(prefix_cls)
y_valid_path='data-raw/train_2/{}y_vaild_299_clip_w_h.bc'.format(prefix_cls)

save_array(x_train_path,X_train)
save_array(x_valid_path,X_valid)
save_array(y_train_path,y_train)
save_array(y_valid_path,y_valid)

X_train=load_array(x_train_path)
X_valid=load_array(x_valid_path)
y_train=load_array(y_train_path)
y_valid=load_array(y_valid_path)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_w_h_resnet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_resnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 neck_clip_w_h_resnet_all_mem_weights.h5
```
Epoch 1/32
229/229 [==============================] - 240s 1s/step - loss: 1.0974 - acc: 0.5768 - val_loss: 1.2942 - val_acc: 0.5870

Epoch 00001: val_loss improved from inf to 1.29417, saving model to models/neck_clip_w_h_resnet_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 175s 763ms/step - loss: 0.8674 - acc: 0.6742 - val_loss: 0.9795 - val_acc: 0.6262

Epoch 00002: val_loss improved from 1.29417 to 0.97948, saving model to models/neck_clip_w_h_resnet_all_mem_weights.h5
Epoch 3/32
229/229 [==============================] - 175s 763ms/step - loss: 0.7607 - acc: 0.7109 - val_loss: 1.0874 - val_acc: 0.6324

Epoch 00003: val_loss did not improve from 0.97948
Epoch 4/32
229/229 [==============================] - 175s 763ms/step - loss: 0.6857 - acc: 0.7505 - val_loss: 0.8548 - val_acc: 0.6850

Epoch 00004: val_loss improved from 0.97948 to 0.85478, saving model to models/neck_clip_w_h_resnet_all_mem_weights.h5
Epoch 5/32
229/229 [==============================] - 174s 762ms/step - loss: 0.6034 - acc: 0.7774 - val_loss: 0.6007 - val_acc: 0.7880

Epoch 00005: val_loss improved from 0.85478 to 0.60072, saving model to models/neck_clip_w_h_resnet_all_mem_weights.h5
Epoch 6/32
229/229 [==============================] - 175s 762ms/step - loss: 0.5629 - acc: 0.7945 - val_loss: 0.7139 - val_acc: 0.7353

Epoch 00006: val_loss did not improve from 0.60072
Epoch 7/32
229/229 [==============================] - 175s 762ms/step - loss: 0.5263 - acc: 0.8108 - val_loss: 0.6436 - val_acc: 0.7757

Epoch 00007: val_loss did not improve from 0.60072
Epoch 8/32
229/229 [==============================] - 174s 761ms/step - loss: 0.4767 - acc: 0.8276 - val_loss: 0.6718 - val_acc: 0.7586

Epoch 00008: val_loss did not improve from 0.60072
Epoch 9/32
 24/229 [==>...........................] - ETA: 2:27 - loss: 0.4865 - acc: 0.8260KeyboardInterrupt
CPU times: user 48min 32s, sys: 4min 21s, total: 52min 53s
Wall time: 25min 9s

```

lr=0.0001 neck2_clip_w_h_resnet_all_mem_weights.h5

```
Epoch 1/32
229/229 [==============================] - 233s 1s/step - loss: 0.4125 - acc: 0.8533 - val_loss: 0.4694 - val_acc: 0.8272

Epoch 00001: val_loss improved from inf to 0.46941, saving model to models/neck2_clip_w_h_resnet_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 174s 758ms/step - loss: 0.3220 - acc: 0.8867 - val_loss: 0.4899 - val_acc: 0.8272

Epoch 00002: val_loss did not improve from 0.46941
Epoch 3/32
229/229 [==============================] - 173s 757ms/step - loss: 0.2863 - acc: 0.8965 - val_loss: 0.4936 - val_acc: 0.8309

Epoch 00003: val_loss did not improve from 0.46941
Epoch 4/32
229/229 [==============================] - 174s 758ms/step - loss: 0.2418 - acc: 0.9166 - val_loss: 0.5258 - val_acc: 0.8370

Epoch 00004: val_loss did not improve from 0.46941
Epoch 5/32
 87/229 [==========>...................] - ETA: 1:43 - loss: 0.1878 - acc: 0.9407KeyboardInterrupt
CPU times: user 27min, sys: 2min 15s, total: 29min 16s
Wall time: 14min 11s

```

lr=0.00001 neck3_clip_w_h_resnet_all_mem_weights.h5
```
Epoch 1/32
229/229 [==============================] - 246s 1s/step - loss: 0.3236 - acc: 0.8901 - val_loss: 0.4637 - val_acc: 0.8395

Epoch 00001: val_loss improved from inf to 0.46375, saving model to models/neck3_clip_w_h_resnet_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 176s 768ms/step - loss: 0.3160 - acc: 0.8873 - val_loss: 0.4574 - val_acc: 0.8395

Epoch 00002: val_loss improved from 0.46375 to 0.45742, saving model to models/neck3_clip_w_h_resnet_all_mem_weights.h5
Epoch 3/32
229/229 [==============================] - 175s 766ms/step - loss: 0.3033 - acc: 0.8943 - val_loss: 0.4639 - val_acc: 0.8431

Epoch 00003: val_loss did not improve from 0.45742
Epoch 4/32
229/229 [==============================] - 175s 765ms/step - loss: 0.2891 - acc: 0.8982 - val_loss: 0.4670 - val_acc: 0.8419

Epoch 00004: val_loss did not improve from 0.45742
Epoch 5/32
 44/229 [====>.........................] - ETA: 2:15 - loss: 0.3067 - acc: 0.8849KeyboardInterrupt
CPU times: user 26min 17s, sys: 2min 18s, total: 28min 35s
Wall time: 14min 4s

```
lr=0.000001 neck4_clip_w_h_resnet_all_mem_weights.h5
=>neck_0521_84_resnet_clip_w_h.csv
```
Epoch 1/32
229/229 [==============================] - 248s 1s/step - loss: 0.3065 - acc: 0.8920 - val_loss: 0.4593 - val_acc: 0.8431

Epoch 00001: val_loss improved from inf to 0.45926, saving model to models/neck4_clip_w_h_resnet_all_mem_weights.h5
Epoch 2/32
229/229 [==============================] - 176s 769ms/step - loss: 0.3052 - acc: 0.8937 - val_loss: 0.4602 - val_acc: 0.8456

Epoch 00002: val_loss did not improve from 0.45926
Epoch 3/32
 62/229 [=======>......................] - ETA: 2:02 - loss: 0.3125 - acc: 0.8853KeyboardInterrupt
CPU times: user 15min 10s, sys: 1min 16s, total: 16min 27s
Wall time: 8min 31s

```

2018.5.18

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：加载round1的模型 round1_coat3_weights.h5
- 
- 
```
#变换的样式
#{0:"299X299",1:"顺时针20度",2:"逆时针20度",3:"向下平移0.2",4:"水平翻转",5:"水平翻转顺时针20度",6:"水平翻转逆时针20度",7:"水平翻转向下平移0.2",8:"加入高斯噪声",9:"水平翻转加入高斯噪声"}
trans_style=[0,8,4]
n_times=len(trans_style)
X_train = np.zeros((n*n_times, width, width, 3), dtype=np.uint8)
y_train = np.zeros((n*n_times, n_class), dtype=np.uint8)

for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
#     if len(tmp_label) > n_class:
#         print(df_load['image_id'][i])
    #for j in range(8):
    for j in range(len(trans_style)):#
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')
inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True)

h = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, 
              callbacks=[EarlyStopping(patience=20), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, y_valid))              

```
结果 lr=0.001 coat_augment_best0518_iception_res.h5
```
Train on 36855 samples, validate on 1446 samples
Epoch 1/4
36855/36855 [==============================] - 917s 25ms/step - loss: 1.0284 - acc: 0.6028 - val_loss: 1.0609 - val_acc: 0.5851

Epoch 00001: val_loss improved from inf to 1.06094, saving model to models/coat_augment_best0518_iception_res.h5
Epoch 2/4
36855/36855 [==============================] - 864s 23ms/step - loss: 0.6677 - acc: 0.7418 - val_loss: 1.0943 - val_acc: 0.6044

Epoch 00002: val_loss did not improve from 1.06094
Epoch 3/4
36855/36855 [==============================] - 865s 23ms/step - loss: 0.4408 - acc: 0.8309 - val_loss: 1.3183 - val_acc: 0.6086

Epoch 00003: val_loss did not improve from 1.06094
Epoch 4/4
22208/36855 [=================>............] - ETA: 5:36 - loss: 0.2917 - acc: 0.8899

```
lr=0.00002 coat3_augment_best0518_iception_res.h5
```
Train on 36855 samples, validate on 1446 samples
Epoch 1/2
36855/36855 [==============================] - 923s 25ms/step - loss: 0.1416 - acc: 0.9507 - val_loss: 1.5526 - val_acc: 0.6259

Epoch 00001: val_loss improved from inf to 1.55264, saving model to models/coat3_augment_best0518_iception_res.h5
Epoch 2/2
 6368/36855 [====>.........................] - ETA: 11:40 - loss: 0.0995 - acc: 0.9670KeyboardInterrupt

```
for layer in cnn_model.layers:
    layer.trainable = False

lr=0.00002 coat4_augment_best0518_iception_res.h5
```
Train on 36855 samples, validate on 1446 samples
Epoch 1/2
36855/36855 [==============================] - 859s 23ms/step - loss: 0.0878 - acc: 0.9698 - val_loss: 1.6813 - val_acc: 0.6259

Epoch 00001: val_loss improved from inf to 1.68131, saving model to models/coat4_augment_best0518_iception_res.h5

```

- 训练类型：collar
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
切分数据 验证集1 和验证集2 分别是0.1 和 0.05

```

#切分数据
n_ration=0.85
n_ration_2=0.95
df_valid_2=df_load[int(n_ration_2*len(df_load)):]
df_valid=df_load[int(n_ration*len(df_load)):int(n_ration_2*len(df_load))]
df_load=df_load[:int(n_ration*len(df_load))]


n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=0.001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}.best0517_InceptionResNetV2.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1, 
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 collar.best0517_InceptionResNetV2.h5
```



```
训练结果 lr=0.0001 collar2.best0517_InceptionResNetV2.h5
```


```









- 训练类型：collar
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
切分数据 验证集1 和验证集2 分别是0.1 和 0.05

```

#切分数据
n_ration=0.85
n_ration_2=0.95
df_valid_2=df_load[int(n_ration_2*len(df_load)):]
df_valid=df_load[int(n_ration*len(df_load)):int(n_ration_2*len(df_load))]
df_load=df_load[:int(n_ration*len(df_load))]


n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=0.001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}.best0517_InceptionResNetV2.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1, 
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 collar.best0517_InceptionResNetV2.h5
```



```
训练结果 lr=0.0001 collar2.best0517_InceptionResNetV2.h5
```


```


- 训练类型：sleeve
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
切分数据 验证集1 和验证集2 分别是0.1 和 0.05

```

#切分数据
n_ration=0.85
n_ration_2=0.95
df_valid_2=df_load[int(n_ration_2*len(df_load)):]
df_valid=df_load[int(n_ration*len(df_load)):int(n_ration_2*len(df_load))]
df_load=df_load[:int(n_ration*len(df_load))]


n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
# x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=0.001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}.best0517_InceptionResNetV2.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1, 
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.01 sleeve.best0517_InceptionResNetV2.h5
```
Epoch 1/32
459/459 [==============================] - 410s 894ms/step - loss: 3.0711 - acc: 0.1396 - val_loss: 2.2515 - val_acc: 0.1499

Epoch 00001: val_loss improved from inf to 2.25155, saving model to models/sleeve.best0517_InceptionResNetV2.h5
Epoch 2/32
459/459 [==============================] - 352s 767ms/step - loss: 2.2508 - acc: 0.1454 - val_loss: 2.2346 - val_acc: 0.1406

Epoch 00002: val_loss improved from 2.25155 to 2.23460, saving model to models/sleeve.best0517_InceptionResNetV2.h5
Epoch 3/32
 22/459 [>.............................] - ETA: 5:21 - loss: 2.2323 - acc: 0.1293KeyboardInterrupt


```
训练结果 lr=0.001 sleeve2.best0517_InceptionResNetV2.h5
```
Epoch 1/32
459/459 [==============================] - 407s 888ms/step - loss: 2.1730 - acc: 0.1572 - val_loss: 3.7392 - val_acc: 0.1291

Epoch 00001: val_loss improved from inf to 3.73918, saving model to models/sleeve2.best0517_InceptionResNetV2.h5
Epoch 2/32
459/459 [==============================] - 353s 769ms/step - loss: 2.1729 - acc: 0.1603 - val_loss: 2.2432 - val_acc: 0.1221

Epoch 00002: val_loss improved from 3.73918 to 2.24319, saving model to models/sleeve2.best0517_InceptionResNetV2.h5
Epoch 3/32
352/459 [======================>.......] - ETA: 1:22 - loss: 2.1591 - acc: 0.1632KeyboardInterrupt

```

训练结果 lr=0.001 sleeve2.best0517_InceptionResNetV2.h5
```


```


训练结果 lr=0.001 sleeve2.best0517_InceptionResNetV2.h5
```


```



- 训练类型：skirt
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
切分数据 验证集1 和验证集2 分别是0.1 和 0.05

```

#切分数据
n_ration=0.85
n_ration_2=0.95
df_valid_2=df_load[int(n_ration_2*len(df_load)):]
df_valid=df_load[int(n_ration*len(df_load)):int(n_ration_2*len(df_load))]
df_load=df_load[:int(n_ration*len(df_load))]


n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=0.001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}.best0517_InceptionResNetV2.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1, 
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 skrit.best0517_InceptionResNetV2.h5
```
Epoch 1/32
333/333 [==============================] - 316s 948ms/step - loss: 4.6689 - acc: 0.4677 - val_loss: 1.7147 - val_acc: 0.4045

Epoch 00001: val_loss improved from inf to 1.71466, saving model to models/skirt.best0517_InceptionResNetV2.h5
Epoch 2/32
333/333 [==============================] - 256s 770ms/step - loss: 1.3871 - acc: 0.5084 - val_loss: 1.5101 - val_acc: 0.4689

Epoch 00002: val_loss improved from 1.71466 to 1.51005, saving model to models/skirt.best0517_InceptionResNetV2.h5
Epoch 3/32
333/333 [==============================] - 255s 766ms/step - loss: 1.3401 - acc: 0.5443 - val_loss: 1.4309 - val_acc: 0.4952

Epoch 00003: val_loss improved from 1.51005 to 1.43088, saving model to models/skirt.best0517_InceptionResNetV2.h5
Epoch 4/32
333/333 [==============================] - 255s 767ms/step - loss: 1.3185 - acc: 0.5611 - val_loss: 2.1093 - val_acc: 0.3169

Epoch 00004: val_loss did not improve from 1.43088
Epoch 5/32
333/333 [==============================] - 254s 763ms/step - loss: 1.2965 - acc: 0.5776 - val_loss: 1.4948 - val_acc: 0.5080

Epoch 00005: val_loss did not improve from 1.43088
Epoch 6/32
333/333 [==============================] - 255s 765ms/step - loss: 1.2468 - acc: 0.6038 - val_loss: 1.6275 - val_acc: 0.4498

Epoch 00006: val_loss did not improve from 1.43088
Epoch 7/32
333/333 [==============================] - 255s 764ms/step - loss: 1.2189 - acc: 0.6160 - val_loss: 1.4858 - val_acc: 0.5040

Epoch 00007: val_loss did not improve from 1.43088
Epoch 8/32
164/333 [=============>................] - ETA: 2:03 - loss: 1.2269 - acc: 0.6046KeyboardInterrupt


```
训练结果 lr=0.0001 skrit2.best0517_InceptionResNetV2.h5
```
Epoch 1/32
333/333 [==============================] - 321s 964ms/step - loss: 1.0085 - acc: 0.6967 - val_loss: 1.0960 - val_acc: 0.6457

Epoch 00001: val_loss improved from inf to 1.09595, saving model to models/skirt2.best0517_InceptionResNetV2.h5
Epoch 2/32
333/333 [==============================] - 255s 766ms/step - loss: 0.9673 - acc: 0.7074 - val_loss: 1.0827 - val_acc: 0.6441

Epoch 00002: val_loss improved from 1.09595 to 1.08266, saving model to models/skirt2.best0517_InceptionResNetV2.h5
Epoch 3/32
333/333 [==============================] - 255s 766ms/step - loss: 0.9294 - acc: 0.7302 - val_loss: 1.0674 - val_acc: 0.6553

Epoch 00003: val_loss improved from 1.08266 to 1.06740, saving model to models/skirt2.best0517_InceptionResNetV2.h5
Epoch 4/32
333/333 [==============================] - 255s 767ms/step - loss: 0.9010 - acc: 0.7407 - val_loss: 1.0843 - val_acc: 0.6377

Epoch 00004: val_loss did not improve from 1.06740
Epoch 5/32
333/333 [==============================] - 255s 766ms/step - loss: 0.8705 - acc: 0.7601 - val_loss: 1.0619 - val_acc: 0.6568

Epoch 00005: val_loss improved from 1.06740 to 1.06185, saving model to models/skirt2.best0517_InceptionResNetV2.h5
Epoch 6/32
333/333 [==============================] - 255s 767ms/step - loss: 0.8511 - acc: 0.7661 - val_loss: 1.0555 - val_acc: 0.6537

Epoch 00006: val_loss improved from 1.06185 to 1.05550, saving model to models/skirt2.best0517_InceptionResNetV2.h5
Epoch 7/32
333/333 [==============================] - 255s 767ms/step - loss: 0.8257 - acc: 0.7783 - val_loss: 1.0681 - val_acc: 0.6664

Epoch 00007: val_loss did not improve from 1.05550
Epoch 8/32
333/333 [==============================] - 264s 792ms/step - loss: 0.8094 - acc: 0.7877 - val_loss: 1.1057 - val_acc: 0.6441

Epoch 00008: val_loss did not improve from 1.05550
Epoch 9/32
333/333 [==============================] - 282s 847ms/step - loss: 0.7852 - acc: 0.7984 - val_loss: 1.0787 - val_acc: 0.6640

Epoch 00009: val_loss did not improve from 1.05550
Epoch 10/32
333/333 [==============================] - 282s 848ms/step - loss: 0.7664 - acc: 0.8076 - val_loss: 1.1333 - val_acc: 0.6497

Epoch 00010: val_loss did not improve from 1.05550
Epoch 11/32
333/333 [==============================] - 285s 855ms/step - loss: 0.7498 - acc: 0.8137 - val_loss: 1.1454 - val_acc: 0.6338

Epoch 00011: val_loss did not improve from 1.05550
Epoch 12/32
 36/333 [==>...........................] - ETA: 3:59 - loss: 0.7065 - acc: 0.8411KeyboardInterrupt

```
训练结果 lr=0.00001 skrit3.best0517_InceptionResNetV2.h5
```
Epoch 1/32
333/333 [==============================] - 346s 1s/step - loss: 0.6940 - acc: 0.8455 - val_loss: 1.1394 - val_acc: 0.6346

Epoch 00001: val_loss improved from inf to 1.13937, saving model to models/skirt3.best0517_InceptionResNetV2.h5
Epoch 2/32
333/333 [==============================] - 258s 773ms/step - loss: 0.6845 - acc: 0.8506 - val_loss: 1.1331 - val_acc: 0.6449

Epoch 00002: val_loss improved from 1.13937 to 1.13309, saving model to models/skirt3.best0517_InceptionResNetV2.h5
Epoch 3/32
333/333 [==============================] - 255s 766ms/step - loss: 0.6728 - acc: 0.8510 - val_loss: 1.1324 - val_acc: 0.6481

Epoch 00003: val_loss improved from 1.13309 to 1.13240, saving model to models/skirt3.best0517_InceptionResNetV2.h5
Epoch 4/32
333/333 [==============================] - 255s 765ms/step - loss: 0.6711 - acc: 0.8564 - val_loss: 1.1319 - val_acc: 0.6497

Epoch 00004: val_loss improved from 1.13240 to 1.13190, saving model to models/skirt3.best0517_InceptionResNetV2.h5
Epoch 5/32
333/333 [==============================] - 255s 766ms/step - loss: 0.6646 - acc: 0.8586 - val_loss: 1.1338 - val_acc: 0.6481

Epoch 00005: val_loss did not improve from 1.13190
Epoch 6/32
333/333 [==============================] - 255s 766ms/step - loss: 0.6638 - acc: 0.8543 - val_loss: 1.1365 - val_acc: 0.6465

Epoch 00006: val_loss did not improve from 1.13190
Epoch 7/32
 15/333 [>.............................] - ETA: 3:52 - loss: 0.6963 - acc: 0.8375KeyboardInterrupt

```

- 训练类型：neckline
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
切分数据 验证集1 和验证集2 分别是0.1 和 0.05

```

#切分数据
n_ration=0.85
n_ration_2=0.95
df_valid_2=df_load[int(n_ration_2*len(df_load)):]
df_valid=df_load[int(n_ration*len(df_load)):int(n_ration_2*len(df_load))]
df_load=df_load[:int(n_ration*len(df_load))]


n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=0.001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}.best0517_InceptionResNetV2.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1, 
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 neckline.best0517_InceptionResNetV2.h5
```
Epoch 1/32
434/434 [==============================] - 402s 926ms/step - loss: 6.2105 - acc: 0.3258 - val_loss: 2.4060 - val_acc: 0.2729

Epoch 00001: val_loss improved from inf to 2.40597, saving model to models/neckline.best0517_InceptionResNetV2.h5
Epoch 2/32
434/434 [==============================] - 340s 784ms/step - loss: 1.9284 - acc: 0.4153 - val_loss: 2.1969 - val_acc: 0.3620

Epoch 00002: val_loss improved from 2.40597 to 2.19691, saving model to models/neckline.best0517_InceptionResNetV2.h5
Epoch 3/32
434/434 [==============================] - 338s 778ms/step - loss: 1.8189 - acc: 0.4837 - val_loss: 2.1016 - val_acc: 0.3742

Epoch 00003: val_loss improved from 2.19691 to 2.10158, saving model to models/neckline.best0517_InceptionResNetV2.h5
Epoch 4/32
 94/434 [=====>........................] - ETA: 4:13 - loss: 1.7302 - acc: 0.5239KeyboardInterrupt


```
训练结果 lr=0.0001 neckline2.best0517_InceptionResNetV2.h5
```
Epoch 1/32
434/434 [==============================] - 397s 915ms/step - loss: 1.4359 - acc: 0.6073 - val_loss: 2.4193 - val_acc: 0.2112

Epoch 00001: val_loss improved from inf to 2.41926, saving model to models/neckline2.best0517_InceptionResNetV2.h5
Epoch 2/32
434/434 [==============================] - 335s 772ms/step - loss: 1.3626 - acc: 0.6342 - val_loss: 2.1367 - val_acc: 0.3669

Epoch 00002: val_loss improved from 2.41926 to 2.13671, saving model to models/neckline2.best0517_InceptionResNetV2.h5
Epoch 3/32
434/434 [==============================] - 333s 768ms/step - loss: 1.3162 - acc: 0.6665 - val_loss: 2.0059 - val_acc: 0.4451

Epoch 00003: val_loss improved from 2.13671 to 2.00591, saving model to models/neckline2.best0517_InceptionResNetV2.h5
Epoch 4/32
434/434 [==============================] - 335s 771ms/step - loss: 1.2680 - acc: 0.6956 - val_loss: 1.8814 - val_acc: 0.4829

Epoch 00004: val_loss improved from 2.00591 to 1.88137, saving model to models/neckline2.best0517_InceptionResNetV2.h5
Epoch 5/32
434/434 [==============================] - 335s 771ms/step - loss: 1.2285 - acc: 0.7115 - val_loss: 2.1079 - val_acc: 0.4493

Epoch 00005: val_loss did not improve from 1.88137
Epoch 6/32
434/434 [==============================] - 333s 767ms/step - loss: 1.1845 - acc: 0.7287 - val_loss: 1.9659 - val_acc: 0.4915

Epoch 00006: val_loss did not improve from 1.88137
Epoch 7/32
434/434 [==============================] - 333s 767ms/step - loss: 1.1578 - acc: 0.7372 - val_loss: 1.9833 - val_acc: 0.4811

Epoch 00007: val_loss did not improve from 1.88137
Epoch 8/32
280/434 [==================>...........] - ETA: 1:53 - loss: 1.1220 - acc: 0.7509KeyboardInterrupt

```



- 训练类型：collar
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：
切分数据 验证集1 和验证集2 分别是0.1 和 0.05

```

#切分数据
n_ration=0.85
n_ration_2=0.95
df_valid_2=df_load[int(n_ration_2*len(df_load)):]
df_valid=df_load[int(n_ration*len(df_load)):int(n_ration_2*len(df_load))]
df_load=df_load[:int(n_ration*len(df_load))]


n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=0.001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}.best0517_InceptionResNetV2.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1, 
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```
训练结果 lr=0.001 collar.best0517_InceptionResNetV2.h5
```
Epoch 1/32
240/240 [==============================] - 253s 1s/step - loss: 4.9144 - acc: 0.6260 - val_loss: 1.4989 - val_acc: 0.4956

Epoch 00001: val_loss improved from inf to 1.49894, saving model to models/collar.best0517_InceptionResNetV2.h5
Epoch 2/32
240/240 [==============================] - 196s 816ms/step - loss: 1.0450 - acc: 0.7025 - val_loss: 1.4749 - val_acc: 0.5585

Epoch 00002: val_loss improved from 1.49894 to 1.47490, saving model to models/collar.best0517_InceptionResNetV2.h5
Epoch 3/32
240/240 [==============================] - 195s 813ms/step - loss: 0.9511 - acc: 0.7544 - val_loss: 1.1182 - val_acc: 0.6744

Epoch 00003: val_loss improved from 1.47490 to 1.11819, saving model to models/collar.best0517_InceptionResNetV2.h5
Epoch 4/32
240/240 [==============================] - 195s 812ms/step - loss: 0.9093 - acc: 0.7657 - val_loss: 0.9741 - val_acc: 0.7318

Epoch 00004: val_loss improved from 1.11819 to 0.97414, saving model to models/collar.best0517_InceptionResNetV2.h5
Epoch 5/32
240/240 [==============================] - 195s 812ms/step - loss: 0.8665 - acc: 0.7866 - val_loss: 1.1483 - val_acc: 0.6645

Epoch 00005: val_loss did not improve from 0.97414
Epoch 6/32
240/240 [==============================] - 195s 811ms/step - loss: 0.8295 - acc: 0.8059 - val_loss: 1.0843 - val_acc: 0.6843

Epoch 00006: val_loss did not improve from 0.97414
Epoch 7/32
240/240 [==============================] - 194s 809ms/step - loss: 0.7902 - acc: 0.8187 - val_loss: 1.0873 - val_acc: 0.7075

Epoch 00007: val_loss did not improve from 0.97414
Epoch 8/32
240/240 [==============================] - 193s 804ms/step - loss: 0.7823 - acc: 0.8160 - val_loss: 1.1474 - val_acc: 0.6667

Epoch 00008: val_loss did not improve from 0.97414
Epoch 9/32
240/240 [==============================] - 193s 805ms/step - loss: 0.7476 - acc: 0.8365 - val_loss: 1.0121 - val_acc: 0.7097

Epoch 00009: val_loss did not improve from 0.97414
Epoch 10/32
240/240 [==============================] - 195s 812ms/step - loss: 0.6876 - acc: 0.8548 - val_loss: 0.9772 - val_acc: 0.7417

Epoch 00010: val_loss did not improve from 0.97414
Epoch 11/32
240/240 [==============================] - 196s 815ms/step - loss: 0.6720 - acc: 0.8652 - val_loss: 1.1375 - val_acc: 0.7108

Epoch 00011: val_loss did not improve from 0.97414
Epoch 12/32
 23/240 [=>............................] - ETA: 2:45 - loss: 0.6271 - acc: 0.8750KeyboardInterrupt


```
训练结果 lr=0.0001 collar2.best0517_InceptionResNetV2.h5
```
Epoch 1/32
240/240 [==============================] - 243s 1s/step - loss: 0.5053 - acc: 0.9122 - val_loss: 0.5675 - val_acc: 0.8805

Epoch 00001: val_loss improved from inf to 0.56754, saving model to models/collar2.best0517_InceptionResNetV2.h5
Epoch 2/32
118/240 [=============>................] - ETA: 1:32 - loss: 0.4364 - acc: 0.9362KeyboardInterrupt


```

2018.5.17
- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：InceptionResNetV2 
- 模型文件 ：

```
cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet',pooling='max')

inputs = Input(shape=(None,width, width,3),name='input_1')

x = inputs
#x = Lambda(preprocess_input, name='preprocessing')(x)
#x = cnn_model(x)
x = TimeDistributed(Lambda(lambda x: cnn_model(x)))(x)
x= LSTM(1024)(x)
#x = GlobalAveragePooling2D()(x)
#x=tf.reshape(x,[-1,5,5*1536])#1536最后一层的filter数
#x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

adam = Adam(lr=0.0001) 

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 64
batch_size = 32

atagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)


train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=batch_size,
        class_mode='categorical')

num_batch_valid=X_valid.shape
X_valid=X_valid.reshape((num_batch_valid[0],1,num_batch_valid[1],num_batch_valid[2],num_batch_valid[3]))

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_lstm_resnet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}4_clip_resnet_all_mem_weights.h5'.format(prefix_cls))
s=len(df_load)/batch_size


val_acc_last=0
i=0

#epochs=20
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    loss_sum=0
    acc_sum=0
    
    time_begin=time.time()
    for X_batch, y_batch in train_generator:
        #print('all  batch is {}'.format(int(s)),'this is batch {}'.format(batches))
        num_batch=X_batch.shape
        X_batch=X_batch.reshape((num_batch[0],1,num_batch[1],num_batch[2],num_batch[3]))
        #y_batch=y_batch.reshape((1,y_batch.shape[0],y_batch.shape[1]))
        history=model.fit(X_batch, y_batch,verbose=0)
        batches += 1
        loss_sum +=history.history['loss'][0]
        acc_sum  +=history.history['acc'][0]
        #print(history.history)
        if batches >= s:
            break
        if batches % 25==0:
           
            print('epoch:',e,'------','All steps is:',s,'this is step:',batches)
            print('train LOSS is :',history.history['loss'][0],'train ACC is :',history.history['acc'][0])
        
    print('This epoch:',e,'---','the average LOSS is:',loss_sum/int(s),'---','the average ACC is:', acc_sum/int(s))
    
    
    val_acc=model.evaluate(X_valid,y_valid)#评估
    print('val_loss is :',val_acc[0],'---','val_acc is :',val_acc[1])
    if val_acc[1] > val_acc_last:
        i +=1
        print('model save {} times'.format(i))
        model.save_weights('models/{0}2_lstm_resnet_all_mem_weights.h5'.format(prefix_cls))
        val_acc_last=val_acc[1]
    time_end=time.time()
    print('This epoch take time is :',time_end-time_begin)


```
训练结果 lr=0.0001
```
Epoch 0
epoch: 0 ------ All steps is: 229.3125 this is step: 25
train LOSS is : 1.50304114818573 train ACC is : 0.28125
epoch: 0 ------ All steps is: 229.3125 this is step: 50
train LOSS is : 1.4665005207061768 train ACC is : 0.375
epoch: 0 ------ All steps is: 229.3125 this is step: 75
train LOSS is : 1.4039450883865356 train ACC is : 0.40625
epoch: 0 ------ All steps is: 229.3125 this is step: 100
train LOSS is : 1.5822622776031494 train ACC is : 0.1875
epoch: 0 ------ All steps is: 229.3125 this is step: 125
train LOSS is : 1.603535532951355 train ACC is : 0.3125
epoch: 0 ------ All steps is: 229.3125 this is step: 150
train LOSS is : 1.6525057554244995 train ACC is : 0.28125
epoch: 0 ------ All steps is: 229.3125 this is step: 175
train LOSS is : 1.4284652471542358 train ACC is : 0.375
epoch: 0 ------ All steps is: 229.3125 this is step: 200
train LOSS is : 1.368055820465088 train ACC is : 0.40625
epoch: 0 ------ All steps is: 229.3125 this is step: 225
train LOSS is : 1.538036823272705 train ACC is : 0.21875
This epoch: 0 --- the average LOSS is: 1.505761155395008 --- the average ACC is: 0.3429312227074236
816/816 [==============================] - 18s 22ms/step
val_loss is : 1.6606568051319497 --- val_acc is : 0.24877450980392157
model save 1 times
This epoch take time is : 268.19216895103455
Epoch 1
epoch: 1 ------ All steps is: 229.3125 this is step: 25
train LOSS is : 1.3429772853851318 train ACC is : 0.40625
epoch: 1 ------ All steps is: 229.3125 this is step: 50
train LOSS is : 1.4212899208068848 train ACC is : 0.4375
epoch: 1 ------ All steps is: 229.3125 this is step: 75
train LOSS is : 1.428562879562378 train ACC is : 0.34375
epoch: 1 ------ All steps is: 229.3125 this is step: 100
train LOSS is : 1.3873281478881836 train ACC is : 0.46875
epoch: 1 ------ All steps is: 229.3125 this is step: 125
train LOSS is : 1.4250911474227905 train ACC is : 0.375
epoch: 1 ------ All steps is: 229.3125 this is step: 150
train LOSS is : 1.5386662483215332 train ACC is : 0.25
epoch: 1 ------ All steps is: 229.3125 this is step: 175
train LOSS is : 1.477383017539978 train ACC is : 0.40625
epoch: 1 ------ All steps is: 229.3125 this is step: 200
train LOSS is : 1.4631074666976929 train ACC is : 0.40625
epoch: 1 ------ All steps is: 229.3125 this is step: 225
train LOSS is : 1.2893280982971191 train ACC is : 0.46875
This epoch: 1 --- the average LOSS is: 1.446735939084182 --- the average ACC is: 0.38037663756759926
816/816 [==============================] - 8s 9ms/step
val_loss is : 1.6760260334201889 --- val_acc is : 0.2818627450980392
model save 2 times
This epoch take time is : 222.21875882148743
Epoch 2
epoch: 2 ------ All steps is: 229.3125 this is step: 25
train LOSS is : 1.3787055015563965 train ACC is : 0.375
epoch: 2 ------ All steps is: 229.3125 this is step: 50
train LOSS is : 1.3638544082641602 train ACC is : 0.40625
epoch: 2 ------ All steps is: 229.3125 this is step: 75
train LOSS is : 1.3428130149841309 train ACC is : 0.375
epoch: 2 ------ All steps is: 229.3125 this is step: 100
train LOSS is : 1.3980871438980103 train ACC is : 0.375
epoch: 2 ------ All steps is: 229.3125 this is step: 125
train LOSS is : 1.3036675453186035 train ACC is : 0.5
epoch: 2 ------ All steps is: 229.3125 this is step: 150
train LOSS is : 1.3407485485076904 train ACC is : 0.4375
epoch: 2 ------ All steps is: 229.3125 this is step: 175
train LOSS is : 1.3260692358016968 train ACC is : 0.4375
epoch: 2 ------ All steps is: 229.3125 this is step: 200
train LOSS is : 1.293941617012024 train ACC is : 0.46875
epoch: 2 ------ All steps is: 229.3125 this is step: 225
train LOSS is : 1.2656774520874023 train ACC is : 0.46875
This epoch: 2 --- the average LOSS is: 1.4063483742126732 --- the average ACC is: 0.39626091705659594
816/816 [==============================] - 8s 9ms/step
val_loss is : 1.7066090457579668 --- val_acc is : 0.14705882352941177
This epoch take time is : 221.75724864006042
Epoch 3
epoch: 3 ------ All steps is: 229.3125 this is step: 25
train LOSS is : 1.4443092346191406 train ACC is : 0.375
epoch: 3 ------ All steps is: 229.3125 this is step: 50
train LOSS is : 1.2349079847335815 train ACC is : 0.59375
epoch: 3 ------ All steps is: 229.3125 this is step: 75
train LOSS is : 1.5139623880386353 train ACC is : 0.4375
epoch: 3 ------ All steps is: 229.3125 this is step: 100
train LOSS is : 1.5613391399383545 train ACC is : 0.28125
epoch: 3 ------ All steps is: 229.3125 this is step: 125
train LOSS is : 1.4882657527923584 train ACC is : 0.28125
epoch: 3 ------ All steps is: 229.3125 this is step: 150
train LOSS is : 1.4870400428771973 train ACC is : 0.34375
epoch: 3 ------ All steps is: 229.3125 this is step: 175
train LOSS is : 1.373055338859558 train ACC is : 0.375
epoch: 3 ------ All steps is: 229.3125 this is step: 200
train LOSS is : 1.4107441902160645 train ACC is : 0.40625
epoch: 3 ------ All steps is: 229.3125 this is step: 225
train LOSS is : 1.2543578147888184 train ACC is : 0.4375
This epoch: 3 --- the average LOSS is: 1.3914193518817686 --- the average ACC is: 0.4027838428468163
816/816 [==============================] - 7s 9ms/step
val_loss is : 1.599692559709736 --- val_acc is : 0.2867647058823529
model save 3 times
This epoch take time is : 220.66488647460938
Epoch 4
epoch: 4 ------ All steps is: 229.3125 this is step: 25
train LOSS is : 1.478316307067871 train ACC is : 0.40625
epoch: 4 ------ All steps is: 229.3125 this is step: 50
train LOSS is : 1.4572839736938477 train ACC is : 0.40625
epoch: 4 ------ All steps is: 229.3125 this is step: 75
train LOSS is : 1.2277390956878662 train ACC is : 0.5625
epoch: 4 ------ All steps is: 229.3125 this is step: 100
train LOSS is : 1.216860294342041 train ACC is : 0.53125
epoch: 4 ------ All steps is: 229.3125 this is step: 125
train LOSS is : 1.5208566188812256 train ACC is : 0.21875
epoch: 4 ------ All steps is: 229.3125 this is step: 150
train LOSS is : 1.3125079870224 train ACC is : 0.40625
epoch: 4 ------ All steps is: 229.3125 this is step: 175
train LOSS is : 1.344630241394043 train ACC is : 0.40625
epoch: 4 ------ All steps is: 229.3125 this is step: 200
train LOSS is : 1.4325309991836548 train ACC is : 0.34375
epoch: 4 ------ All steps is: 229.3125 this is step: 225
train LOSS is : 1.4455299377441406 train ACC is : 0.375
This epoch: 4 --- the average LOSS is: 1.379683156200892 --- the average ACC is: 0.41588427952803897
816/816 [==============================] - 7s 9ms/step
val_loss is : 1.7331070666219675 --- val_acc is : 0.20833333333333334
This epoch take time is : 219.8269762992859
Epoch 5
epoch: 5 ------ All steps is: 229.3125 this is step: 25
train LOSS is : 1.2789382934570312 train ACC is : 0.5625
epoch: 5 ------ All steps is: 229.3125 this is step: 50
train LOSS is : 1.2041192054748535 train ACC is : 0.53125
epoch: 5 ------ All steps is: 229.3125 this is step: 75
train LOSS is : 1.2918169498443604 train ACC is : 0.5
epoch: 5 ------ All steps is: 229.3125 this is step: 100
train LOSS is : 1.38344407081604 train ACC is : 0.3125
epoch: 5 ------ All steps is: 229.3125 this is step: 125
train LOSS is : 1.3058829307556152 train ACC is : 0.46875
epoch: 5 ------ All steps is: 229.3125 this is step: 150
train LOSS is : 1.4886705875396729 train ACC is : 0.34375
epoch: 5 ------ All steps is: 229.3125 this is step: 175
train LOSS is : 1.496636986732483 train ACC is : 0.40625
epoch: 5 ------ All steps is: 229.3125 this is step: 200
train LOSS is : 1.5285980701446533 train ACC is : 0.21875
epoch: 5 ------ All steps is: 229.3125 this is step: 225
train LOSS is : 1.5342183113098145 train ACC is : 0.34375
This epoch: 5 --- the average LOSS is: 1.3673172210501792 --- the average ACC is: 0.41239082969432317
816/816 [==============================] - 8s 9ms/step
val_loss is : 1.6929797822353887 --- val_acc is : 0.17401960784313725
This epoch take time is : 219.17875814437866
Epoch 6

```

2018.5.16
- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： densenet201 clip
- 模型文件 ：
```
crip=int(width*0.5)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])), (width, width))
    X[i] = cv2.resize(X[i][:crip], (width, width))
    y[i][tmp_label.find('y')] = 1

cnn_model =InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])


epochs = 24
batch_size = 32


datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)


%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_resnet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}2_clip_densenet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```
训练结果
lr=1e-3 neck_clip_resnet_all_mem_weights.h5  后面可以切换成sgd优化器
```
Epoch 1/24
229/229 [==============================] - 233s 1s/step - loss: 1.3236 - acc: 0.4439 - val_loss: 1.6237 - val_acc: 0.4608

Epoch 00001: val_loss improved from inf to 1.62370, saving model to models/neck_clip_resnet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 173s 754ms/step - loss: 1.0486 - acc: 0.5909 - val_loss: 1.1482 - val_acc: 0.6238

Epoch 00002: val_loss improved from 1.62370 to 1.14819, saving model to models/neck_clip_resnet_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 173s 754ms/step - loss: 0.9079 - acc: 0.6612 - val_loss: 0.9850 - val_acc: 0.6262

Epoch 00003: val_loss improved from 1.14819 to 0.98496, saving model to models/neck_clip_resnet_all_mem_weights.h5
Epoch 4/24
229/229 [==============================] - 172s 753ms/step - loss: 0.8430 - acc: 0.6822 - val_loss: 0.8559 - val_acc: 0.6814

Epoch 00004: val_loss improved from 0.98496 to 0.85586, saving model to models/neck_clip_resnet_all_mem_weights.h5
Epoch 5/24
108/229 [=============>................] - ETA: 1:27 - loss: 0.7699 - acc: 0.7168KeyboardInterrupt
CPU times: user 26min 7s, sys: 2min 24s, total: 28min 31s
Wall time: 14min 16s

```

lr=1e-3 neck2_clip_resnet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 173s 756ms/step - loss: 0.7496 - acc: 0.7191 - val_loss: 0.8698 - val_acc: 0.6850

Epoch 00001: val_loss improved from inf to 0.86984, saving model to models/neck2_clip_resnet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 173s 754ms/step - loss: 0.7114 - acc: 0.7358 - val_loss: 0.8109 - val_acc: 0.6900

Epoch 00002: val_loss improved from 0.86984 to 0.81090, saving model to models/neck2_clip_resnet_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 173s 754ms/step - loss: 0.6722 - acc: 0.7534 - val_loss: 0.7517 - val_acc: 0.7304

Epoch 00003: val_loss improved from 0.81090 to 0.75174, saving model to models/neck2_clip_resnet_all_mem_weights.h5
Epoch 4/24
229/229 [==============================] - 173s 754ms/step - loss: 0.6392 - acc: 0.7669 - val_loss: 0.6971 - val_acc: 0.7451

Epoch 00004: val_loss improved from 0.75174 to 0.69710, saving model to models/neck2_clip_resnet_all_mem_weights.h5
Epoch 5/24
229/229 [==============================] - 173s 754ms/step - loss: 0.6163 - acc: 0.7780 - val_loss: 0.6804 - val_acc: 0.7537

Epoch 00005: val_loss improved from 0.69710 to 0.68045, saving model to models/neck2_clip_resnet_all_mem_weights.h5
Epoch 6/24
229/229 [==============================] - 173s 753ms/step - loss: 0.5801 - acc: 0.7859 - val_loss: 0.7573 - val_acc: 0.7316

Epoch 00006: val_loss did not improve from 0.68045
Epoch 7/24
229/229 [==============================] - 172s 753ms/step - loss: 0.5438 - acc: 0.8033 - val_loss: 0.6761 - val_acc: 0.7647

Epoch 00007: val_loss improved from 0.68045 to 0.67609, saving model to models/neck2_clip_resnet_all_mem_weights.h5
Epoch 8/24
229/229 [==============================] - 172s 753ms/step - loss: 0.5322 - acc: 0.7997 - val_loss: 0.7101 - val_acc: 0.7635

Epoch 00008: val_loss did not improve from 0.67609
Epoch 9/24
229/229 [==============================] - 173s 754ms/step - loss: 0.4928 - acc: 0.8235 - val_loss: 0.7480 - val_acc: 0.7426

Epoch 00009: val_loss did not improve from 0.67609
Epoch 10/24
229/229 [==============================] - 172s 753ms/step - loss: 0.5026 - acc: 0.8149 - val_loss: 0.8203 - val_acc: 0.7243

Epoch 00010: val_loss did not improve from 0.67609
Epoch 11/24
229/229 [==============================] - 172s 753ms/step - loss: 0.4622 - acc: 0.8334 - val_loss: 0.6885 - val_acc: 0.7745

Epoch 00011: val_loss did not improve from 0.67609
Epoch 12/24
168/229 [=====================>........] - ETA: 43s - loss: 0.4542 - acc: 0.8362KeyboardInterrupt
CPU times: user 1h 4min 57s, sys: 6min 9s, total: 1h 11min 6s
Wall time: 33min 51s

```

lr=1e-3 neck3_clip_resnet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 173s 755ms/step - loss: 0.4551 - acc: 0.8322 - val_loss: 0.6561 - val_acc: 0.7574

Epoch 00001: val_loss improved from inf to 0.65610, saving model to models/neck3_clip_resnet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 172s 753ms/step - loss: 0.4187 - acc: 0.8504 - val_loss: 0.6530 - val_acc: 0.7855

Epoch 00002: val_loss improved from 0.65610 to 0.65295, saving model to models/neck3_clip_resnet_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 172s 753ms/step - loss: 0.4022 - acc: 0.8534 - val_loss: 0.6374 - val_acc: 0.7806

Epoch 00003: val_loss improved from 0.65295 to 0.63737, saving model to models/neck3_clip_resnet_all_mem_weights.h5
Epoch 4/24
229/229 [==============================] - 172s 753ms/step - loss: 0.3793 - acc: 0.8657 - val_loss: 0.7596 - val_acc: 0.7623

Epoch 00004: val_loss did not improve from 0.63737
Epoch 5/24
229/229 [==============================] - 172s 753ms/step - loss: 0.3789 - acc: 0.8664 - val_loss: 0.8067 - val_acc: 0.7194

Epoch 00005: val_loss did not improve from 0.63737
Epoch 6/24
229/229 [==============================] - 172s 753ms/step - loss: 0.3482 - acc: 0.8724 - val_loss: 0.7410 - val_acc: 0.7561

Epoch 00006: val_loss did not improve from 0.63737
Epoch 7/24
229/229 [==============================] - 172s 753ms/step - loss: 0.3492 - acc: 0.8687 - val_loss: 0.6626 - val_acc: 0.7966

Epoch 00007: val_loss did not improve from 0.63737
Epoch 8/24
 14/229 [>.............................] - ETA: 2:36 - loss: 0.3112 - acc: 0.8951KeyboardInterrupt
CPU times: user 39min 5s, sys: 3min 45s, total: 42min 50s
Wall time: 20min 25s

```

lr=1e-4 neck4_clip_resnet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 232s 1s/step - loss: 0.2910 - acc: 0.8981 - val_loss: 1.5961 - val_acc: 0.2561

Epoch 00001: val_loss improved from inf to 1.59606, saving model to models/neck4_clip_resnet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 171s 749ms/step - loss: 0.2367 - acc: 0.9163 - val_loss: 0.8709 - val_acc: 0.7978

Epoch 00002: val_loss improved from 1.59606 to 0.87095, saving model to models/neck4_clip_resnet_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 171s 749ms/step - loss: 0.2045 - acc: 0.9272 - val_loss: 0.5059 - val_acc: 0.8211

Epoch 00003: val_loss improved from 0.87095 to 0.50585, saving model to models/neck4_clip_resnet_all_mem_weights.h5
Epoch 4/24
229/229 [==============================] - 172s 749ms/step - loss: 0.1871 - acc: 0.9353 - val_loss: 0.5875 - val_acc: 0.8248

Epoch 00004: val_loss did not improve from 0.50585
Epoch 5/24
 13/229 [>.............................] - ETA: 2:36 - loss: 0.1687 - acc: 0.9279KeyboardInterrupt
CPU times: user 23min 58s, sys: 2min 8s, total: 26min 6s
Wall time: 13min 11s

```

lr=1e-4
```
Epoch 1/24
229/229 [==============================] - 173s 756ms/step - loss: 0.2899 - acc: 0.8987 - val_loss: 0.6017 - val_acc: 0.8076

Epoch 00001: val_loss improved from inf to 0.60166, saving model to models/neck5_clip_resnet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 172s 749ms/step - loss: 0.2294 - acc: 0.9204 - val_loss: 0.6192 - val_acc: 0.8235

Epoch 00002: val_loss did not improve from 0.60166
Epoch 3/24
229/229 [==============================] - 174s 761ms/step - loss: 0.2074 - acc: 0.9232 - val_loss: 0.6378 - val_acc: 0.8174

Epoch 00003: val_loss did not improve from 0.60166
Epoch 4/24
229/229 [==============================] - 172s 750ms/step - loss: 0.1892 - acc: 0.9314 - val_loss: 0.6590 - val_acc: 0.8088

Epoch 00004: val_loss did not improve from 0.60166
Epoch 5/24
229/229 [==============================] - 172s 750ms/step - loss: 0.1779 - acc: 0.9378 - val_loss: 0.6720 - val_acc: 0.8248

Epoch 00005: val_loss did not improve from 0.60166
Epoch 6/24
229/229 [==============================] - 172s 750ms/step - loss: 0.1611 - acc: 0.9435 - val_loss: 0.6848 - val_acc: 0.8272

Epoch 00006: val_loss did not improve from 0.60166
Epoch 7/24
172/229 [=====================>........] - ETA: 41s - loss: 0.1514 - acc: 0.9475KeyboardInterrupt
CPU times: user 37min 33s, sys: 3min 27s, total: 41min 1s
Wall time: 19min 25s

```
lr=1e-5 neck5_clip_resnet_all_mem_weights.h5
=>neck_0516_82_resnet_clip.csv
```
Epoch 1/24
229/229 [==============================] - 231s 1s/step - loss: 0.1936 - acc: 0.9322 - val_loss: 0.6346 - val_acc: 0.8248

Epoch 00001: val_loss improved from inf to 0.63460, saving model to models/neck5_clip_resnet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 172s 752ms/step - loss: 0.1852 - acc: 0.9346 - val_loss: 0.6377 - val_acc: 0.8223

Epoch 00002: val_loss did not improve from 0.63460
Epoch 3/24
229/229 [==============================] - 172s 751ms/step - loss: 0.1741 - acc: 0.9371 - val_loss: 0.6422 - val_acc: 0.8174

Epoch 00003: val_loss did not improve from 0.63460
Epoch 4/24
 31/229 [===>..........................] - ETA: 2:22 - loss: 0.1730 - acc: 0.9405KeyboardInterrupt
CPU times: user 18min 47s, sys: 1min 42s, total: 20min 30s

```

2018.5.15


- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： densenet201 clip
- 模型文件 ：
- 
```
crip=int(width*0.5)
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_load['image_id'][i])), (width, width))
    X[i] = cv2.resize(X[i][:crip], (width, width))
    y[i][tmp_label.find('y')] = 1


cnn_model =DenseNet201(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=1e-3)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])


epochs = 24
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        #brightness_range=10,
        fill_mode = 'constant',
        cval = 0)

%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_clip_densenet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_clip_densenet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

lr=1e-3 neck_clip_densenet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 186s 811ms/step - loss: 1.7164 - acc: 0.2668 - val_loss: 1.7414 - val_acc: 0.3346

Epoch 00001: val_loss improved from inf to 1.74135, saving model to models/neck_clip_densenet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 122s 531ms/step - loss: 1.6788 - acc: 0.2594 - val_loss: 6.5667 - val_acc: 0.2561

Epoch 00002: val_loss did not improve from 1.74135
Epoch 3/24
 78/229 [=========>....................] - ETA: 1:17 - loss: 1.7054 - acc: 0.2760KeyboardInterrupt
CPU times: user 11min 9s, sys: 47.7 s, total: 11min 57s
Wall time: 6min 16s

```

lr=1e-4 neck2_clip_densenet_all_mem_weights.h5

```
Epoch 1/24
229/229 [==============================] - 184s 805ms/step - loss: 1.4597 - acc: 0.3680 - val_loss: 1.5711 - val_acc: 0.2537

Epoch 00001: val_loss improved from inf to 1.57110, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 122s 533ms/step - loss: 1.2745 - acc: 0.4637 - val_loss: 1.1971 - val_acc: 0.4902

Epoch 00002: val_loss improved from 1.57110 to 1.19706, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 122s 534ms/step - loss: 1.1228 - acc: 0.5390 - val_loss: 1.0222 - val_acc: 0.5833

Epoch 00003: val_loss improved from 1.19706 to 1.02221, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 4/24
229/229 [==============================] - 122s 533ms/step - loss: 0.9884 - acc: 0.6060 - val_loss: 0.8617 - val_acc: 0.6446

Epoch 00004: val_loss improved from 1.02221 to 0.86169, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 5/24
229/229 [==============================] - 122s 533ms/step - loss: 0.8826 - acc: 0.6525 - val_loss: 0.9873 - val_acc: 0.6262

Epoch 00005: val_loss did not improve from 0.86169
Epoch 6/24
229/229 [==============================] - 122s 532ms/step - loss: 0.8163 - acc: 0.6821 - val_loss: 0.8736 - val_acc: 0.6630

Epoch 00006: val_loss did not improve from 0.86169
Epoch 7/24
229/229 [==============================] - 122s 533ms/step - loss: 0.7576 - acc: 0.7102 - val_loss: 0.7708 - val_acc: 0.6949

Epoch 00007: val_loss improved from 0.86169 to 0.77075, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 8/24
229/229 [==============================] - 122s 534ms/step - loss: 0.6797 - acc: 0.7435 - val_loss: 0.8131 - val_acc: 0.6728

Epoch 00008: val_loss did not improve from 0.77075
Epoch 9/24
229/229 [==============================] - 122s 533ms/step - loss: 0.6512 - acc: 0.7541 - val_loss: 0.7359 - val_acc: 0.7120

Epoch 00009: val_loss improved from 0.77075 to 0.73587, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 10/24
229/229 [==============================] - 122s 533ms/step - loss: 0.6014 - acc: 0.7740 - val_loss: 0.6875 - val_acc: 0.7414

Epoch 00010: val_loss improved from 0.73587 to 0.68745, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 11/24
229/229 [==============================] - 122s 533ms/step - loss: 0.5531 - acc: 0.7943 - val_loss: 0.6734 - val_acc: 0.7635

Epoch 00011: val_loss improved from 0.68745 to 0.67344, saving model to models/neck2_clip_densenet_all_mem_weights.h5
Epoch 12/24
229/229 [==============================] - 122s 534ms/step - loss: 0.5219 - acc: 0.8053 - val_loss: 0.7257 - val_acc: 0.7292

Epoch 00012: val_loss did not improve from 0.67344
Epoch 13/24
229/229 [==============================] - 122s 531ms/step - loss: 0.4904 - acc: 0.8198 - val_loss: 0.7390 - val_acc: 0.7365

Epoch 00013: val_loss did not improve from 0.67344
Epoch 14/24
229/229 [==============================] - 122s 532ms/step - loss: 0.4505 - acc: 0.8325 - val_loss: 0.7206 - val_acc: 0.7610

Epoch 00014: val_loss did not improve from 0.67344
Epoch 15/24
229/229 [==============================] - 122s 533ms/step - loss: 0.4311 - acc: 0.8409 - val_loss: 0.7830 - val_acc: 0.7255

Epoch 00015: val_loss did not improve from 0.67344
Epoch 16/24
229/229 [==============================] - 122s 533ms/step - loss: 0.3960 - acc: 0.8575 - val_loss: 0.7077 - val_acc: 0.7316

Epoch 00016: val_loss did not improve from 0.67344
Epoch 17/24
229/229 [==============================] - 122s 533ms/step - loss: 0.3741 - acc: 0.8617 - val_loss: 0.8154 - val_acc: 0.7292

Epoch 00017: val_loss did not improve from 0.67344
Epoch 18/24
 26/229 [==>...........................] - ETA: 1:42 - loss: 0.3835 - acc: 0.8666KeyboardInterrupt
CPU times: user 1h 12min 24s, sys: 5min 21s, total: 1h 17min 46s
Wall time: 36min 36s
```

lr=1e-5 neck3_clip_densenet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 192s 836ms/step - loss: 0.2718 - acc: 0.9006 - val_loss: 0.6992 - val_acc: 0.7525

Epoch 00001: val_loss improved from inf to 0.69921, saving model to models/neck3_clip_densenet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 122s 535ms/step - loss: 0.2355 - acc: 0.9175 - val_loss: 0.7256 - val_acc: 0.7353

Epoch 00002: val_loss did not improve from 0.69921
Epoch 3/24
 36/229 [===>..........................] - ETA: 1:38 - loss: 0.2046 - acc: 0.9297KeyboardInterrupt
CPU times: user 10min 28s, sys: 43.6 s, total: 11min 12s
Wall time: 5min 59s

```

lr=1e-5 neck4_clip_densenet_all_mem_weights.h5

```
Epoch 1/24
229/229 [==============================] - 123s 538ms/step - loss: 0.4667 - acc: 0.8266 - val_loss: 0.6072 - val_acc: 0.7770

Epoch 00001: val_loss improved from inf to 0.60723, saving model to models/neck4_clip_densenet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 123s 536ms/step - loss: 0.4078 - acc: 0.8456 - val_loss: 0.6194 - val_acc: 0.7696

Epoch 00002: val_loss did not improve from 0.60723
Epoch 3/24
229/229 [==============================] - 123s 536ms/step - loss: 0.3910 - acc: 0.8535 - val_loss: 0.6282 - val_acc: 0.7721

Epoch 00003: val_loss did not improve from 0.60723
Epoch 4/24
229/229 [==============================] - 122s 535ms/step - loss: 0.3696 - acc: 0.8645 - val_loss: 0.6563 - val_acc: 0.7696

Epoch 00004: val_loss did not improve from 0.60723
Epoch 5/24
229/229 [==============================] - 123s 536ms/step - loss: 0.3663 - acc: 0.8654 - val_loss: 0.6592 - val_acc: 0.7635

Epoch 00005: val_loss did not improve from 0.60723
Epoch 6/24
229/229 [==============================] - 123s 536ms/step - loss: 0.3571 - acc: 0.8713 - val_loss: 0.6572 - val_acc: 0.7659

Epoch 00006: val_loss did not improve from 0.60723
Epoch 7/24
141/229 [=================>............] - ETA: 45s - loss: 0.3433 - acc: 0.8701KeyboardInterrupt
CPU times: user 27min 23s, sys: 2min 6s, total: 29min 29s
Wall time: 13min 33s


```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： densenet201
- 模型文件 ：neck_densenet_all_mem_weights.h5
- neck1_densenet_all_mem_weights.h5
```
cnn_model =DenseNet201(include_top=False, input_shape=(width, width, 3), weights='imagenet')

nputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train=load_array('data-raw/train_2/{}X_train_224.bc'.format(prefix_cls))
X_valid=load_array('data-raw/train_2/{}X_valid_224.bc'.format(prefix_cls))
y_train=load_array('data-raw/train_2/{}y_train_224.bc'.format(prefix_cls))
y_valid=load_array('data-raw/train_2/{}y_vaild_224.bc'.format(prefix_cls))


# Compile the model
adam = Adam(lr=1e-4)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])


epochs = 24
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of [[total]] heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)


%%time
#prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_densenet_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}nasnet3_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInte

```

lr=1e-4 neck1_densenet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 181s 791ms/step - loss: 1.4783 - acc: 0.3945 - val_loss: 1.2487 - val_acc: 0.4963

Epoch 00001: val_loss improved from inf to 1.24866, saving model to models/neck1_densenet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 120s 523ms/step - loss: 1.0458 - acc: 0.5853 - val_loss: 1.0555 - val_acc: 0.5919

Epoch 00002: val_loss improved from 1.24866 to 1.05553, saving model to models/neck1_densenet_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 119s 521ms/step - loss: 0.8620 - acc: 0.6611 - val_loss: 0.9044 - val_acc: 0.6397

Epoch 00003: val_loss improved from 1.05553 to 0.90442, saving model to models/neck1_densenet_all_mem_weights.h5
Epoch 4/24
229/229 [==============================] - 119s 519ms/step - loss: 0.7585 - acc: 0.7013 - val_loss: 0.9812 - val_acc: 0.6066

Epoch 00004: val_loss did not improve from 0.90442
Epoch 5/24
229/229 [==============================] - 120s 526ms/step - loss: 0.6473 - acc: 0.7487 - val_loss: 0.8836 - val_acc: 0.6887

Epoch 00005: val_loss improved from 0.90442 to 0.88363, saving model to models/neck1_densenet_all_mem_weights.h5
Epoch 6/24
229/229 [==============================] - 121s 527ms/step - loss: 0.5608 - acc: 0.7830 - val_loss: 1.0885 - val_acc: 0.6336

Epoch 00006: val_loss did not improve from 0.88363
Epoch 7/24
229/229 [==============================] - 121s 530ms/step - loss: 0.4994 - acc: 0.8099 - val_loss: 1.2097 - val_acc: 0.6005

Epoch 00007: val_loss did not improve from 0.88363
Epoch 8/24
229/229 [==============================] - 121s 529ms/step - loss: 0.4382 - acc: 0.8343 - val_loss: 1.1478 - val_acc: 0.6495

Epoch 00008: val_loss did not improve from 0.88363
Epoch 9/24
 34/229 [===>..........................] - ETA: 1:38 - loss: 0.3280 - acc: 0.8897KeyboardInterrupt
CPU times: user 34min, sys: 2min 37s, total: 36min 38s
Wall time: 17min 48s
```
lr=1e-5 neck2_densenet_all_mem_weights.h5
```
Epoch 1/24
229/229 [==============================] - 183s 799ms/step - loss: 0.4840 - acc: 0.8184 - val_loss: 0.8006 - val_acc: 0.7010

Epoch 00001: val_loss improved from inf to 0.80055, saving model to models/neck2_densenet_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 122s 534ms/step - loss: 0.4012 - acc: 0.8533 - val_loss: 0.8131 - val_acc: 0.6998

Epoch 00002: val_loss did not improve from 0.80055
Epoch 3/24
229/229 [==============================] - 122s 533ms/step - loss: 0.3690 - acc: 0.8651 - val_loss: 0.8298 - val_acc: 0.6973

Epoch 00003: val_loss did not improve from 0.80055
Epoch 4/24
229/229 [==============================] - 122s 532ms/step - loss: 0.3350 - acc: 0.8748 - val_loss: 0.8635 - val_acc: 0.6998

Epoch 00004: val_loss did not improve from 0.80055
Epoch 5/24
132/229 [================>.............] - ETA: 49s - loss: 0.3096 - acc: 0.8866KeyboardInterrupt
CPU times: user 20min 12s, sys: 1min 29s, total: 21min 42s
Wall time: 10min 52s


```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： Nasnet
- 模型文件 ：necknasnet_all_mem_weights.h5
- necknasnet2_all_mem_weights.h5
```
cnn_model = NASNetLarge(include_top=False, input_shape=(width, width, 3), weights='imagenet')
inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=1e-5)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 24
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)


%%time
prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}nasnet2_all_mem_weights.h5'.format(prefix_cls), verbose=1, 
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('models/{0}nasnet_all_mem_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```


lr=1e-4 necknasnet_all_mem_weights.h5
```
Epoch 1/24
458/458 [==============================] - 891s 2s/step - loss: 1.1730 - acc: 0.5180 - val_loss: 0.7977 - val_acc: 0.6973

Epoch 00001: val_loss improved from inf to 0.79771, saving model to models/necknasnet_all_mem_weights.h5
Epoch 2/24
458/458 [==============================] - 786s 2s/step - loss: 0.7962 - acc: 0.7004 - val_loss: 0.6819 - val_acc: 0.7475

Epoch 00002: val_loss improved from 0.79771 to 0.68189, saving model to models/necknasnet_all_mem_weights.h5
Epoch 3/24
458/458 [==============================] - 783s 2s/step - loss: 0.6383 - acc: 0.7633 - val_loss: 0.7433 - val_acc: 0.7206

Epoch 00003: val_loss did not improve from 0.68189
Epoch 4/24
458/458 [==============================] - 785s 2s/step - loss: 0.5509 - acc: 0.8033 - val_loss: 0.6701 - val_acc: 0.7610

Epoch 00004: val_loss improved from 0.68189 to 0.67012, saving model to models/necknasnet_all_mem_weights.h5
Epoch 5/24
458/458 [==============================] - 785s 2s/step - loss: 0.4670 - acc: 0.8277 - val_loss: 0.6756 - val_acc: 0.7684

Epoch 00005: val_loss did not improve from 0.67012
Epoch 6/24
458/458 [==============================] - 782s 2s/step - loss: 0.4028 - acc: 0.8546 - val_loss: 0.7472 - val_acc: 0.7623

Epoch 00006: val_loss did not improve from 0.67012
Epoch 7/24
458/458 [==============================] - 788s 2s/step - loss: 0.3539 - acc: 0.8726 - val_loss: 0.7987 - val_acc: 0.7770

Epoch 00007: val_loss did not improve from 0.67012
Epoch 8/24
458/458 [==============================] - 788s 2s/step - loss: 0.3171 - acc: 0.8888 - val_loss: 0.7871 - val_acc: 0.7782

Epoch 00008: val_loss did not improve from 0.67012
Epoch 9/24
 17/458 [>.............................] - ETA: 12:20 - loss: 0.2735 - acc: 0.9007KeyboardInterrupt
CPU times: user 1h 55min 29s, sys: 32min 30s, total: 2h 27min 59s
Wall time: 1h 47min 45s

```
lr=1e-5 necknasnet_all_mem_weights.h5
=>neck_0515_80_nasnet.csv
```
Epoch 1/24
458/458 [==============================] - 895s 2s/step - loss: 0.3665 - acc: 0.8703 - val_loss: 0.5872 - val_acc: 0.7941

Epoch 00001: val_loss improved from inf to 0.58716, saving model to models/necknasnet2_all_mem_weights.h5
Epoch 2/24
458/458 [==============================] - 784s 2s/step - loss: 0.3224 - acc: 0.8820 - val_loss: 0.5895 - val_acc: 0.7978

Epoch 00002: val_loss did not improve from 0.58716
Epoch 3/24
458/458 [==============================] - 786s 2s/step - loss: 0.2837 - acc: 0.9013 - val_loss: 0.6098 - val_acc: 0.8027

Epoch 00003: val_loss did not improve from 0.58716
Epoch 4/24
 71/458 [===>..........................] - ETA: 10:45 - loss: 0.2349 - acc: 0.9129KeyboardInterrupt
CPU times: user 47min 8s, sys: 12min 46s, total: 59min 55s
Wall time: 44min 2s
```
lr=1e-6 necknasnet3_all_mem_weights.h5

```
Epoch 1/24
458/458 [==============================] - 907s 2s/step - loss: 0.3295 - acc: 0.8805 - val_loss: 0.5856 - val_acc: 0.7966

Epoch 00001: val_loss improved from inf to 0.58556, saving model to models/necknasnet3_all_mem_weights.h5
Epoch 2/24
100/458 [=====>........................] - ETA: 10:01 - loss: 0.2981 - acc: 0.8944KeyboardInterrupt
CP
```
lr=1e-5 necknasnet4_all_mem_weights.h5
```
Epoch 1/24
458/458 [==============================] - 890s 2s/step - loss: 0.3186 - acc: 0.8849 - val_loss: 0.5985 - val_acc: 0.8027

Epoch 00001: val_loss improved from inf to 0.59855, saving model to models/necknasnet4_all_mem_weights.h5
Epoch 2/24
458/458 [==============================] - 779s 2s/step - loss: 0.2743 - acc: 0.9013 - val_loss: 0.6165 - val_acc: 0.8015

Epoch 00002: val_loss did not improve from 0.59855
Epoch 3/24
458/458 [==============================] - 779s 2s/step - loss: 0.2576 - acc: 0.9111 - val_loss: 0.6454 - val_acc: 0.7966

Epoch 00003: val_loss did not improve from 0.59855
Epoch 4/24
 71/458 [===>..........................] - ETA: 10:41 - loss: 0.2080 - acc: 0.9296KeyboardInterrupt
CPU times: user 46min 6s, sys: 13min, total: 59min 7s
Wall time: 43min 43s

```


2018.5.13


- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： finetune
- 模型文件 ：
InceptionResNetV2
coat4.best0513_InceptionResNetV2.h5
coat6.best0513_InceptionResNetV2.h5 val_acc=0.8458
=>coat_0513_InceptionResNetV2.csv

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width,
3), weights='imagenet')
inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)
model.load_weights('models/{}3.best0504_InceptionResNetV2.h5'.format(prefix_cls))
#加载以前训练好的模型，继续测试

LAYERS_TO_FREEZE = 300

for layer in
model.layers[2].layers[:LAYERS_TO_FREEZE]:
       layer.trainable = False
for
layer in model.layers[2].layers[LAYERS_TO_FREEZE:]:
       layer.trainable =
True
       
# Compile the model
adam = Adam(lr=0.00001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction =
ReduceLROnPlateau(monitor='val_acc',
#
patience=3,
#                                             verbose=1,
#
factor=0.1,
#                                             min_lr=0.00001)
epochs
= 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =
False, # set input mean to 0 over the dataset
        samplewise_center = False,
# set each sample mean to 0
        featurewise_std_normalization = False, #
divide inputs by std of the dataset
        samplewise_std_normalization =
False, # divide each input by its std
        zca_whitening = False, # apply ZCA
whitening
        rotation_range = 10, # randomly rotate images in the range
(degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
width_shift_range = 0.08, # randomly shift images horizontally (fraction of
total width)
        height_shift_range = 0.08, # randomly shift images
vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly
flip images
        vertical_flip = False,
        shear_range = 0.05,
fill_mode = 'constant',
        cval = 0)
        
        
train_generator =
datagen.flow_from_directory(
        'data-
raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width,
width),
        batch_size=32,
        class_mode='categorical')

#
validation_generator = datagen.flow_fr

prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='models/{0}4.best0513_InceptionResNetV2.h5'.format(prefix_cls),
verbose=1,
save_best_only=True,save_weights_only=True, mode='val_acc')
#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
epochs=epochs,
                                  validation_data = (X_valid,
y_valid),
                                  #validation_data = (X_valid,
y_valid),
                                  verbose=1,
steps_per_epoch= int(len(image_path)) // batch_size,
#validation_steps= int(len(image_path)*0.9) // batch_size,
callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```

训练结果lr=0.00001

```{.python .input}
Epoch 1/32
406/406 [==============================] - 329s
811ms/step - loss: 0.0721 - acc: 0.9726 - val_loss: 0.7198 - val_acc: 0.8416
Epoch 00001: val_loss improved from inf to 0.71976, saving model to
models/coat4.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406
[==============================] - 322s 794ms/step - loss: 0.0761 - acc: 0.9720
- val_loss: 0.7249 - val_acc: 0.8409

Epoch 00002: val_loss did not improve from
0.71976
Epoch 3/32
406/406 [==============================] - 323s 795ms/step -
loss: 0.0741 - acc: 0.9716 - val_loss: 0.7259 - val_acc: 0.8416

Epoch 00003:
val_loss did not improve from 0.71976
Epoch 4/32
406/406
[==============================] - 322s 792ms/step - loss: 0.0732 - acc: 0.9715
- val_loss: 0.7283 - val_acc: 0.8416

Epoch 00004: val_loss did not improve from
0.71976
Epoch 5/32
406/406 [==============================] - 324s 797ms/step -
loss: 0.0749 - acc: 0.9711 - val_loss: 0.7282 - val_acc: 0.8423

Epoch 00005:
val_loss did not improve from 0.71976
Epoch 6/32
406/406
[==============================] - 324s 797ms/step - loss: 0.0764 - acc: 0.9704
- val_loss: 0.7300 - val_acc: 0.8430

Epoch 00006: val_loss did not improve from
0.71976
Epoch 7/32
406/406 [==============================] - 321s 791ms/step -
loss: 0.0745 - acc: 0.9706 - val_loss: 0.7307 - val_acc: 0.8416

Epoch 00007:
val_loss did not improve from 0.71976
Epoch 8/32
406/406
[==============================] - 324s 799ms/step - loss: 0.0739 - acc: 0.9728
- val_loss: 0.7299 - val_acc: 0.8409

Epoch 00008: val_loss did not improve from
0.71976
Epoch 9/32
325/406 [=======================>......] - ETA: 1:01 - loss:
0.0723 - acc: 0.9713KeyboardInterrupt

```

lr=0.000001 coat5.best0513_InceptionResNetV2.h5
coat6.best0513_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406
[==============================] - 384s 945ms/step - loss: 0.0728 - acc: 0.9717
- val_loss: 0.7314 - val_acc: 0.8430

Epoch 00001: val_loss improved from inf to
0.73144, saving model to models/coat5.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 336s 827ms/step - loss: 0.0769 - acc:
0.9705 - val_loss: 0.7316 - val_acc: 0.8444

Epoch 00002: val_loss did not
improve from 0.73144
Epoch 3/32
406/406 [==============================] - 332s
818ms/step - loss: 0.0700 - acc: 0.9732 - val_loss: 0.7304 - val_acc: 0.8437
Epoch 00003: val_loss improved from 0.73144 to 0.73045, saving model to
models/coat5.best0513_InceptionResNetV2.h5
Epoch 4/32
406/406
[==============================] - 331s 816ms/step - loss: 0.0693 - acc: 0.9734
- val_loss: 0.7299 - val_acc: 0.8430

Epoch 00004: val_loss improved from
0.73045 to 0.72994, saving model to models/coat5.best0513_InceptionResNetV2.h5
Epoch 5/32
406/406 [==============================] - 331s 816ms/step - loss:
0.0746 - acc: 0.9704 - val_loss: 0.7274 - val_acc: 0.8402

Epoch 00005: val_loss
improved from 0.72994 to 0.72741, saving model to
models/coat5.best0513_InceptionResNetV2.h5
Epoch 6/32
406/406
[==============================] - 332s 818ms/step - loss: 0.0702 - acc: 0.9729
- val_loss: 0.7339 - val_acc: 0.8451

Epoch 00006: val_loss did not improve from
0.72741
Epoch 7/32
406/406 [==============================] - 332s 817ms/step -
loss: 0.0748 - acc: 0.9728 - val_loss: 0.7307 - val_acc: 0.8423

Epoch 00007:
val_loss did not improve from 0.72741
Epoch 8/32
406/406
[==============================] - 331s 815ms/step - loss: 0.0705 - acc: 0.9747
- val_loss: 0.7304 - val_acc: 0.8430

Epoch 00008: val_loss did not improve from
0.72741
Epoch 9/32
406/406 [==============================] - 332s 817ms/step -
loss: 0.0705 - acc: 0.9743 - val_loss: 0.7308 - val_acc: 0.8458

Epoch 00009:
val_loss did not improve from 0.72741
Epoch 10/32
 40/406
[=>............................] - ETA: 4:24 - loss: 0.0739 - acc:
0.9703KeyboardInterrupt
```

```{.python .input}
Epoch 1/32
406/406
[==============================] - 340s 836ms/step - loss: 0.0657 - acc: 0.9771
- val_loss: 0.7325 - val_acc: 0.8444

Epoch 00001: val_loss improved from inf to
0.73245, saving model to models/coat6.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 336s 827ms/step - loss: 0.0789 - acc:
0.9714 - val_loss: 0.7293 - val_acc: 0.8458

Epoch 00002: val_loss improved from
0.73245 to 0.72932, saving model to models/coat6.best0513_InceptionResNetV2.h5
Epoch 3/32
406/406 [==============================] - 332s 818ms/step - loss:
0.0728 - acc: 0.9727 - val_loss: 0.7358 - val_acc: 0.8423

Epoch 00003: val_loss
did not improve from 0.72932
Epoch 4/32
406/406 [==============================]
- 332s 817ms/step - loss: 0.0710 - acc: 0.9731 - val_loss: 0.7345 - val_acc:
0.8437

Epoch 00004: val_loss did not improve from 0.72932
Epoch 5/32
 10/406
[..............................] - ETA: 4:44 - loss: 0.0435 - acc:
0.9844KeyboardInterrupt
In [14]:
```

l2正则化 coat7.best0513_InceptionResNetV2.h5

```{.python .input}
x = Dense(n_class,
activation='softmax',
name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)
```

lr=0.001，在模型coat6的基础上.best0513_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406 [==============================] - 323s 795ms/step - loss: 226.6585 -
acc: 0.9704 - val_loss: 169.1457 - val_acc: 0.8506

Epoch 00002: val_loss
improved from 298.23037 to 169.14568, saving model to
models/coat7.best0513_InceptionResNetV2.h5
Epoch 3/32
353/406
[=========================>....] - ETA: 40s - loss: 134.4664 - acc:
0.9682KeyboardInterrupt
```

lr=0.0001,coat8.best0513_InceptionResNetV2.h5 0.8520

```{.python .input}
Epoch 1/32
406/406 [==============================] - 380s 936ms/step -
loss: 120.6483 - acc: 0.9677 - val_loss: 85.1202 - val_acc: 0.8444

Epoch 00001:
val_loss improved from inf to 85.12018, saving model to
models/coat8.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406
[==============================] - 322s 794ms/step - loss: 63.1943 - acc: 0.9619
- val_loss: 46.0430 - val_acc: 0.8375

Epoch 00002: val_loss improved from
85.12018 to 46.04296, saving model to models/coat8.best0513_InceptionResNetV2.h5
Epoch 3/32
406/406 [==============================] - 318s 784ms/step - loss:
34.4163 - acc: 0.9600 - val_loss: 25.3421 - val_acc: 0.8299

Epoch 00003:
val_loss improved from 46.04296 to 25.34210, saving model to
models/coat8.best0513_InceptionResNetV2.h5
Epoch 4/32
406/406
[==============================] - 316s 779ms/step - loss: 18.9171 - acc: 0.9581
- val_loss: 14.0404 - val_acc: 0.8333

Epoch 00004: val_loss improved from
25.34210 to 14.04040, saving model to models/coat8.best0513_InceptionResNetV2.h5
Epoch 5/32
406/406 [==============================] - 316s 778ms/step - loss:
10.4160 - acc: 0.9613 - val_loss: 7.8412 - val_acc: 0.8340

Epoch 00005:
val_loss improved from 14.04040 to 7.84125, saving model to
models/coat8.best0513_InceptionResNetV2.h5
Epoch 6/32
406/406
[==============================] - 314s 774ms/step - loss: 5.7484 - acc: 0.9577
- val_loss: 4.4739 - val_acc: 0.8237

Epoch 00006: val_loss improved from
7.84125 to 4.47392, saving model to models/coat8.best0513_InceptionResNetV2.h5
Epoch 7/32
406/406 [==============================] - 315s 775ms/step - loss:
3.2159 - acc: 0.9594 - val_loss: 2.6409 - val_acc: 0.8423

Epoch 00007: val_loss
improved from 4.47392 to 2.64095, saving model to
models/coat8.best0513_InceptionResNetV2.h5
Epoch 8/32
406/406
[==============================] - 315s 776ms/step - loss: 1.8893 - acc: 0.9616
- val_loss: 1.7602 - val_acc: 0.8333

Epoch 00008: val_loss improved from
2.64095 to 1.76023, saving model to models/coat8.best0513_InceptionResNetV2.h5
Epoch 9/32
406/406 [==============================] - 316s 778ms/step - loss:
1.2139 - acc: 0.9639 - val_loss: 1.2753 - val_acc: 0.8520

Epoch 00009: val_loss
improved from 1.76023 to 1.27526, saving model to
models/coat8.best0513_InceptionResNetV2.h5
Epoch 10/32
123/406
[========>.....................] - ETA: 3:27 - loss: 0.9843 - acc:
0.9604KeyboardInterrupt

```

lr=0.0001,coat9.best0513_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406 [==============================] - 321s 790ms/step -
loss: 0.8332 - acc: 0.9671 - val_loss: 1.0601 - val_acc: 0.8320

Epoch 00001:
val_loss improved from inf to 1.06011, saving model to
models/coat9.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406
[==============================] - 320s 787ms/step - loss: 0.7204 - acc: 0.9700
- val_loss: 0.9710 - val_acc: 0.8382

Epoch 00002: val_loss improved from
1.06011 to 0.97095, saving model to models/coat9.best0513_InceptionResNetV2.h5
Epoch 3/32
406/406 [==============================] - 317s 780ms/step - loss:
0.6627 - acc: 0.9720 - val_loss: 0.9508 - val_acc: 0.8458

Epoch 00003: val_loss
improved from 0.97095 to 0.95081, saving model to
models/coat9.best0513_InceptionResNetV2.h5
Epoch 4/32
406/406
[==============================] - 317s 781ms/step - loss: 0.6436 - acc: 0.9711
- val_loss: 0.9231 - val_acc: 0.8506

Epoch 00004: val_loss improved from
0.95081 to 0.92305, saving model to models/coat9.best0513_InceptionResNetV2.h5
Epoch 5/32
406/406 [==============================] - 317s 780ms/step - loss:
0.6312 - acc: 0.9719 - val_loss: 0.9472 - val_acc: 0.8382

Epoch 00005: val_loss
did not improve from 0.92305
Epoch 6/32
406/406 [==============================]
- 322s 792ms/step - loss: 0.6138 - acc: 0.9748 - val_loss: 0.9326 - val_acc:
0.8451

Epoch 00006: val_loss did not improve from 0.92305
Epoch 7/32
 35/406
[=>............................] - ETA: 4:32 - loss: 0.6039 - acc:
0.9795KeyboardInterrupt

```

lr=0.0001,coat10.best0513_InceptionResNetV2.h5
0.8527

```{.python .input}
Epoch 1/32
406/406 [==============================] - 382s
940ms/step - loss: 0.5886 - acc: 0.9825 - val_loss: 0.9078 - val_acc: 0.8506
Epoch 00001: val_loss improved from inf to 0.90783, saving model to
models/coat10.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406
[==============================] - 330s 814ms/step - loss: 0.5756 - acc: 0.9837
- val_loss: 0.9047 - val_acc: 0.8520

Epoch 00002: val_loss improved from
0.90783 to 0.90471, saving model to models/coat10.best0513_InceptionResNetV2.h5
Epoch 3/32
406/406 [==============================] - 326s 804ms/step - loss:
0.5772 - acc: 0.9850 - val_loss: 0.9013 - val_acc: 0.8527

Epoch 00003: val_loss
improved from 0.90471 to 0.90132, saving model to
models/coat10.best0513_InceptionResNetV2.h5
Epoch 4/32
118/406
[=======>......................] - ETA: 3:32 - loss: 0.5775 - acc:
0.9831KeyboardInterrupt

```

lr=0.0001,coat11.best0513_InceptionResNetV2.h5
0.8555

```{.python .input}
Epoch 1/32
406/406 [==============================] - 329s 811ms/step
- loss: 0.5717 - acc: 0.9865 - val_loss: 0.9000 - val_acc: 0.8555

Epoch 00001:
val_loss improved from inf to 0.90001, saving model to
models/coat11.best0513_InceptionResNetV2.h5
Epoch 2/32
149/406
[==========>...................] - ETA: 3:17 - loss: 0.5698 - acc:
0.9862KeyboardInterrupt
In [14]:

```

coat12.best0513_InceptionResNetV2.h5
=>coat_0514_InceptionResNetV2.csv

```{.python .input}
Epoch 1/32
406/406
[==============================] - 330s 812ms/step - loss: 0.5706 - acc: 0.9864
- val_loss: 0.9020 - val_acc: 0.8541

Epoch 00001: val_loss improved from inf to
0.90204, saving model to models/coat12.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 327s 805ms/step - loss: 0.5648 - acc:
0.9880 - val_loss: 0.9038 - val_acc: 0.8534

Epoch 00002: val_loss did not
improve from 0.90204
Epoch 3/32
406/406 [==============================] - 327s
805ms/step - loss: 0.5684 - acc: 0.9856 - val_loss: 0.9046 - val_acc: 0.8562
Epoch 00003: val_loss did not improve from 0.90204
Epoch 4/32
152/406
[==========>...................] - ETA: 3:10 - loss: 0.5687 - acc:
0.9862KeyboardInterrupt

```

coat13.best0513_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406 [==============================] - 341s 839ms/step - loss: 0.5645 - acc:
0.9865 - val_loss: 0.8936 - val_acc: 0.8610

Epoch 00001: val_loss improved from
inf to 0.89364, saving model to models/coat13.best0513_InceptionResNetV2.h5
Epoch 2/32
125/406 [========>.....................] - ETA: 3:38 - loss: 0.5638 -
acc: 0.9885KeyboardInterrupt

```

coat14.best0513_InceptionResNetV2.h5 0.8617

```{.python .input}
Epoch 1/32
406/406 [==============================] - 333s 820ms/step -
loss: 0.5626 - acc: 0.9876 - val_loss: 0.8937 - val_acc: 0.8617

Epoch 00001:
val_loss improved from inf to 0.89371, saving model to
models/coat14.best0513_InceptionResNetV2.h5
Epoch 2/32
 98/406
[======>.......................] - ETA: 3:56 - loss: 0.5699 - acc:
0.9853KeyboardInterrupt

```


coat15.best0513_InceptionResNetV2.h5  0.8672
=>coat_0514_08672_InceptionResNetV2.csv
```
Epoch 1/32
406/406
[==============================] - 389s 957ms/step - loss: 0.5578 - acc: 0.9884
- val_loss: 0.8890 - val_acc: 0.8610

Epoch 00001: val_loss improved from inf to
0.88904, saving model to models/coat15.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 332s 817ms/step - loss: 0.5528 - acc:
0.9894 - val_loss: 0.8937 - val_acc: 0.8562

Epoch 00002: val_loss did not
improve from 0.88904
Epoch 3/32
406/406 [==============================] - 329s
810ms/step - loss: 0.5574 - acc: 0.9876 - val_loss: 0.8898 - val_acc: 0.8568
Epoch 00003: val_loss did not improve from 0.88904
Epoch 4/32
406/406
[==============================] - 326s 804ms/step - loss: 0.5549 - acc: 0.9885
- val_loss: 0.8874 - val_acc: 0.8617

Epoch 00004: val_loss improved from
0.88904 to 0.88744, saving model to models/coat15.best0513_InceptionResNetV2.h5
Epoch 5/32
406/406 [==============================] - 324s 798ms/step - loss:
0.5530 - acc: 0.9892 - val_loss: 0.8770 - val_acc: 0.8672

Epoch 00005: val_loss
improved from 0.88744 to 0.87703, saving model to
models/coat15.best0513_InceptionResNetV2.h5
Epoch 6/32
 94/406
[=====>........................] - ETA: 3:51 - loss: 0.5478 - acc:
0.9904KeyboardInterrupt
```

coat16.best0513_InceptionResNetV2.h5 


```
Epoch
1/32
406/406 [==============================] - 333s 820ms/step - loss: 0.5474 -
acc: 0.9901 - val_loss: 0.8837 - val_acc: 0.8596

Epoch 00001: val_loss improved
from inf to 0.88372, saving model to models/coat16.best0513_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 328s 807ms/step - loss:
0.5535 - acc: 0.9886 - val_loss: 0.8879 - val_acc: 0.8624

Epoch 00002: val_loss
did not improve from 0.88372
Epoch 3/32
406/406 [==============================]
- 327s 807ms/step - loss: 0.5512 - acc: 0.9905 - val_loss: 0.8872 - val_acc:
0.8631

Epoch 00003: val_loss did not improve from 0.88372
Epoch 4/32
406/406
[==============================] - 322s 794ms/step - loss: 0.5456 - acc: 0.9907
- val_loss: 0.8876 - val_acc: 0.8617

Epoch 00004: val_loss did not improve from
0.88372
Epoch 5/32
406/406 [==============================] - 319s 786ms/step -
loss: 0.5501 - acc: 0.9906 - val_loss: 0.8857 - val_acc: 0.8645

Epoch 00005:
val_loss did not improve from 0.88372
Epoch 6/32
406/406
[==============================] - 319s 787ms/step - loss: 0.5441 - acc: 0.9908
- val_loss: 0.8839 - val_acc: 0.8624

Epoch 00006: val_loss did not improve from
0.88372
Epoch 7/32
406/406 [==============================] - 323s 797ms/step -
loss: 0.5484 - acc: 0.9891 - val_loss: 0.8806 - val_acc: 0.8638

Epoch 00007:
val_loss improved from 0.88372 to 0.88057, saving model to
models/coat16.best0513_InceptionResNetV2.h5
Epoch 8/32
 27/406
[>.............................] - ETA: 4:39 - loss: 0.5326 - acc:
0.9942KeyboardInterrupt

```

2018.5.11

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： Xeption
disk
- 模型文件 ：coat2_augment.best0511_Xception.h5
coat3_augment.best0511_Xception.h5

```{.python .input}
#变换的样式
trans_style=[0,1,2,4,5,6]
for i in tqdm(range(n)):
    tmp_label = df_load['label'][i]
#     if len(tmp_label) > n_class:
#         print(df_load['image_id'][i])
    #for j in range(8):
    for j in range(len(trans_style)):#
        X_train[i*n_times+j] = cv2.resize(cv2.imread(outdir + df_load['image_id'][i] + "-%s.jpg"%trans_style[j]), (width, width))
        y_train[i*n_times+j][tmp_label.find('y')] = 1

df_valid.reset_index(inplace=True)
del df_valid['index']

n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')
inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

adam = Adam(lr=0.001)
prefix_cls = cur_class.split('_')[0]

model.compile(optimizer=adam,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

checkpointer = ModelCheckpoint(filepath=save_path, verbose=1, 
                               save_best_only=True)

h = model.fit(X_train, y_train, batch_size=32, epochs=2, 
              callbacks=[EarlyStopping(patience=3), checkpointer], 
              shuffle=True, 
              validation_data=(X_valid, y_valid))

```

训练结果lr=0.001 coat2_augment.best0511_Xception.h5

```{.python .input}
Train on 78048 samples, validate on 1446 samples
Epoch 1/2
78048/78048 [==============================] - 1941s 25ms/step - loss: 1.1093 - acc: 0.5604 - val_loss: 1.2666 - val_acc: 0.5297

Epoch 00001: val_loss improved from inf to 1.26658, saving model to models/coat2_augment.best0511_Xception.h5
Epoch 2/2

```

lr=0.00005 coat3_augment.best0511_Xception.h5

```{.python .input}
Train on 78048 samples, validate on 1446 samples
Epoch 1/1
78048/78048 [==============================] - 1948s 25ms/step - loss: 0.6089 - acc: 0.7547 - val_loss: 0.9355 - val_acc: 0.6508

Epoch 00001: val_loss improved from inf to 0.93551, saving model to models/coat3_augment.best0511_Xception.h5
```

2018.5.10
-----------------------------------------------------------------
-
训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： Xeption disk
- 模型文件
：neck_disk_batch.best0510_xception.h5

```{.python .input}


```

训练结果lr=0.001

训练结果lr=0.0001

训练结果lr=0.00001
--------------------------------------------------------------------------------------
- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： Xeption disk
- 模型文件
：neck_disk_batch.best0510_xception.h5

```{.python .input}
n_ration=0.9
df_valid=df_load[int(n_ration*len(df_load)):]
df_load=df_load[:int(n_ration*len(df_load))]

task = cur_class
image_path = []
for i in range(len(df_load)):
    image_path.append(('data

n_class = len(df_load['label'][0])
width = 299 # 定义图片大小


mkdir_if_not_exist(['data-raw/train_2/train_valid', task])
mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'train'])
#mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'val'])
m = len(list(image_path[0][1]))
for mm in range(m):
    mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'train', str(mm)])
    #mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'val', str(mm)])

    import random, shutil

    n = len(image_path)
    random.seed(1024)
    random.shuffle(image_path)
    #train_count = 0
    for path, label in image_path:
        label_index = list(label).index('y')
        #if train_count < n * 0.9:
        shutil.copy(path,os.path.join('data-raw/train_2/train_valid', task, 'train', str(label_index)))


df_valid.reset_index(inplace=True)
del df_valid['index']

n_valid=len(df_valid)
X_valid = np.zeros((n_valid, width, width, 3), dtype=np.uint8)
y_valid = np.zeros((n_valid, n_class), dtype=np.uint8)
for i in tqdm(range(n_valid)):
    tmp_label = df_valid['label'][i]
    if len(tmp_label) > n_class:
        print(df_load['image_id'][i])
    X_valid[i] = cv2.resize(cv2.imread('data-raw/train_2/{0}'.format(df_valid['image_id'][i])), (width, width))
    y_valid[i][tmp_label.find('y')] = 1

import gc
del df_valid
gc.collect()

cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 64
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=batch_size,
        class_mode='categorical')

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_disk_batch.best0510_xception.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}_memory_batch.best0505_InceptionResNetV2.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

训练结果lr=0.001 neck_disk_batch.best0510_xception.h5

```{.python .input}
Epoch 1/64
458/458 [==============================] - 635s 1s/step - loss: 1.4374 - acc: 0.3669 - val_loss: 1.5451 - val_acc: 0.3701

Epoch 00001: val_loss improved from inf to 1.54509, saving model to models/neck_disk_batch.best0510_xception.h5
Epoch 2/64
458/458 [==============================] - 627s 1s/step - loss: 1.2090 - acc: 0.4994 - val_loss: 1.2405 - val_acc: 0.5086

Epoch 00002: val_loss improved from 1.54509 to 1.24046, saving model to models/neck_disk_batch.best0510_xception.h5
Epoch 3/64
458/458 [==============================] - 628s 1s/step - loss: 1.0664 - acc: 0.5717 - val_loss: 1.1650 - val_acc: 0.5453

Epoch 00003: val_loss improved from 1.24046 to 1.16498, saving model to models/neck_disk_batch.best0510_xception.h5
Epoch 4/64
458/458 [==============================] - 627s 1s/step - loss: 0.9642 - acc: 0.6230 - val_loss: 1.5235 - val_acc: 0.5110

Epoch 00004: val_loss did not improve from 1.16498
Epoch 5/64
458/458 [==============================] - 627s 1s/step - loss: 0.9055 - acc: 0.6498 - val_loss: 1.0897 - val_acc: 0.5858

Epoch 00005: val_loss improved from 1.16498 to 1.08974, saving model to models/neck_disk_batch.best0510_xception.h5
Epoch 6/64
458/458 [==============================] - 627s 1s/step - loss: 0.8535 - acc: 0.6654 - val_loss: 1.2675 - val_acc: 0.5417

Epoch 00006: val_loss did not improve from 1.08974
Epoch 7/64
458/458 [==============================] - 627s 1s/step - loss: 0.8231 - acc: 0.6799 - val_loss: 1.3273 - val_acc: 0.5760

Epoch 00007: val_loss did not improve from 1.08974
Epoch 8/64
 27/458 [>.............................] - ETA: 9:32 - loss: 0.7463 - acc: 0.7269KeyboardInterrupt

```

训练结果lr=0.0001

训练结果lr=0.00001

2018.5.9

- 训练类型：coat
- 前提条件：
- 训练参数和模型
-
环境：google GPU
- keyWords ： Xeption
-
模型文件 ：coat1_all_mem_weights.h5
coat2_all_mem_weights.h5
coat3_all_mem_weights.h5

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

import gc
del X
gc.collect()

# Compile the model
adam = Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 20
batch_size = 8

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)




%%time
prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}1_all_mem_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

训练结果 lr=0.001 coat1_all_mem_weights.h5

```{.python .input}
Epoch 1/24
1626/1626 [==============================] - 410s 252ms/step - loss: 1.6818 - acc: 0.3290 - val_loss: 1.5665 - val_acc: 0.4391

Epoch 00001: val_loss improved from inf to 1.56651, saving model to models/coat1_all_mem_weights.h5
Epoch 2/24
1626/1626 [==============================] - 399s 245ms/step - loss: 1.4798 - acc: 0.4111 - val_loss: 1.5329 - val_acc: 0.4474

Epoch 00002: val_loss improved from 1.56651 to 1.53294, saving model to models/coat1_all_mem_weights.h5
Epoch 3/24
1626/1626 [==============================] - 400s 246ms/step - loss: 1.3738 - acc: 0.4550 - val_loss: 1.3785 - val_acc: 0.4550

Epoch 00003: val_loss improved from 1.53294 to 1.37852, saving model to models/coat1_all_mem_weights.h5
Epoch 4/24
1626/1626 [==============================] - 399s 245ms/step - loss: 1.2837 - acc: 0.4848 - val_loss: 1.2819 - val_acc: 0.5062

Epoch 00004: val_loss improved from 1.37852 to 1.28189, saving model to models/coat1_all_mem_weights.h5
Epoch 5/24
1626/1626 [==============================] - 399s 245ms/step - loss: 1.2022 - acc: 0.5209 - val_loss: 1.3170 - val_acc: 0.5131

Epoch 00005: val_loss did not improve from 1.28189
Epoch 6/24
1626/1626 [==============================] - 398s 245ms/step - loss: 1.1488 - acc: 0.5446 - val_loss: 1.2141 - val_acc: 0.5249

Epoch 00006: val_loss improved from 1.28189 to 1.21409, saving model to models/coat1_all_mem_weights.h5
Epoch 7/24
1626/1626 [==============================] - 399s 246ms/step - loss: 1.0924 - acc: 0.5637 - val_loss: 1.1810 - val_acc: 0.5429

Epoch 00007: val_loss improved from 1.21409 to 1.18101, saving model to models/coat1_all_mem_weights.h5
Epoch 8/24
1626/1626 [==============================] - 398s 245ms/step - loss: 1.0371 - acc: 0.5851 - val_loss: 1.2589 - val_acc: 0.5533

Epoch 00008: val_loss did not improve from 1.18101
Epoch 9/24
1626/1626 [==============================] - 392s 241ms/step - loss: 1.0026 - acc: 0.5992 - val_loss: 1.0910 - val_acc: 0.5726

Epoch 00009: val_loss improved from 1.18101 to 1.09096, saving model to models/coat1_all_mem_weights.h5
Epoch 10/24
1626/1626 [==============================] - 397s 244ms/step - loss: 0.9550 - acc: 0.6195 - val_loss: 1.1273 - val_acc: 0.5692

Epoch 00010: val_loss did not improve from 1.09096
Epoch 11/24
1626/1626 [==============================] - 399s 246ms/step - loss: 0.9118 - acc: 0.6345 - val_loss: 1.0774 - val_acc: 0.5864

Epoch 00011: val_loss improved from 1.09096 to 1.07742, saving model to models/coat1_all_mem_weights.h5
Epoch 12/24
1626/1626 [==============================] - 399s 246ms/step - loss: 0.8821 - acc: 0.6425 - val_loss: 1.1863 - val_acc: 0.5726

Epoch 00012: val_loss did not improve from 1.07742
Epoch 13/24
1626/1626 [==============================] - 400s 246ms/step - loss: 0.8303 - acc: 0.6601 - val_loss: 1.2259 - val_acc: 0.5574

Epoch 00013: val_loss did not improve from 1.07742
Epoch 14/24
1626/1626 [==============================] - 400s 246ms/step - loss: 0.8036 - acc: 0.6779 - val_loss: 1.2333 - val_acc: 0.5678

Epoch 00014: val_loss did not improve from 1.07742
Epoch 15/24
1072/1626 [==================>...........] - ETA: 2:12 - loss: 0.7443 - acc: 0.6989


```

训练结果 lr=0.0001 coat2_all_mem_weights.h5

```{.python .input}
Epoch 1/24
1626/1626 [==============================] - 408s 251ms/step - loss: 0.7360 - acc: 0.7023 - val_loss: 0.9987 - val_acc: 0.6369

Epoch 00001: val_loss improved from inf to 0.99868, saving model to models/coat2_all_mem_weights.h5
Epoch 2/24
1626/1626 [==============================] - 398s 245ms/step - loss: 0.6507 - acc: 0.7345 - val_loss: 1.0421 - val_acc: 0.6321

Epoch 00002: val_loss did not improve from 0.99868
Epoch 3/24
 408/1626 [======>.......................] - ETA: 4:50 - loss: 0.6111 - acc: 0.7580KeyboardInterrupt
CPU times: user 23min 16s, sys: 3min 28s, total: 26min 45s
Wall time: 15min 20s

```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： Xeption
- 模型文件
：neck_all_mem_weights.h5
neck2_all_mem_weights.h5
neck3_all_mem_weights.h5

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

import gc
del X
gc.collect()

# Compile the model
adam = Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 20
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)


datagen.fit(X_train,seed=123)

%%time
prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_all_mem_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('models/{0}_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

训练结果 lr=0.001 neck_all_mem_weights.h5 neck2_all_mem_weights.h5

```{.python .input}
Epoch 1/20
458/458 [==============================] - 205s 447ms/step - loss: 1.4070 - acc: 0.3898 - val_loss: 1.7733 - val_acc: 0.4363

Epoch 00001: val_loss improved from inf to 1.77326, saving model to models/neck_all_mem_weights.h5
Epoch 2/20
458/458 [==============================] - 193s 421ms/step - loss: 1.1660 - acc: 0.5143 - val_loss: 2.3048 - val_acc: 0.3799

Epoch 00002: val_loss did not improve from 1.77326
Epoch 3/20
458/458 [==============================] - 193s 422ms/step - loss: 1.0292 - acc: 0.5921 - val_loss: 1.0006 - val_acc: 0.6471

Epoch 00003: val_loss improved from 1.77326 to 1.00057, saving model to models/neck_all_mem_weights.h5
Epoch 4/20
458/458 [==============================] - 194s 423ms/step - loss: 0.9620 - acc: 0.6242 - val_loss: 0.9730 - val_acc: 0.6324

Epoch 00004: val_loss improved from 1.00057 to 0.97303, saving model to models/neck_all_mem_weights.h5
Epoch 5/20
458/458 [==============================] - 194s 423ms/step - loss: 0.8694 - acc: 0.6666 - val_loss: 1.2805 - val_acc: 0.5784

Epoch 00005: val_loss did not improve from 0.97303
Epoch 6/20
458/458 [==============================] - 193s 421ms/step - loss: 0.8300 - acc: 0.6931 - val_loss: 1.0739 - val_acc: 0.6630

Epoch 00006: val_loss did not improve from 0.97303
Epoch 7/20
458/458 [==============================] - 193s 421ms/step - loss: 0.7844 - acc: 0.7005 - val_loss: 0.8040 - val_acc: 0.7292

Epoch 00007: val_loss improved from 0.97303 to 0.80398, saving model to models/neck_all_mem_weights.h5
Epoch 8/20
458/458 [==============================] - 193s 420ms/step - loss: 0.7450 - acc: 0.7202 - val_loss: 0.7734 - val_acc: 0.6936

Epoch 00008: val_loss improved from 0.80398 to 0.77338, saving model to models/neck_all_mem_weights.h5
Epoch 9/20
458/458 [==============================] - 194s 424ms/step - loss: 0.7066 - acc: 0.7416 - val_loss: 0.6898 - val_acc: 0.7672

Epoch 00009: val_loss improved from 0.77338 to 0.68976, saving model to models/neck_all_mem_weights.h5
Epoch 10/20
458/458 [==============================] - 193s 422ms/step - loss: 0.6981 - acc: 0.7390 - val_loss: 0.8952 - val_acc: 0.6838

Epoch 00010: val_loss did not improve from 0.68976
Epoch 11/20
458/458 [==============================] - 195s 425ms/step - loss: 0.6609 - acc: 0.7429 - val_loss: 0.8163 - val_acc: 0.7230

Epoch 00011: val_loss did not improve from 0.68976
Epoch 12/20
458/458 [==============================] - 193s 421ms/step - loss: 0.6247 - acc: 0.7651 - val_loss: 0.7801 - val_acc: 0.7206

Epoch 00012: val_loss did not improve from 0.68976
Epoch 13/20
458/458 [==============================] - 193s 422ms/step - loss: 0.6249 - acc: 0.7663 - val_loss: 0.6741 - val_acc: 0.7512

Epoch 00013: val_loss improved from 0.68976 to 0.67410, saving model to models/neck_all_mem_weights.h5
Epoch 14/20
458/458 [==============================] - 194s 423ms/step - loss: 0.5934 - acc: 0.7801 - val_loss: 0.7153 - val_acc: 0.7353

Epoch 00014: val_loss did not improve from 0.67410
Epoch 15/20
458/458 [==============================] - 194s 424ms/step - loss: 0.5725 - acc: 0.7839 - val_loss: 0.6819 - val_acc: 0.7561

Epoch 00015: val_loss did not improve from 0.67410
Epoch 16/20
458/458 [==============================] - 194s 423ms/step - loss: 0.5461 - acc: 0.7933 - val_loss: 0.6699 - val_acc: 0.7647

Epoch 00016: val_loss improved from 0.67410 to 0.66990, saving model to models/neck_all_mem_weights.h5
Epoch 17/20
458/458 [==============================] - 194s 424ms/step - loss: 0.5131 - acc: 0.8094 - val_loss: 0.6904 - val_acc: 0.7819

Epoch 00017: val_loss did not improve from 0.66990
Epoch 18/20
458/458 [==============================] - 194s 423ms/step - loss: 0.5077 - acc: 0.8095 - val_loss: 0.7054 - val_acc: 0.7696

Epoch 00018: val_loss did not improve from 0.66990
Epoch 19/20
458/458 [==============================] - 194s 423ms/step - loss: 0.5047 - acc: 0.8147 - val_loss: 0.8060 - val_acc: 0.7328

Epoch 1/20
458/458 [==============================] - 194s 423ms/step - loss: 0.5196 - acc: 0.8050 - val_loss: 0.8518 - val_acc: 0.7279

Epoch 00001: val_loss improved from inf to 0.85177, saving model to models/neck2_all_mem_weights.h5
Epoch 2/20
458/458 [==============================] - 193s 421ms/step - loss: 0.5008 - acc: 0.8161 - val_loss: 0.7159 - val_acc: 0.7512

Epoch 00002: val_loss improved from 0.85177 to 0.71594, saving model to models/neck2_all_mem_weights.h5
Epoch 3/20
458/458 [==============================] - 193s 422ms/step - loss: 0.4961 - acc: 0.8160 - val_loss: 0.6910 - val_acc: 0.7708

Epoch 00003: val_loss improved from 0.71594 to 0.69095, saving model to models/neck2_all_mem_weights.h5
Epoch 4/20
458/458 [==============================] - 193s 421ms/step - loss: 0.4786 - acc: 0.8198 - val_loss: 0.7263 - val_acc: 0.7696

Epoch 00004: val_loss did not improve from 0.69095
Epoch 5/20
458/458 [==============================] - 192s 419ms/step - loss: 0.4692 - acc: 0.8227 - val_loss: 0.6848 - val_acc: 0.7733

Epoch 00005: val_loss improved from 0.69095 to 0.68480, saving model to models/neck2_all_mem_weights.h5
Epoch 6/20
458/458 [==============================] - 193s 421ms/step - loss: 0.4507 - acc: 0.8364 - val_loss: 0.7785 - val_acc: 0.7475

Epoch 00006: val_loss did not improve from 0.68480
Epoch 7/20
458/458 [==============================] - 195s 425ms/step - loss: 0.4286 - acc: 0.8427 - val_loss: 0.7521 - val_acc: 0.7696

Epoch 00007: val_loss did not improve from 0.68480
Epoch 8/20


```

训练结果 lr=0.0001 neck3_all_mem_weights.h5

```{.python .input}
Epoch 1/24
458/458 [==============================] - 205s 448ms/step - loss: 0.3488 - acc: 0.8704 - val_loss: 0.6572 - val_acc: 0.7917

Epoch 00001: val_loss improved from inf to 0.65720, saving model to models/neck3_all_mem_weights.h5
Epoch 2/24
458/458 [==============================] - 194s 423ms/step - loss: 0.3127 - acc: 0.8837 - val_loss: 0.6452 - val_acc: 0.8027

Epoch 00002: val_loss improved from 0.65720 to 0.64521, saving model to models/neck3_all_mem_weights.h5
Epoch 3/24
458/458 [==============================] - 193s 422ms/step - loss: 0.3058 - acc: 0.8885 - val_loss: 0.6518 - val_acc: 0.8051

Epoch 00003: val_loss did not improve from 0.64521
Epoch 4/24
458/458 [==============================] - 194s 424ms/step - loss: 0.2745 - acc: 0.8970 - val_loss: 0.7031 - val_acc: 0.7941

Epoch 00004: val_loss did not improve from 0.64521
Epoch 5/24
458/458 [==============================] - 194s 423ms/step - loss: 0.2661 - acc: 0.9004 - val_loss: 0.7267 - val_acc: 0.7978

Epoch 00005: val_loss did not improve from 0.64521
Epoch 6/24
458/458 [==============================] - 194s 423ms/step - loss: 0.2508 - acc: 0.9062 - val_loss: 0.7339 - val_acc: 0.7966

Epoch 00006: val_loss did not improve from 0.64521
Epoch 7/24
458/458 [==============================] - 193s 422ms/step - loss: 0.2245 - acc: 0.9150 - val_loss: 0.7425 - val_acc: 0.7941

Epoch 00007: val_loss did not improve from 0.64521
Epoch 8/24
170/458 [==========>...................] - ETA: 1:57 - loss: 0.2502 - acc: 0.9038KeyboardInterrupt
CPU times: user 34min 56s, sys: 5min 49s, total: 40min 45s

```

训练结果 lr=0.00001 neck4_all_mem_weights.h5
=>neck_0509_81_Xception.csv

```{.python .input}
Epoch 1/24
458/458 [==============================] - 204s 446ms/step - loss: 0.2847 - acc: 0.8925 - val_loss: 0.6405 - val_acc: 0.8088

Epoch 00001: val_loss improved from inf to 0.64049, saving model to models/neck4_all_mem_weights.h5
Epoch 2/24
458/458 [==============================] - 194s 422ms/step - loss: 0.2888 - acc: 0.8910 - val_loss: 0.6413 - val_acc: 0.8100

Epoch 00002: val_loss did not improve from 0.64049
Epoch 3/24
458/458 [==============================] - 194s 423ms/step - loss: 0.2833 - acc: 0.8974 - val_loss: 0.6444 - val_acc: 0.8076

Epoch 00003: val_loss did not improve from 0.64049
Epoch 4/24
458/458 [==============================] - 194s 424ms/step - loss: 0.2749 - acc: 0.9011 - val_loss: 0.6515 - val_acc: 0.8076

Epoch 00004: val_loss did not improve from 0.64049
Epoch 5/24
458/458 [==============================] - 193s 422ms/step - loss: 0.2814 - acc: 0.8948 - val_loss: 0.6600 - val_acc: 0.8064

Epoch 00005: val_loss did not improve from 0.64049
Epoch 6/24
458/458 [==============================] - 193s 422ms/step - loss: 0.2743 - acc: 0.8992 - val_loss: 0.6604 - val_acc: 0.8088

Epoch 00006: val_loss did not improve from 0.64049
Epoch 7/24
458/458 [==============================] - 194s 423ms/step - loss: 0.2614 - acc: 0.9023 - val_loss: 0.6599 - val_acc: 0.8088

Epoch 00007: val_loss did not improve from 0.64049
Epoch 8/24
152/458 [========>.....................] - ETA: 2:05 - loss: 0.2719 - acc: 0.8947KeyboardInterrupt
CPU times: user 34min 43s, sys: 5min 49s, total: 40min 33s
Wall time: 24min

```
neck6_all_mem_weights.h5'
```
Epoch 1/24
229/229 [==============================] - 192s 837ms/step - loss: 0.2814 - acc: 0.8939 - val_loss: 0.6752 - val_acc: 0.8100

Epoch 00001: val_loss improved from inf to 0.67516, saving model to models/neck6_all_mem_weights.h5
Epoch 2/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2593 - acc: 0.9046 - val_loss: 0.6685 - val_acc: 0.8039

Epoch 00002: val_loss improved from 0.67516 to 0.66852, saving model to models/neck6_all_mem_weights.h5
Epoch 3/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2562 - acc: 0.9046 - val_loss: 0.7045 - val_acc: 0.8015

Epoch 00003: val_loss did not improve from 0.66852
Epoch 4/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2503 - acc: 0.9037 - val_loss: 0.7287 - val_acc: 0.7904

Epoch 00004: val_loss did not improve from 0.66852
Epoch 5/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2346 - acc: 0.9118 - val_loss: 0.7365 - val_acc: 0.7978

Epoch 00005: val_loss did not improve from 0.66852
Epoch 6/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2208 - acc: 0.9184 - val_loss: 0.7702 - val_acc: 0.7855

Epoch 00006: val_loss did not improve from 0.66852
Epoch 7/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2161 - acc: 0.9168 - val_loss: 0.7688 - val_acc: 0.7978

Epoch 00007: val_loss did not improve from 0.66852
Epoch 8/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2254 - acc: 0.9174 - val_loss: 0.7701 - val_acc: 0.7966

Epoch 00008: val_loss did not improve from 0.66852
Epoch 9/24
229/229 [==============================] - 181s 790ms/step - loss: 0.2030 - acc: 0.9216 - val_loss: 0.8022 - val_acc: 0.7917

Epoch 00009: val_loss did not improve from 0.66852
Epoch 10/24
229/229 [==============================] - 181s 790ms/step - loss: 0.1987 - acc: 0.9256 - val_loss: 0.8103 - val_acc: 0.7917

Epoch 00010: val_loss did not improve from 0.66852
Epoch 11/24
 55/229 [======>.......................] - ETA: 2:13 - loss: 0.1922 - acc: 0.9313KeyboardInterrupt
CPU times: user 43min 45s, sys: 8min 43s, total: 52min 29s
Wall time: 31min 12s
```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： XeptionResNetV2  
-
模型文件 ：neck_l2_new2.best0507_InceptionResNetV2.h5

```{.python .input}
cnn_model = Xception(include_top=False, weights='imagenet',input_shape=(width, width, 3))

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

# Compile the model
adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)
train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=batch_size,
        class_mode='categorical')

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}r2_1.best0509_Xception.h5'.format(prefix_cls), verbose=1,
                             save_best_only=True,save_weights_only=True, mode='val_acc')

try:
  # Fit the model
  history = model.fit_generator(train_generator,
                                epochs=epochs,
                                validation_data = (X_valid, y_valid),
                                #validation_data = (X_valid, y_valid),
                                verbose=1,
                                steps_per_epoch= int(len(image_path)) // batch_size,
                                #validation_steps= int(len(image_path)*0.9) // batch_size,
                                callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
  print('KeyboardInterrupt')

```

训练结果lr=0.001 neckr2_1.best0509_Xception.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 196s 856ms/step - loss: 1.2252 - acc: 0.5005 - val_loss: 2.9554 - val_acc: 0.3824

Epoch 00001: val_loss improved from inf to 2.95542, saving model to models/neckr2_1.best0509_Xception.h5
Epoch 2/32
229/229 [==============================] - 181s 788ms/step - loss: 0.9865 - acc: 0.6156 - val_loss: 1.5554 - val_acc: 0.4988

Epoch 00002: val_loss improved from 2.95542 to 1.55536, saving model to models/neckr2_1.best0509_Xception.h5
Epoch 3/32
229/229 [==============================] - 180s 788ms/step - loss: 0.8674 - acc: 0.6682 - val_loss: 1.3900 - val_acc: 0.4951

Epoch 00003: val_loss improved from 1.55536 to 1.39001, saving model to models/neckr2_1.best0509_Xception.h5
Epoch 4/32
229/229 [==============================] - 180s 788ms/step - loss: 0.7969 - acc: 0.6908 - val_loss: 1.0441 - val_acc: 0.6275

Epoch 00004: val_loss improved from 1.39001 to 1.04412, saving model to models/neckr2_1.best0509_Xception.h5
Epoch 5/32
229/229 [==============================] - 181s 788ms/step - loss: 0.7211 - acc: 0.7233 - val_loss: 0.8768 - val_acc: 0.6875

Epoch 00005: val_loss improved from 1.04412 to 0.87685, saving model to models/neckr2_1.best0509_Xception.h5
Epoch 6/32
229/229 [==============================] - 181s 790ms/step - loss: 0.6859 - acc: 0.7379 - val_loss: 1.3536 - val_acc: 0.6115

Epoch 00006: val_loss did not improve from 0.87685
Epoch 7/32
229/229 [==============================] - 180s 786ms/step - loss: 0.6450 - acc: 0.7547 - val_loss: 0.9107 - val_acc: 0.6826

Epoch 00007: val_loss did not improve from 0.87685
Epoch 8/32
229/229 [==============================] - 180s 788ms/step - loss: 0.5966 - acc: 0.7755 - val_loss: 1.4177 - val_acc: 0.5919

Epoch 00008: val_loss did not improve from 0.87685
Epoch 9/32
229/229 [==============================] - 181s 789ms/step - loss: 0.5771 - acc: 0.7787 - val_loss: 2.2906 - val_acc: 0.4167

Epoch 00009: val_loss did not improve from 0.87685
Epoch 10/32
229/229 [==============================] - 181s 789ms/step - loss: 0.5363 - acc: 0.7973 - val_loss: 0.9392 - val_acc: 0.6618

Epoch 00010: val_loss did not improve from 0.87685
Epoch 11/32
229/229 [==============================] - 180s 788ms/step - loss: 0.5055 - acc: 0.8078 - val_loss: 1.0590 - val_acc: 0.6458

Epoch 00011: val_loss did not improve from 0.87685
Epoch 12/32
122/229 [==============>...............] - ETA: 1:21 - loss: 0.4537 - acc: 0.8316KeyboardInterrupt
```

lr=0.0001 neckr2_2.best0509_Xception.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 192s 837ms/step - loss: 0.5491 - acc: 0.7928 - val_loss: 0.8970 - val_acc: 0.6850

Epoch 00001: val_loss improved from inf to 0.89701, saving model to models/neckr2_2.best0509_Xception.h5
Epoch 2/32
229/229 [==============================] - 181s 792ms/step - loss: 0.4756 - acc: 0.8242 - val_loss: 0.8913 - val_acc: 0.7010

Epoch 00002: val_loss improved from 0.89701 to 0.89135, saving model to models/neckr2_2.best0509_Xception.h5
Epoch 3/32
229/229 [==============================] - 181s 793ms/step - loss: 0.4296 - acc: 0.8364 - val_loss: 0.9444 - val_acc: 0.7047

Epoch 00003: val_loss did not improve from 0.89135
Epoch 4/32
229/229 [==============================] - 181s 793ms/step - loss: 0.3831 - acc: 0.8548 - val_loss: 0.8953 - val_acc: 0.7071

Epoch 00004: val_loss did not improve from 0.89135
Epoch 5/32
229/229 [==============================] - 182s 793ms/step - loss: 0.3534 - acc: 0.8703 - val_loss: 1.0786 - val_acc: 0.6887

Epoch 00005: val_loss did not improve from 0.89135
Epoch 6/32
229/229 [==============================] - 182s 793ms/step - loss: 0.3239 - acc: 0.8789 - val_loss: 1.0824 - val_acc: 0.6912

Epoch 00006: val_loss did not improve from 0.89135
Epoch 7/32
229/229 [==============================] - 181s 793ms/step - loss: 0.2813 - acc: 0.8957 - val_loss: 1.0721 - val_acc: 0.6936

Epoch 00007: val_loss did not improve from 0.89135
Epoch 8/32
229/229 [==============================] - 182s 793ms/step - loss: 0.2599 - acc: 0.9002 - val_loss: 1.0627 - val_acc: 0.7157

Epoch 00008: val_loss did not improve from 0.89135
Epoch 9/32
229/229 [==============================] - 182s 793ms/step - loss: 0.2427 - acc: 0.9100 - val_loss: 1.3757 - val_acc: 0.6850

Epoch 00009: val_loss did not improve from 0.89135
Epoch 10/32
229/229 [==============================] - 181s 792ms/step - loss: 0.2070 - acc: 0.9251 - val_loss: 1.3302 - val_acc: 0.6814

Epoch 00010: val_loss did not improve from 0.89135
Epoch 11/32
229/229 [==============================] - 182s 793ms/step - loss: 0.1968 - acc: 0.9296 - val_loss: 1.5782 - val_acc: 0.6654

Epoch 00011: val_loss did not improve from 0.89135
Epoch 12/32
147/229 [==================>...........] - ETA: 1:02 - loss: 0.1733 - acc: 0.9354KeyboardInterru
```

lr=0.0001 neckr2_3.best0509_Xception.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 182s 795ms/step - loss: 0.4212 - acc: 0.8440 - val_loss: 0.9734 - val_acc: 0.6961

Epoch 00001: val_loss improved from inf to 0.97344, saving model to models/neckr2_3.best0509_Xception.h5
Epoch 2/32
229/229 [==============================] - 181s 792ms/step - loss: 0.3873 - acc: 0.8529 - val_loss: 1.0683 - val_acc: 0.7022

Epoch 00002: val_loss did not improve from 0.97344
Epoch 3/32
229/229 [==============================] - 181s 792ms/step - loss: 0.3543 - acc: 0.8648 - val_loss: 0.9294 - val_acc: 0.7120

Epoch 00003: val_loss improved from 0.97344 to 0.92936, saving model to models/neckr2_3.best0509_Xception.h5
Epoch 4/32
229/229 [==============================] - 181s 792ms/step - loss: 0.3398 - acc: 0.8752 - val_loss: 1.0800 - val_acc: 0.7034

Epoch 00004: val_loss did not improve from 0.92936
Epoch 5/32
229/229 [==============================] - 181s 792ms/step - loss: 0.3162 - acc: 0.8808 - val_loss: 1.1297 - val_acc: 0.6973

Epoch 00005: val_loss did not improve from 0.92936
Epoch 6/32
229/229 [==============================] - 181s 792ms/step - loss: 0.2903 - acc: 0.8912 - val_loss: 0.9782 - val_acc: 0.6998

Epoch 00006: val_loss did not improve from 0.92936
Epoch 7/32
229/229 [==============================] - 181s 792ms/step - loss: 0.2620 - acc: 0.9057 - val_loss: 1.1636 - val_acc: 0.6973

Epoch 00007: val_loss did not improve from 0.929
```

2018.5.7

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：
InceptionResNetV2  l2=5
- 模型文件 ：neck_l2_new2.best0507_InceptionResNetV2.h5

```{.python .input}
#随机切分数据
n_ration=0.1
df_valid=df_load.sample(n=None, frac=n_ration, replace=False, weights=None, random_state=12, axis=0)
#

#求两个列表的差集
def difference(left, right, on):
    """
    difference of two dataframes
    :param left: left dataframe
    :param right: right dataframe
    :param on: join key
    :return: difference dataframe
    """
    df = pd.merge(left, right, how='left', on=on)
    left_columns = left.columns
    col_y = df.columns[left_columns.size]
    df = df[df[col_y].isnull()]
    df = df.ix[:, 0:left_columns.size]
    df.columns = left_columns
    return df

df_load=difference(df_load,df_valid,'image_id')#the train data

df_valid.reset_index(inplace=True)
del df_valid['index']
df_load.reset_index(inplace=True)
del df_load['index']


cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation='relu', name='relu_layer1')(x)

x = Dense(256, activation='relu', name='relu_layer2')(x)

x = Dense(64, activation='relu', name='relu_layer3')(x)

x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

# Compile the model
adam = Adam(lr=0.00001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)

train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_l2_new2.best0507_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}3_l2.best0507_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

训练结果lr=0.001  neck_l2_new2.best0507_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 245s 1s/step - loss: 18.1046 - acc: 0.3244 - val_loss: 15.0168 - val_acc: 0.1583

Epoch 00001: val_loss improved from inf to 15.01681, saving model to models/neck_l2_new2.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 192s 840ms/step - loss: 2.1246 - acc: 0.4583 - val_loss: 2.8998 - val_acc: 0.2209

Epoch 00002: val_loss improved from 15.01681 to 2.89983, saving model to models/neck_l2_new2.best0507_InceptionResNetV2.h5
Epoch 3/32
229/229 [==============================] - 193s 842ms/step - loss: 1.2039 - acc: 0.4914 - val_loss: 1.5728 - val_acc: 0.3791

Epoch 00003: val_loss improved from 2.89983 to 1.57277, saving model to models/neck_l2_new2.best0507_InceptionResNetV2.h5
Epoch 4/32
229/229 [==============================] - 192s 840ms/step - loss: 1.1113 - acc: 0.5248 - val_loss: 1.3919 - val_acc: 0.4503

Epoch 00004: val_loss improved from 1.57277 to 1.39189, saving model to models/neck_l2_new2.best0507_InceptionResNetV2.h5
Epoch 5/32
229/229 [==============================] - 192s 840ms/step - loss: 1.0393 - acc: 0.5545 - val_loss: 1.4340 - val_acc: 0.4638

Epoch 00005: val_loss did not improve from 1.39189
Epoch 6/32
229/229 [==============================] - 193s 843ms/step - loss: 1.0052 - acc: 0.5756 - val_loss: 1.6040 - val_acc: 0.3632

Epoch 00006: val_loss did not improve from 1.39189
Epoch 7/32
229/229 [==============================] - 192s 837ms/step - loss: 0.9358 - acc: 0.6357 - val_loss: 1.3333 - val_acc: 0.5252

Epoch 00007: val_loss improved from 1.39189 to 1.33327, saving model to models/neck_l2_new2.best0507_InceptionResNetV2.h5
Epoch 8/32
229/229 [==============================] - 192s 840ms/step - loss: 0.9126 - acc: 0.6444 - val_loss: 1.3888 - val_acc: 0.4380

Epoch 00008: val_loss did not improve from 1.33327
Epoch 9/32
229/229 [==============================] - 192s 840ms/step - loss: 0.8617 - acc: 0.6821 - val_loss: 1.5475 - val_acc: 0.4049

Epoch 00009: val_loss did not improve from 1.33327
Epoch 10/32
229/229 [==============================] - 192s 840ms/step - loss: 0.8211 - acc: 0.7013 - val_loss: 1.5097 - val_acc: 0.4834

Epoch 00010: val_loss did not improve from 1.33327
Epoch 11/32
 41/229 [====>.........................] - ETA: 2:29 - loss: 0.7244 - acc: 0.7454KeyboardInterrupt
```

训练结果lr=0.0001  neck_l2_new3.best0507_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 242s 1s/step - loss: 0.7837 - acc: 0.7073 - val_loss: 1.4255 - val_acc: 0.4896

Epoch 00001: val_loss improved from inf to 1.42549, saving model to models/neck_l2_new3.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 192s 841ms/step - loss: 0.6754 - acc: 0.7480 - val_loss: 1.3131 - val_acc: 0.5423

Epoch 00002: val_loss improved from 1.42549 to 1.31308, saving model to models/neck_l2_new3.best0507_InceptionResNetV2.h5
Epoch 3/32
229/229 [==============================] - 193s 843ms/step - loss: 0.6420 - acc: 0.7642 - val_loss: 1.2612 - val_acc: 0.5362

Epoch 00003: val_loss improved from 1.31308 to 1.26121, saving model to models/neck_l2_new3.best0507_InceptionResNetV2.h5
Epoch 4/32
229/229 [==============================] - 193s 843ms/step - loss: 0.6106 - acc: 0.7749 - val_loss: 1.2154 - val_acc: 0.5693

Epoch 00004: val_loss improved from 1.26121 to 1.21537, saving model to models/neck_l2_new3.best0507_InceptionResNetV2.h5
Epoch 5/32
229/229 [==============================] - 192s 840ms/step - loss: 0.5723 - acc: 0.7916 - val_loss: 1.2704 - val_acc: 0.5632

Epoch 00005: val_loss did not improve from 1.21537
Epoch 6/32
229/229 [==============================] - 193s 841ms/step - loss: 0.5230 - acc: 0.8123 - val_loss: 1.2652 - val_acc: 0.5521

Epoch 00006: val_loss did not improve from 1.21537
Epoch 7/32
 35/229 [===>..........................] - ETA: 2:34 - loss: 0.5181 - acc: 0.8125KeyboardInterrupt
```

训练结果lr=0.00001  neck_l2_new4.best0507_InceptionResNetV2.h5

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： InceptionResNetV2
l2=5
- 模型文件
：neck__l2_new.best0507_InceptionResNetV2.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation='relu', name='relu_layer1')(x)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu', name='relu_layer2')(x)
x = Dropout(0.5)(x)
x = Dense(64, activation='relu', name='relu_layer3')(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

# Compile the model
adam = Adam(lr=0.00001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)

train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_l2_new.best0507_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}3_l2.best0507_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

训练结果lr=0.001 neck_l2_new.best0507_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 253s 1s/step - loss: 18.9217 - acc: 0.2462 - val_loss: 4.7554 - val_acc: 0.2782

Epoch 00001: val_loss improved from inf to 4.75544, saving model to models/neck_l2_new.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 193s 842ms/step - loss: 2.5004 - acc: 0.2799 - val_loss: 1.7092 - val_acc: 0.1863

Epoch 00002: val_loss improved from 4.75544 to 1.70925, saving model to models/neck_l2_new.best0507_InceptionResNetV2.h5
Epoch 3/32
229/229 [==============================] - 193s 842ms/step - loss: 1.5215 - acc: 0.3168 - val_loss: 2.0851 - val_acc: 0.3051

Epoch 00003: val_loss did not improve from 1.70925
Epoch 4/32
229/229 [==============================] - 192s 840ms/step - loss: 1.4199 - acc: 0.3350 - val_loss: 1.8739 - val_acc: 0.2708

Epoch 00004: val_loss did not improve from 1.70925
Epoch 5/32
229/229 [==============================] - 193s 841ms/step - loss: 1.3981 - acc: 0.3333 - val_loss: 1.5299 - val_acc: 0.2745

Epoch 00005: val_loss improved from 1.70925 to 1.52993, saving model to models/neck_l2_new.best0507_InceptionResNetV2.h5
Epoch 6/32
229/229 [==============================] - 193s 842ms/step - loss: 1.3682 - acc: 0.3477 - val_loss: 1.5337 - val_acc: 0.3064

Epoch 00006: val_loss did not improve from 1.52993
Epoch 7/32
229/229 [==============================] - 192s 839ms/step - loss: 1.3470 - acc: 0.3615 - val_loss: 1.4518 - val_acc: 0.3370

Epoch 00007: val_loss improved from 1.52993 to 1.45178, saving model to models/neck_l2_new.best0507_InceptionResNetV2.h5
Epoch 8/32
229/229 [==============================] - 193s 842ms/step - loss: 1.3338 - acc: 0.3650 - val_loss: 1.4812 - val_acc: 0.3272

Epoch 00008: val_loss did not improve from 1.45178
Epoch 9/32
135/229 [================>.............] - ETA: 1:15 - loss: 1.3034 - acc: 0.3759KeyboardInterrupt

```

训练结果lr=0.0001

训练结果lr=0.00001

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
-
keyWords ： InceptionResNetV2
l2=5
- 模型文件 ：neck_l2.best0507_InceptionResNetV2.h5
neck2_l2.best0507_InceptionResNetV2.h5
neck3_l2.best0507_InceptionResNetV2.h5
neck4_l2.best0507_InceptionResNetV2.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(1000, activation='relu', name='relu_layer')(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(5),bias_regularizer=regularizers.l2(5))(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

adam = Adam(lr=0.00001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)


train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')  

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}2_l2.best0507_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')      

```

训练结果 lr=0.001 neck2_l2.best0507_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 249s 1s/step - loss: 6.3803 - acc: 0.3334 - val_loss: 8.2504 - val_acc: 0.1458

Epoch 00001: val_loss improved from inf to 8.25039, saving model to models/neck2_l2.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 193s 843ms/step - loss: 1.2557 - acc: 0.4813 - val_loss: 1.6842 - val_acc: 0.2843

Epoch 00002: val_loss improved from 8.25039 to 1.68424, saving model to models/neck2_l2.best0507_InceptionResNetV2.h5
Epoch 3/32
229/229 [==============================] - 193s 843ms/step - loss: 1.1135 - acc: 0.5333 - val_loss: 1.5384 - val_acc: 0.3873

Epoch 00003: val_loss improved from 1.68424 to 1.53842, saving model to models/neck2_l2.best0507_InceptionResNetV2.h5
Epoch 4/32
229/229 [==============================] - 194s 846ms/step - loss: 1.0536 - acc: 0.5784 - val_loss: 1.3772 - val_acc: 0.4853

Epoch 00004: val_loss improved from 1.53842 to 1.37716, saving model to models/neck2_l2.best0507_InceptionResNetV2.h5
Epoch 5/32
229/229 [==============================] - 194s 848ms/step - loss: 0.9450 - acc: 0.6463 - val_loss: 1.2992 - val_acc: 0.4828

Epoch 00005: val_loss improved from 1.37716 to 1.29919, saving model to models/neck2_l2.best0507_InceptionResNetV2.h5
Epoch 6/32
229/229 [==============================] - 195s 850ms/step - loss: 0.8871 - acc: 0.6779 - val_loss: 1.2674 - val_acc: 0.5380

Epoch 00006: val_loss improved from 1.29919 to 1.26739, saving model to models/neck2_l2.best0507_InceptionResNetV2.h5
Epoch 7/32
229/229 [==============================] - 193s 841ms/step - loss: 0.8178 - acc: 0.7101 - val_loss: 1.3196 - val_acc: 0.5123

Epoch 00007: val_loss did not improve from 1.26739
Epoch 8/32
229/229 [==============================] - 193s 841ms/step - loss: 0.7663 - acc: 0.7328 - val_loss: 1.4401 - val_acc: 0.5184

Epoch 00008: val_loss did not improve from 1.26739
Epoch 9/32
229/229 [==============================] - 192s 840ms/step - loss: 0.7160 - acc: 0.7553 - val_loss: 1.5437 - val_acc: 0.4865

```

训练结果lr=0.0001 neck3_l2.best0507_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 242s 1s/step - loss: 0.6835 - acc: 0.7677 - val_loss: 1.2042 - val_acc: 0.5821

Epoch 00001: val_loss improved from inf to 1.20417, saving model to models/neck3_l2.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 192s 837ms/step - loss: 0.6002 - acc: 0.7977 - val_loss: 1.1738 - val_acc: 0.5980

Epoch 00002: val_loss improved from 1.20417 to 1.17379, saving model to models/neck3_l2.best0507_InceptionResNetV2.h5
Epoch 3/32
229/229 [==============================] - 192s 838ms/step - loss: 0.5449 - acc: 0.8122 - val_loss: 1.2034 - val_acc: 0.5833

Epoch 00003: val_loss did not improve from 1.17379
Epoch 4/32
229/229 [==============================] - 192s 837ms/step - loss: 0.5127 - acc: 0.8311 - val_loss: 1.2140 - val_acc: 0.5772

Epoch 00004: val_loss did not improve from 1.17379
Epoch 5/32
229/229 [==============================] - 191s 834ms/step - loss: 0.4791 - acc: 0.8473 - val_loss: 1.2548 - val_acc: 0.6066

Epoch 00005: val_loss did not improve from 1.17379
Epoch 6/32
229/229 [==============================] - 191s 836ms/step - loss: 0.4269 - acc: 0.8656 - val_loss: 1.1722 - val_acc: 0.6360

Epoch 00006: val_loss improved from 1.17379 to 1.17225, saving model to models/neck3_l2.best0507_InceptionResNetV2.h5
Epoch 7/32
229/229 [==============================] - 191s 835ms/step - loss: 0.4024 - acc: 0.8737 - val_loss: 1.3447 - val_acc: 0.6262

Epoch 00007: val_loss did not improve from 1.17225
Epoch 8/32
229/229 [==============================] - 191s 834ms/step - loss: 0.3788 - acc: 0.8812 - val_loss: 1.1395 - val_acc: 0.6483

Epoch 00008: val_loss improved from 1.17225 to 1.13953, saving model to models/neck3_l2.best0507_InceptionResNetV2.h5
Epoch 9/32
229/229 [==============================] - 191s 835ms/step - loss: 0.3437 - acc: 0.8935 - val_loss: 1.2476 - val_acc: 0.6213

Epoch 00009: val_loss did not improve from 1.13953
Epoch 10/32
229/229 [==============================] - 191s 833ms/step - loss: 0.3232 - acc: 0.9032 - val_loss: 1.3912 - val_acc: 0.6078

Epoch 00010: val_loss did not improve from 1.13953
Epoch 11/32
 51/229 [=====>........................] - ETA: 2:20 - loss: 0.2933 - acc: 0.9161KeyboardInterrupt

```

训练结果lr=0.00001 neck4_l2.best0507_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
229/229 [==============================] - 246s 1s/step - loss: 0.3417 - acc: 0.8984 - val_loss: 1.1924 - val_acc: 0.6422

Epoch 00001: val_loss improved from inf to 1.19240, saving model to models/neck4_l2.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 192s 837ms/step - loss: 0.3216 - acc: 0.9024 - val_loss: 1.2505 - val_acc: 0.6324

Epoch 00002: val_loss did not improve from 1.19240
Epoch 3/32
229/229 [==============================] - 192s 836ms/step - loss: 0.3045 - acc: 0.9140 - val_loss: 1.2568 - val_acc: 0.6287

Epoch 00003: val_loss did not improve from 1.19240
Epoch 4/32
229/229 [==============================] - 192s 837ms/step - loss: 0.2953 - acc: 0.9135 - val_loss: 1.2503 - val_acc: 0.6336

Epoch 00004: val_loss did not improve from 1.19240
Epoch 5/32
229/229 [==============================] - 191s 836ms/step - loss: 0.2869 - acc: 0.9181 - val_loss: 1.3357 - val_acc: 0.6324

Epoch 00005: val_loss did not improve from 1.19240

```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： InceptionResNetV2
l2=0.1
- 模型文件 ：neck_l2.best0507_InceptionResNetV2.h5'

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(1000, activation='relu', name='relu_layer')(x)
x = Dense(n_class, activation='softmax', name='softmax',kernel_regularizer=regularizers.l2(0.1),bias_regularizer=regularizers.l2(0.1))(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

adam = Adam(lr=0.00001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)


train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')  

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}_l2.best0507_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True,save_weights_only=True, mode='val_acc')

#model.load_weights('models/{0}2.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')      

```

训练结果 lr=0.00001

```{.python .input}

Epoch 1/32
229/229 [==============================] - 249s 1s/step - loss: 2.4962 - acc: 0.3097 - val_loss: 2.3727 - val_acc: 0.3958

Epoch 00001: val_loss improved from inf to 2.37271, saving model to models/neck_l2.best0507_InceptionResNetV2.h5
Epoch 2/32
229/229 [==============================] - 193s 842ms/step - loss: 2.2339 - acc: 0.4453 - val_loss: 2.1538 - val_acc: 0.4828

Epoch 00002: val_loss improved from 2.37271 to 2.15378, saving model to models/neck_l2.best0507_InceptionResNetV2.h5
Epoch 3/32
229/229 [==============================] - 193s 841ms/step - loss: 1.9803 - acc: 0.5353 - val_loss: 1.9975 - val_acc: 0.5245

Epoch 00003: val_loss improved from 2.15378 to 1.99754, saving model to models/neck_l2.best0507_InceptionResNetV2.h5
Epoch 4/32
229/229 [==============================] - 192s 840ms/step - loss: 1.7483 - acc: 0.6391 - val_loss: 1.8817 - val_acc: 0.5662

Epoch 00004: val_loss improved from 1.99754 to 1.88170, saving model to models/neck_l2.best0507_InceptionResNetV2.h5
Epoch 5/32
229/229 [==============================] - 192s 840ms/step - loss: 1.5783 - acc: 0.6995 - val_loss: 1.7942 - val_acc: 0.6164

Epoch 00005: val_loss improved from 1.88170 to 1.79415, saving model to models/neck_l2.best0507_InceptionResNetV2.h5
Epoch 6/32
229/229 [==============================] - 193s 842ms/step - loss: 1.4349 - acc: 0.7441 - val_loss: 1.7521 - val_acc: 0.6373

Epoch 00006: val_loss improved from 1.79415 to 1.75214, saving model to models/neck_l2.best0507_InceptionResNetV2.h5
Epoch 7/32
229/229 [==============================] - 191s 835ms/step - loss: 1.2855 - acc: 0.7933 - val_loss: 1.8523 - val_acc: 0.6299

Epoch 00007: val_loss did not improve from 1.75214
Epoch 8/32
229/229 [==============================] - 191s 835ms/step - loss: 1.1696 - acc: 0.8273 - val_loss: 1.8729 - val_acc: 0.6164

Epoch 00008: val_loss did not improve from 1.75214
Epoch 9/32
229/229 [==============================] - 192s 837ms/step - loss: 1.0579 - acc: 0.8620 - val_loss: 1.9723 - val_acc: 0.6262

Epoch 00009: val_loss did not improve from 1.75214
Epoch 10/32
 26/229 [==>...........................] - ETA: 2:38 - loss: 1.0069 - acc: 0.8822KeyboardInterrupt
```

2018.5.5

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ：
InceptionResNetV2  
- 模型文件 ：neck_memory_batch.best0505_InceptionResNetV2.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

# Compile the model
adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 64
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)



train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=batch_size,
        class_mode='categorical')


        prefix_cls = cur_class.split('_')[0]

        checkpointer = ModelCheckpoint(filepath='models/{0}_memory_batch.best0505_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                                       save_best_only=True,save_weights_only=True, mode='val_acc')

        #model.load_weights(load_filepath)

try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果lr=0.001 ,neck_memory_batch.best0505_InceptionResNetV2.h5

```{.python .input}
Epoch 1/64
229/229 [==============================] - 245s 1s/step - loss: 1.2888 - acc: 0.4576 - val_loss: 2.9201 - val_acc: 0.2267

Epoch 00001: val_loss improved from inf to 2.92010, saving model to models/neck_memory_batch.best0505_InceptionResNetV2.h5
Epoch 2/64
229/229 [==============================] - 194s 848ms/step - loss: 1.0201 - acc: 0.5927 - val_loss: 1.5094 - val_acc: 0.4706

Epoch 00002: val_loss improved from 2.92010 to 1.50943, saving model to models/neck_memory_batch.best0505_InceptionResNetV2.h5
Epoch 3/64
229/229 [==============================] - 194s 849ms/step - loss: 0.8979 - acc: 0.6489 - val_loss: 1.4805 - val_acc: 0.4718

Epoch 00003: val_loss improved from 1.50943 to 1.48049, saving model to models/neck_memory_batch.best0505_InceptionResNetV2.h5
Epoch 4/64
229/229 [==============================] - 194s 847ms/step - loss: 0.7955 - acc: 0.7009 - val_loss: 1.2534 - val_acc: 0.5343

Epoch 00004: val_loss improved from 1.48049 to 1.25339, saving model to models/neck_memory_batch.best0505_InceptionResNetV2.h5
Epoch 5/64
229/229 [==============================] - 194s 847ms/step - loss: 0.7387 - acc: 0.7210 - val_loss: 1.4577 - val_acc: 0.5147

Epoch 00005: val_loss did not improve from 1.25339
Epoch 6/64
229/229 [==============================] - 195s 850ms/step - loss: 0.6622 - acc: 0.7499 - val_loss: 1.4277 - val_acc: 0.4963

Epoch 00006: val_loss did not improve from 1.25339
Epoch 7/64
229/229 [==============================] - 194s 845ms/step - loss: 0.6221 - acc: 0.7705 - val_loss: 1.3144 - val_acc: 0.5294

Epoch 00007: val_loss did not improve from 1.25339
Epoch 8/64
229/229 [==============================] - 194s 845ms/step - loss: 0.5784 - acc: 0.7919 - val_loss: 1.3621 - val_acc: 0.5784

Epoch 00008: val_loss did not improve from 1.25339
Epoch 9/64
229/229 [==============================] - 194s 846ms/step - loss: 0.5394 - acc: 0.8041 - val_loss: 1.3106 - val_acc: 0.5049

Epoch 00009: val_loss did not improve from 1.25339
Epoch 10/64
229/229 [==============================] - 194s 846ms/step - loss: 0.5010 - acc: 0.8164 - val_loss: 1.4893 - val_acc: 0.5123

Epoch 00010: val_loss did not improve from 1.25339
Epoch 11/64
229/229 [==============================] - 194s 845ms/step - loss: 0.4800 - acc: 0.8291 - val_loss: 1.3342 - val_acc: 0.5735

Epoch 00011: val_loss did not improve from 1.25339
Epoch 12/64
229/229 [==============================] - 193s 845ms/step - loss: 0.4349 - acc: 0.8391 - val_loss: 1.5606 - val_acc: 0.5319

Epoch 00012: val_loss did not improve from 1.25339
Epoch 13/64
229/229 [==============================] - 193s 844ms/step - loss: 0.4075 - acc: 0.8497 - val_loss: 1.4829 - val_acc: 0.5576

Epoch 00013: val_loss did not improve from 1.25339
Epoch 14/64
229/229 [==============================] - 192s 837ms/step - loss: 0.3691 - acc: 0.8676 - val_loss: 1.3071 - val_acc: 0.5980

Epoch 00014: val_loss did not improve from 1.25339
Epoch 15/64
229/229 [==============================] - 192s 837ms/step - loss: 0.3532 - acc: 0.8763 - val_loss: 1.5118 - val_acc: 0.5650

Epoch 00015: val_loss did not improve from 1.25339
Epoch 16/64
229/229 [==============================] - 191s 834ms/step - loss: 0.3258 - acc: 0.8831 - val_loss: 1.7224 - val_acc: 0.5625

Epoch 00016: val_loss did not improve from 1.25339
Epoch 17/64
229/229 [==============================] - 191s 833ms/step - loss: 0.3108 - acc: 0.8874 - val_loss: 1.3586 - val_acc: 0.5797

Epoch 00017: val_loss did not improve from 1.25339
Epoch 18/64
229/229 [==============================] - 191s 834ms/step - loss: 0.2830 - acc: 0.8974 - val_loss: 1.5790 - val_acc: 0.5723

Epoch 00018: val_loss did not improve from 1.25339
Epoch 19/64
 16/229 [=>............................] - ETA: 2:34 - loss: 0.3204 - acc: 0.8828KeyboardInterrupt

```

训练结果lr=0.0001 ,neck2_memory_batch.best0505_InceptionResNetV2.h5

```{.python .input}


```

训练结果lr=0.00001 ,neck3_memory_batch.best0505_InceptionResNetV2.h5

```{.python .input}
Epoch 1/64
229/229 [==============================] - 245s 1s/step - loss: 0.0353 - acc: 0.9880 - val_loss: 2.0160 - val_acc: 0.6311

Epoch 00001: val_loss improved from inf to 2.01602, saving model to models/neck3_memory_batch.best0505_InceptionResNetV2.h5
Epoch 2/64
229/229 [==============================] - 192s 838ms/step - loss: 0.0384 - acc: 0.9884 - val_loss: 2.0066 - val_acc: 0.6336

Epoch 00002: val_loss improved from 2.01602 to 2.00663, saving model to models/neck3_memory_batch.best0505_InceptionResNetV2.h5
Epoch 3/64
229/229 [==============================] - 192s 838ms/step - loss: 0.0257 - acc: 0.9915 - val_loss: 2.0656 - val_acc: 0.6324

Epoch 00003: val_loss did not improve from 2.00663
Epoch 4/64
229/229 [==============================] - 192s 840ms/step - loss: 0.0384 - acc: 0.9873 - val_loss: 2.0533 - val_acc: 0.6373

Epoch 00004: val_loss did not improve from 2.00663
Epoch 5/64
229/229 [==============================] - 192s 839ms/step - loss: 0.0329 - acc: 0.9904 - val_loss: 2.0754 - val_acc: 0.6348

Epoch 00005: val_loss did not improve from 2.00663
Epoch 6/64
136/229 [================>.............] - ETA: 1:14 - loss: 0.0267 - acc: 0.9908KeyboardInterrupt

```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： InceptionResNetV2  
-
模型文件 ：neck.best0505_InceptionResNetV2.h5  
模型

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

prefix_cls = cur_class.split('_')[0]
prefix_cls

# Compile the model
adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 64
batch_size = 256

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)

train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')

        prefix_cls = cur_class.split('_')[0]

        checkpointer = ModelCheckpoint(filepath='models/{0}.best0505_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                                       save_best_only=True,save_weights_only=True, mode='val_acc')

try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

训练结果 lr=0.001 neck.best0505_InceptionResNetV2.h5
batch_size=256

```{.python .input}
Epoch 1/32
28/28 [==============================] - 86s 3s/step - loss: 1.5631 - acc: 0.3181 - val_loss: 4.0573 - val_acc: 0.2819

Epoch 00001: val_loss improved from inf to 4.05730, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 2/32
28/28 [==============================] - 28s 989ms/step - loss: 1.3353 - acc: 0.4408 - val_loss: 4.7377 - val_acc: 0.1949

Epoch 00002: val_loss did not improve from 4.05730
Epoch 3/32
28/28 [==============================] - 28s 987ms/step - loss: 1.2478 - acc: 0.4866 - val_loss: 3.6088 - val_acc: 0.2157

Epoch 00003: val_loss improved from 4.05730 to 3.60883, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 4/32
28/28 [==============================] - 28s 991ms/step - loss: 1.2040 - acc: 0.4955 - val_loss: 1.4112 - val_acc: 0.4216

Epoch 00004: val_loss improved from 3.60883 to 1.41117, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 5/32
28/28 [==============================] - 28s 1s/step - loss: 1.2127 - acc: 0.5059 - val_loss: 2.4130 - val_acc: 0.2843

Epoch 00005: val_loss did not improve from 1.41117
Epoch 6/32
28/28 [==============================] - 28s 990ms/step - loss: 1.1238 - acc: 0.5212 - val_loss: 1.8092 - val_acc: 0.4007

Epoch 00006: val_loss did not improve from 1.41117
Epoch 7/32
28/28 [==============================] - 28s 990ms/step - loss: 1.1159 - acc: 0.5234 - val_loss: 2.1529 - val_acc: 0.4252

Epoch 00007: val_loss did not improve from 1.41117
Epoch 8/32
28/28 [==============================] - 28s 990ms/step - loss: 1.0807 - acc: 0.5625 - val_loss: 3.7158 - val_acc: 0.3076

Epoch 00008: val_loss did not improve from 1.41117
Epoch 9/32
28/28 [==============================] - 29s 1s/step - loss: 1.0158 - acc: 0.6004 - val_loss: 2.0948 - val_acc: 0.3542

Epoch 00009: val_loss did not improve from 1.41117
Epoch 10/32
28/28 [==============================] - 28s 989ms/step - loss: 1.0516 - acc: 0.5759 - val_loss: 3.3949 - val_acc: 0.4093

Epoch 00010: val_loss did not improve from 1.41117
Epoch 11/32
28/28 [==============================] - 28s 989ms/step - loss: 0.9729 - acc: 0.6138 - val_loss: 2.6121 - val_acc: 0.2610

Epoch 00011: val_loss did not improve from 1.41117
Epoch 12/32
28/28 [==============================] - 27s 979ms/step - loss: 1.0354 - acc: 0.5677 - val_loss: 1.7541 - val_acc: 0.4473

Epoch 00012: val_loss did not improve from 1.41117
Epoch 13/32
28/28 [==============================] - 28s 991ms/step - loss: 0.9539 - acc: 0.6239 - val_loss: 1.6232 - val_acc: 0.4620

Epoch 00013: val_loss did not improve from 1.41117
Epoch 14/32
28/28 [==============================] - 28s 991ms/step - loss: 0.9726 - acc: 0.6161 - val_loss: 6.2901 - val_acc: 0.3064

Epoch 00014: val_loss did not improve from 1.41117

Epoch 1/64
28/28 [==============================] - 83s 3s/step - loss: 0.9812 - acc: 0.6306 - val_loss: 2.1615 - val_acc: 0.3100

Epoch 00001: val_loss improved from inf to 2.16146, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 2/64
28/28 [==============================] - 28s 990ms/step - loss: 1.0237 - acc: 0.5915 - val_loss: 2.6558 - val_acc: 0.2806

Epoch 00002: val_loss did not improve from 2.16146
Epoch 3/64
28/28 [==============================] - 28s 990ms/step - loss: 0.9917 - acc: 0.5949 - val_loss: 2.1812 - val_acc: 0.3762

Epoch 00003: val_loss did not improve from 2.16146
Epoch 4/64
28/28 [==============================] - 27s 980ms/step - loss: 0.9998 - acc: 0.6148 - val_loss: 6.3671 - val_acc: 0.1434

Epoch 00004: val_loss did not improve from 2.16146
Epoch 5/64
28/28 [==============================] - 28s 992ms/step - loss: 0.9702 - acc: 0.6194 - val_loss: 1.5190 - val_acc: 0.4265

Epoch 00005: val_loss improved from 2.16146 to 1.51896, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 6/64
28/28 [==============================] - 28s 991ms/step - loss: 0.9286 - acc: 0.6228 - val_loss: 1.2539 - val_acc: 0.5196

Epoch 00006: val_loss improved from 1.51896 to 1.25389, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 7/64
28/28 [==============================] - 28s 992ms/step - loss: 0.9252 - acc: 0.6384 - val_loss: 1.4587 - val_acc: 0.4449

Epoch 00007: val_loss did not improve from 1.25389
Epoch 8/64
28/28 [==============================] - 28s 994ms/step - loss: 0.9660 - acc: 0.6317 - val_loss: 1.2913 - val_acc: 0.5110

Epoch 00008: val_loss did not improve from 1.25389
Epoch 9/64
28/28 [==============================] - 29s 1s/step - loss: 0.8714 - acc: 0.6853 - val_loss: 1.7940 - val_acc: 0.4265

Epoch 00009: val_loss did not improve from 1.25389
Epoch 10/64
28/28 [==============================] - 28s 990ms/step - loss: 0.8558 - acc: 0.6629 - val_loss: 1.8087 - val_acc: 0.3762

Epoch 00010: val_loss did not improve from 1.25389
Epoch 11/64
28/28 [==============================] - 28s 990ms/step - loss: 0.8632 - acc: 0.6618 - val_loss: 1.4729 - val_acc: 0.4865

Epoch 00011: val_loss did not improve from 1.25389
Epoch 12/64
28/28 [==============================] - 28s 993ms/step - loss: 0.8172 - acc: 0.6975 - val_loss: 1.2308 - val_acc: 0.5331

Epoch 00012: val_loss improved from 1.25389 to 1.23082, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 13/64
28/28 [==============================] - 28s 991ms/step - loss: 0.8785 - acc: 0.6518 - val_loss: 1.9737 - val_acc: 0.4216

Epoch 00013: val_loss did not improve from 1.23082
Epoch 14/64
28/28 [==============================] - 28s 991ms/step - loss: 0.8714 - acc: 0.6775 - val_loss: 1.4195 - val_acc: 0.4804

Epoch 00014: val_loss did not improve from 1.23082
Epoch 15/64
28/28 [==============================] - 28s 990ms/step - loss: 0.8325 - acc: 0.6853 - val_loss: 1.1507 - val_acc: 0.5478

Epoch 00015: val_loss improved from 1.23082 to 1.15066, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 16/64
28/28 [==============================] - 28s 992ms/step - loss: 0.8777 - acc: 0.6540 - val_loss: 1.0966 - val_acc: 0.5895

Epoch 00016: val_loss improved from 1.15066 to 1.09660, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 17/64
28/28 [==============================] - 28s 1s/step - loss: 0.8621 - acc: 0.6709 - val_loss: 1.6551 - val_acc: 0.3824

Epoch 00017: val_loss did not improve from 1.09660
Epoch 18/64
28/28 [==============================] - 28s 992ms/step - loss: 0.7545 - acc: 0.7132 - val_loss: 1.7206 - val_acc: 0.4363

Epoch 00018: val_loss did not improve from 1.09660
Epoch 19/64
28/28 [==============================] - 28s 992ms/step - loss: 0.7400 - acc: 0.7031 - val_loss: 1.3268 - val_acc: 0.5380

Epoch 00019: val_loss did not improve from 1.09660
Epoch 20/64
28/28 [==============================] - 28s 992ms/step - loss: 0.7630 - acc: 0.7210 - val_loss: 1.4170 - val_acc: 0.4645

Epoch 00020: val_loss did not improve from 1.09660
Epoch 21/64
28/28 [==============================] - 28s 989ms/step - loss: 0.7922 - acc: 0.7009 - val_loss: 1.0271 - val_acc: 0.5956

Epoch 00021: val_loss improved from 1.09660 to 1.02711, saving model to models/neck.best0505_InceptionResNetV2.h5
Epoch 22/64
28/28 [==============================] - 28s 991ms/step - loss: 0.7699 - acc: 0.6998 - val_loss: 1.1651 - val_acc: 0.5760

Epoch 00022: val_loss did not improve from 1.02711
Epoch 23/64
28/28 [==============================] - 28s 991ms/step - loss: 0.7908 - acc: 0.7065 - val_loss: 1.6857 - val_acc: 0.4424

Epoch 00023: val_loss did not improve from 1.02711
Epoch 24/64
28/28 [==============================] - 28s 992ms/step - loss: 0.7341 - acc: 0.7188 - val_loss: 1.1349 - val_acc: 0.5490

Epoch 00024: val_loss did not improve from 1.02711
Epoch 25/64
28/28 [==============================] - 29s 1s/step - loss: 0.7208 - acc: 0.7366 - val_loss: 1.1474 - val_acc: 0.5429

Epoch 00025: val_loss did not improve from 1.02711
Epoch 26/64
28/28 [==============================] - 28s 990ms/step - loss: 0.6264 - acc: 0.7623 - val_loss: 1.2481 - val_acc: 0.5331

Epoch 00026: val_loss did not improve from 1.02711
Epoch 27/64
28/28 [==============================] - 28s 991ms/step - loss: 0.6663 - acc: 0.7478 - val_loss: 1.5418 - val_acc: 0.4412

Epoch 00027: val_loss did not improve from 1.02711
Epoch 28/64
28/28 [==============================] - 28s 993ms/step - loss: 0.7157 - acc: 0.7210 - val_loss: 1.1864 - val_acc: 0.5417

Epoch 00028: val_loss did not improve from 1.02711

Epoch 29/64
28/28 [==============================] - 28s 991ms/step - loss: 0.7432 - acc: 0.7065 - val_loss: 2.2950 - val_acc: 0.4461

Epoch 00029: val_loss did not improve from 1.02711
Epoch 30/64
28/28 [==============================] - 28s 991ms/step - loss: 0.7059 - acc: 0.7366 - val_loss: 1.8249 - val_acc: 0.4681

Epoch 00030: val_loss did not improve from 1.02711
Epoch 31/64
28/28 [==============================] - 27s 980ms/step - loss: 0.7003 - acc: 0.7247 - val_loss: 1.1743 - val_acc: 0.5711

Epoch 00031: val_loss did not improve from 1.02711

```

训练结果 lr=0.0001 neck1.best0505_InceptionResNetV2.h5
batch_size=256

```{.python .input}
Epoch 1/64
28/28 [==============================] - 90s 3s/step - loss: 0.5727 - acc: 0.7857 - val_loss: 1.1291 - val_acc: 0.5797

Epoch 00001: val_loss improved from inf to 1.12914, saving model to models/neck1.best0505_InceptionResNetV2.h5
Epoch 2/64
28/28 [==============================] - 28s 993ms/step - loss: 0.5143 - acc: 0.8058 - val_loss: 1.1833 - val_acc: 0.5613

Epoch 00002: val_loss did not improve from 1.12914
Epoch 3/64
28/28 [==============================] - 28s 994ms/step - loss: 0.5131 - acc: 0.8103 - val_loss: 1.1906 - val_acc: 0.5625

Epoch 00003: val_loss did not improve from 1.12914
Epoch 4/64
28/28 [==============================] - 28s 984ms/step - loss: 0.5103 - acc: 0.8033 - val_loss: 1.2070 - val_acc: 0.5637

Epoch 00004: val_loss did not improve from 1.12914
Epoch 5/64
28/28 [==============================] - 28s 995ms/step - loss: 0.5153 - acc: 0.8136 - val_loss: 1.0967 - val_acc: 0.5980

Epoch 00005: val_loss improved from 1.12914 to 1.09668, saving model to models/neck1.best0505_InceptionResNetV2.h5
Epoch 6/64
28/28 [==============================] - 28s 993ms/step - loss: 0.4587 - acc: 0.8214 - val_loss: 1.1808 - val_acc: 0.5870

Epoch 00006: val_loss did not improve from 1.09668
Epoch 7/64
28/28 [==============================] - 28s 993ms/step - loss: 0.4611 - acc: 0.8270 - val_loss: 1.1286 - val_acc: 0.5895

Epoch 00007: val_loss did not improve from 1.09668
Epoch 8/64
28/28 [==============================] - 28s 994ms/step - loss: 0.5218 - acc: 0.7991 - val_loss: 1.1473 - val_acc: 0.5809

Epoch 00008: val_loss did not improve from 1.09668
Epoch 9/64
28/28 [==============================] - 29s 1s/step - loss: 0.4464 - acc: 0.8447 - val_loss: 1.2653 - val_acc: 0.5686

Epoch 00009: val_loss did not improve from 1.09668
Epoch 10/64
28/28 [==============================] - 28s 993ms/step - loss: 0.4360 - acc: 0.8359 - val_loss: 1.1471 - val_acc: 0.5895

Epoch 00010: val_loss did not improve from 1.09668
Epoch 11/64
28/28 [==============================] - 28s 994ms/step - loss: 0.4269 - acc: 0.8382 - val_loss: 1.1641 - val_acc: 0.5784

Epoch 00011: val_loss did not improve from 1.09668
Epoch 12/64
28/28 [==============================] - 28s 994ms/step - loss: 0.4389 - acc: 0.8359 - val_loss: 1.2119 - val_acc: 0.5760

Epoch 00012: val_loss did not improve from 1.09668
Epoch 13/64
28/28 [==============================] - 28s 995ms/step - loss: 0.4234 - acc: 0.8527 - val_loss: 1.2795 - val_acc: 0.5735

Epoch 00013: val_loss did not improve from 1.09668
Epoch 14/64
28/28 [==============================] - 28s 992ms/step - loss: 0.4335 - acc: 0.8415 - val_loss: 1.3504 - val_acc: 0.5735

Epoch 00014: val_loss did not improve from 1.09668
Epoch 15/64
28/28 [==============================] - 28s 992ms/step - loss: 0.3828 - acc: 0.8717 - val_loss: 1.2493 - val_acc: 0.5919

Epoch 00015: val_loss did not improve from 1.09668
Epoch 16/64
28/28 [==============================] - 28s 995ms/step - loss: 0.4487 - acc: 0.8460 - val_loss: 1.2054 - val_acc: 0.5833

Epoch 00016: val_loss did not improve from 1.09668
Epoch 17/64
28/28 [==============================] - 29s 1s/step - loss: 0.4488 - acc: 0.8359 - val_loss: 1.2630 - val_acc: 0.5797

Epoch 00017: val_loss did not improve from 1.09668
Epoch 18/64
28/28 [==============================] - 28s 994ms/step - loss: 0.3994 - acc: 0.8504 - val_loss: 1.2244 - val_acc: 0.5833

Epoch 00018: val_loss did not improve from 1.09668
Epoch 19/64
28/28 [==============================] - 28s 996ms/step - loss: 0.3956 - acc: 0.8583 - val_loss: 1.2065 - val_acc: 0.6042

Epoch 00019: val_loss did not improve from 1.09668
Epoch 20/64
28/28 [==============================] - 28s 994ms/step - loss: 0.4163 - acc: 0.8527 - val_loss: 1.2080 - val_acc: 0.6238

Epoch 00020: val_loss did not improve from 1.09668
Epoch 21/64
28/28 [==============================] - 28s 993ms/step - loss: 0.4059 - acc: 0.8371 - val_loss: 1.1744 - val_acc: 0.6262

Epoch 00021: val_loss did not improve from 1.09668
Epoch 22/64
28/28 [==============================] - 28s 983ms/step - loss: 0.3639 - acc: 0.8703 - val_loss: 1.3365 - val_acc: 0.5870

Epoch 00022: val_loss did not improve from 1.09668
Epoch 23/64
28/28 [==============================] - 28s 993ms/step - loss: 0.3729 - acc: 0.8650 - val_loss: 1.3404 - val_acc: 0.5907

Epoch 00023: val_loss did not improve from 1.09668
Epoch 24/64
28/28 [==============================] - 28s 993ms/step - loss: 0.3952 - acc: 0.8638 - val_loss: 1.4063 - val_acc: 0.5760

Epoch 00024: val_loss did not improve from 1.09668
Epoch 25/64
28/28 [==============================] - 29s 1s/step - loss: 0.3475 - acc: 0.8772 - val_loss: 1.2557 - val_acc: 0.5931

Epoch 00025: val_loss did not improve from 1.09668
Epoch 26/64
28/28 [==============================] - 28s 993ms/step - loss: 0.3341 - acc: 0.8828 - val_loss: 1.1607 - val_acc: 0.6189

Epoch 00026: val_loss did not improve from 1.09668
Epoch 27/64
28/28 [==============================] - 28s 991ms/step - loss: 0.3419 - acc: 0.8795 - val_loss: 1.2166 - val_acc: 0.6140

Epoch 00027: val_loss did not improve from 1.09668
Epoch 28/64
 9/28 [========>.....................] - ETA: 13s - loss: 0.3477 - acc: 0.8785KeyboardInterrupt


```

lr=0.0001 batch_size=64

```{.python .input}
Epoch 1/64
114/114 [==============================] - 157s 1s/step - loss: 0.3499 - acc: 0.8717 - val_loss: 1.5018 - val_acc: 0.5674

Epoch 00001: val_loss improved from inf to 1.50175, saving model to models/neck2.best0505_InceptionResNetV2.h5
Epoch 2/64
114/114 [==============================] - 93s 814ms/step - loss: 0.3466 - acc: 0.8736 - val_loss: 1.3169 - val_acc: 0.6042

Epoch 00002: val_loss improved from 1.50175 to 1.31694, saving model to models/neck2.best0505_InceptionResNetV2.h5
Epoch 3/64
114/114 [==============================] - 99s 872ms/step - loss: 0.3143 - acc: 0.8846 - val_loss: 1.4151 - val_acc: 0.5870

Epoch 00003: val_loss did not improve from 1.31694
Epoch 4/64
114/114 [==============================] - 94s 828ms/step - loss: 0.3050 - acc: 0.8879 - val_loss: 1.3177 - val_acc: 0.6103

Epoch 00004: val_loss did not improve from 1.31694
Epoch 5/64
114/114 [==============================] - 99s 873ms/step - loss: 0.2644 - acc: 0.9054 - val_loss: 1.5143 - val_acc: 0.5907

Epoch 00005: val_loss did not improve from 1.31694
Epoch 6/64
114/114 [==============================] - 94s 828ms/step - loss: 0.2837 - acc: 0.8936 - val_loss: 1.6578 - val_acc: 0.5870

Epoch 00006: val_loss did not improve from 1.31694
Epoch 7/64
 61/114 [===============>..............] - ETA: 42s - loss: 0.2648 - acc: 0.9037KeyboardInterrupt


```

lr=0.00001 batch_size=64

```{.python .input}


```

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- 环境：google GPU
- keyWords ： InceptionResNetV2
-
模型文件 ：coat1.best0504_InceptionResNetV2.h5  coat2.best0504_InceptionResNetV2.h5
coat3.best0504_InceptionResNetV2.h5
模型

=>coat_0505_InceptionResNetV2.csv

```{.python .input}
#切分数据


mkdir_if_not_exist(['data-raw/train_2/train_valid', task])
mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'train'])
#mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'val'])
m = len(list(image_path[0][1]))
for mm in range(m):
    mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'train', str(mm)])
    #mkdir_if_not_exist(['data-raw/train_2/train_valid', task, 'val', str(mm)])

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.07, # randomly zoom image
        width_shift_range = 0.08, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.08, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.05,
        fill_mode = 'constant',
        cval = 0)
train_generator = datagen.flow_from_directory(
        'data-raw/train_2/train_valid/{0}/train'.format(task),
        target_size=(width, width),
        batch_size=32,
        class_mode='categorical')

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='models/{0}1.best0504_InceptionResNetV2.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True,save_weights_only=True, mode='val_acc')

model.load_weights('models/{0}.best0504_InceptionResNetV2.h5'.format(prefix_cls))
try:
    # Fit the model
    history = model.fit_generator(train_generator,
                                  epochs=epochs,
                                  validation_data = (X_valid, y_valid),
                                  #validation_data = (X_valid, y_valid),
                                  verbose=1,
                                  steps_per_epoch= int(len(image_path)) // batch_size,
                                  #validation_steps= int(len(image_path)*0.9) // batch_size,
                                  callbacks=[EarlyStopping(patience=10), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

训练结果 lr=0.001，coat1.best0504_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406 [==============================] - 388s 956ms/step - loss: 0.6253 - acc: 0.7487 - val_loss: 0.7689 - val_acc: 0.7109

Epoch 00001: val_loss improved from inf to 0.76891, saving model to models/coat1.best0504_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 338s 832ms/step - loss: 0.5676 - acc: 0.7679 - val_loss: 0.9964 - val_acc: 0.6473

Epoch 00002: val_loss did not improve from 0.76891
Epoch 3/32
406/406 [==============================] - 335s 825ms/step - loss: 0.5444 - acc: 0.7819 - val_loss: 2.9536 - val_acc: 0.4405

Epoch 00003: val_loss did not improve from 0.76891
Epoch 4/32
406/406 [==============================] - 333s 821ms/step - loss: 0.5119 - acc: 0.7910 - val_loss: 0.7581 - val_acc: 0.7303

Epoch 00004: val_loss improved from 0.76891 to 0.75811, saving model to models/coat1.best0504_InceptionResNetV2.h5
Epoch 5/32
406/406 [==============================] - 332s 819ms/step - loss: 0.4728 - acc: 0.8106 - val_loss: 0.7304 - val_acc: 0.7393

Epoch 00005: val_loss improved from 0.75811 to 0.73036, saving model to models/coat1.best0504_InceptionResNetV2.h5
Epoch 6/32
406/406 [==============================] - 333s 821ms/step - loss: 0.4389 - acc: 0.8211 - val_loss: 2.9675 - val_acc: 0.3250

Epoch 00006: val_loss did not improve from 0.73036
Epoch 7/32
406/406 [==============================] - 333s 820ms/step - loss: 0.4217 - acc: 0.8331 - val_loss: 0.8967 - val_acc: 0.7033

Epoch 00007: val_loss did not improve from 0.73036
Epoch 8/32
406/406 [==============================] - 332s 818ms/step - loss: 0.3905 - acc: 0.8448 - val_loss: 1.0080 - val_acc: 0.7040

Epoch 00008: val_loss did not improve from 0.73036
Epoch 9/32
406/406 [==============================] - 334s 822ms/step - loss: 0.3721 - acc: 0.8558 - val_loss: 0.8285 - val_acc: 0.7420

Epoch 00009: val_loss did not improve from 0.73036
Epoch 10/32
406/406 [==============================] - 332s 819ms/step - loss: 0.3321 - acc: 0.8688 - val_loss: 0.9783 - val_acc: 0.7289

Epoch 00010: val_loss did not improve from 0.73036
Epoch 11/32
406/406 [==============================] - 333s 820ms/step - loss: 0.3249 - acc: 0.8744 - val_loss: 0.9223 - val_acc: 0.7227

Epoch 00011: val_loss did not improve from 0.73036
Epoch 12/32
406/406 [==============================] - 339s 836ms/step - loss: 0.3133 - acc: 0.8781 - val_loss: 0.8479 - val_acc: 0.7414

Epoch 00012: val_loss did not improve from 0.73036
Epoch 13/32
406/406 [==============================] - 336s 828ms/step - loss: 0.2739 - acc: 0.8947 - val_loss: 0.8213 - val_acc: 0.7704

Epoch 00013: val_loss did not improve from 0.73036
Epoch 14/32
406/406 [==============================] - 333s 821ms/step - loss: 0.2727 - acc: 0.8955 - val_loss: 0.9159 - val_acc: 0.7427

Epoch 00014: val_loss did not improve from 0.73036
Epoch 15/32
406/406 [==============================] - 333s 821ms/step - loss: 0.2680 - acc: 0.8962 - val_loss: 0.9350 - val_acc: 0.7331

Epoch 00015: val_loss did not improve from 0.73036

```

训练结果 lr=0.0001，coat2.best0504_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406 [==============================] - 391s 962ms/step - loss: 0.2905 - acc: 0.8852 - val_loss: 0.5859 - val_acc: 0.8105

Epoch 00001: val_loss improved from inf to 0.58586, saving model to models/coat2.best0504_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 335s 825ms/step - loss: 0.2162 - acc: 0.9136 - val_loss: 0.6048 - val_acc: 0.8195

Epoch 00002: val_loss did not improve from 0.58586
Epoch 3/32
406/406 [==============================] - 332s 818ms/step - loss: 0.1833 - acc: 0.9270 - val_loss: 0.6256 - val_acc: 0.8216

Epoch 00003: val_loss did not improve from 0.58586
Epoch 4/32
406/406 [==============================] - 331s 815ms/step - loss: 0.1728 - acc: 0.9340 - val_loss: 0.5998 - val_acc: 0.8257

Epoch 00004: val_loss did not improve from 0.58586
Epoch 5/32
406/406 [==============================] - 331s 814ms/step - loss: 0.1494 - acc: 0.9443 - val_loss: 0.6181 - val_acc: 0.8326

Epoch 00005: val_loss did not improve from 0.58586
Epoch 6/32
406/406 [==============================] - 331s 816ms/step - loss: 0.1310 - acc: 0.9481 - val_loss: 0.6837 - val_acc: 0.8237

Epoch 00006: val_loss did not improve from 0.58586
Epoch 7/32
406/406 [==============================] - 331s 815ms/step - loss: 0.1209 - acc: 0.9544 - val_loss: 0.7365 - val_acc: 0.8154

Epoch 00007: val_loss did not improve from 0.58586
Epoch 8/32
406/406 [==============================] - 331s 814ms/step - loss: 0.1163 - acc: 0.9561 - val_loss: 0.6828 - val_acc: 0.8278

Epoch 00008: val_loss did not improve from 0.58586
Epoch 9/32
406/406 [==============================] - 331s 816ms/step - loss: 0.1037 - acc: 0.9604 - val_loss: 0.6942 - val_acc: 0.8347

Epoch 00009: val_loss did not improve from 0.58586
```

训练结果 lr=0.00001，coat3.best0504_InceptionResNetV2.h5

```{.python .input}
Epoch 1/32
406/406 [==============================] - 414s 1s/step - loss: 0.0816 - acc: 0.9703 - val_loss: 0.7100 - val_acc: 0.8402

Epoch 00001: val_loss improved from inf to 0.71002, saving model to models/coat3.best0504_InceptionResNetV2.h5
Epoch 2/32
406/406 [==============================] - 336s 828ms/step - loss: 0.0849 - acc: 0.9682 - val_loss: 0.7110 - val_acc: 0.8416

Epoch 00002: val_loss did not improve from 0.71002
Epoch 3/32
406/406 [==============================] - 332s 818ms/step - loss: 0.0821 - acc: 0.9684 - val_loss: 0.7155 - val_acc: 0.8396

Epoch 00003: val_loss did not improve from 0.71002
Epoch 4/32
406/406 [==============================] - 333s 819ms/step - loss: 0.0802 - acc: 0.9714 - val_loss: 0.7047 - val_acc: 0.8423

Epoch 00004: val_loss improved from 0.71002 to 0.70473, saving model to models/coat3.best0504_InceptionResNetV2.h5
Epoch 5/32
 25/406 [>.............................] - ETA: 4:36 - loss: 0.0784 - acc: 0.9700KeyboardInterrupt

```

- 训练类型：coat
- 前提条件：
- 训练参数和模型
- keyWords ： Xception 无增强
- 模型文件
：coat_Xception_raw_weights.h5

```{.python .input}
prefix_cls = cur_class.split('_')[0]

save_path='{0}_Xception_raw_weights.h5'.format(prefix_cls)
load_path='{0}_Xception_raw_weights.h5'.format(prefix_cls)

cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape, X_valid.shape

epochs = 32
batch_size = 32

# Compile the model
adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer

%%time


checkpointer = ModelCheckpoint(filepath=save_path, verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights(load_filepath)

try:

    h = model.fit(X_train, y_train, batch_size=32, epochs=epochs,
              callbacks=[EarlyStopping(patience=7), checkpointer],
              shuffle=True, verbose=1,
              validation_data=(X_valid, y_valid))
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

训练结果 lr=0.001

```{.python .input}


```

训练结果 lr=0.0001

```{.python .input}
Train on 13008 samples, validate on 1446 samples
Epoch 1/32
13008/13008 [==============================] - 329s 25ms/step - loss: 0.0986 - acc: 0.9667 - val_loss: 1.5134 - val_acc: 0.6307

Epoch 00001: val_loss improved from inf to 1.51340, saving model to coat_Xception_raw_weights.h5
Epoch 2/32
13008/13008 [==============================] - 321s 25ms/step - loss: 0.0382 - acc: 0.9877 - val_loss: 1.6724 - val_acc: 0.6238

Epoch 00002: val_loss did not improve
Epoch 3/32
13008/13008 [==============================] - 321s 25ms/step - loss: 0.0257 - acc: 0.9932 - val_loss: 1.7912 - val_acc: 0.6286

Epoch 00003: val_loss did not improve
Epoch 4/32
13008/13008 [==============================] - 321s 25ms/step - loss: 0.0206 - acc: 0.9938 - val_loss: 1.8979 - val_acc: 0.6231

Epoch 00004: val_loss did not improve
Epoch 5/32
 4032/13008 [========>.....................] - ETA: 3:35 - loss: 0.0162 - acc: 0.9950KeyboardInterrupt
CPU times: user 18min 15s, sys: 5min 55s, total: 24min 10s
Wall time: 23min 15s

```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- keyWords ： Xception
- 模型文件
：neck_r2_Xception_new_weights.h5  neck_r2_Xception_new2_weights.h5
=>neck_0504_Xception.csv

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')
inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

epochs = 32
batch_size = 32

adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

prefix_cls = cur_class.split('_')[0]

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)



datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]
save_filepath='{0}_r2_Xception_new2_weights.h5'.format(prefix_cls)
load_filepath='{0}_r2_Xception_new_weights.h5'.format(prefix_cls)

%%time


checkpointer = ModelCheckpoint(filepath=save_filepath, verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights(load_filepath)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=7), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

训练结果 lr=0.001，neck_r2_Xception_new_weights.h5

```{.python .input}
Epoch 1/32
 - 254s - loss: 1.0084 - acc: 0.6016 - val_loss: 1.6152 - val_acc: 0.4044

Epoch 00001: val_loss improved from inf to 1.61517, saving model to neck_r2_Xception_new_weights.h5
Epoch 2/32
 - 248s - loss: 0.8608 - acc: 0.6673 - val_loss: 1.2976 - val_acc: 0.5895

Epoch 00002: val_loss improved from 1.61517 to 1.29757, saving model to neck_r2_Xception_new_weights.h5
Epoch 3/32
 - 248s - loss: 0.7776 - acc: 0.7004 - val_loss: 0.9000 - val_acc: 0.7230

Epoch 00003: val_loss improved from 1.29757 to 0.89996, saving model to neck_r2_Xception_new_weights.h5
Epoch 4/32
 - 247s - loss: 0.7105 - acc: 0.7278 - val_loss: 0.7105 - val_acc: 0.7365

Epoch 00004: val_loss improved from 0.89996 to 0.71048, saving model to neck_r2_Xception_new_weights.h5
Epoch 5/32
 - 247s - loss: 0.6684 - acc: 0.7490 - val_loss: 0.9283 - val_acc: 0.6801

Epoch 00005: val_loss did not improve
Epoch 6/32
 - 248s - loss: 0.6166 - acc: 0.7659 - val_loss: 0.7324 - val_acc: 0.7402

Epoch 00006: val_loss did not improve
Epoch 7/32
 - 248s - loss: 0.5971 - acc: 0.7707 - val_loss: 0.6401 - val_acc: 0.7708

Epoch 00007: val_loss improved from 0.71048 to 0.64015, saving model to neck_r2_Xception_new_weights.h5
Epoch 8/32
 - 248s - loss: 0.5613 - acc: 0.7885 - val_loss: 0.7690 - val_acc: 0.7218

Epoch 00008: val_loss did not improve
Epoch 9/32
 - 248s - loss: 0.5300 - acc: 0.8001 - val_loss: 0.9126 - val_acc: 0.7426

Epoch 00009: val_loss did not improve
Epoch 10/32
 - 247s - loss: 0.5149 - acc: 0.8100 - val_loss: 0.7542 - val_acc: 0.7414

Epoch 00010: val_loss did not improve
Epoch 11/32
 - 248s - loss: 0.4796 - acc: 0.8244 - val_loss: 0.7527 - val_acc: 0.7574

Epoch 00011: val_loss did not improve
Epoch 12/32
 - 248s - loss: 0.4463 - acc: 0.8306 - val_loss: 0.8557 - val_acc: 0.7194

Epoch 00012: val_loss did not improve
Epoch 13/32
 - 248s - loss: 0.4345 - acc: 0.8381 - val_loss: 1.0342 - val_acc: 0.7377

Epoch 00013: val_loss did not improve
Epoch 14/32
KeyboardInterrupt
CPU times: user 59min 57s, sys: 10min 11s, total: 1h 10min 8s
```

训练结果 lr=0.0001，neck_r2_Xception_new2_weights.h5
neck_r2_Xception_new3_weights.h5

```{.python .input}
Epoch 1/32
 - 262s - loss: 0.3766 - acc: 0.8649 - val_loss: 0.5999 - val_acc: 0.7953

Epoch 00001: val_loss improved from inf to 0.59987, saving model to neck_r2_Xception_new2_weights.h5
Epoch 2/32
 - 242s - loss: 0.3361 - acc: 0.8766 - val_loss: 0.5907 - val_acc: 0.7990

Epoch 00002: val_loss improved from 0.59987 to 0.59070, saving model to neck_r2_Xception_new2_weights.h5
Epoch 3/32
 - 242s - loss: 0.3188 - acc: 0.8801 - val_loss: 0.6191 - val_acc: 0.8002

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 242s - loss: 0.3082 - acc: 0.8895 - val_loss: 0.6401 - val_acc: 0.7978

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 242s - loss: 0.2819 - acc: 0.8934 - val_loss: 0.6429 - val_acc: 0.8027

Epoch 00005: val_loss did not improve
Epoch 6/32
 - 242s - loss: 0.2514 - acc: 0.9068 - val_loss: 0.6937 - val_acc: 0.7917

Epoch 00006: val_loss did not improve
Epoch 7/32

Epoch 1/32
229/229 [==============================] - 243s 1s/step - loss: 0.3143 - acc: 0.8856 - val_loss: 0.6312 - val_acc: 0.7880

Epoch 00001: val_loss improved from inf to 0.63117, saving model to neck_r2_Xception_new2_weights.h5
Epoch 2/32
229/229 [==============================] - 242s 1s/step - loss: 0.3062 - acc: 0.8873 - val_loss: 0.6555 - val_acc: 0.7831

Epoch 00002: val_loss did not improve
Epoch 3/32
229/229 [==============================] - 242s 1s/step - loss: 0.2777 - acc: 0.8968 - val_loss: 0.6303 - val_acc: 0.7978

Epoch 00003: val_loss improved from 0.63117 to 0.63027, saving model to neck_r2_Xception_new2_weights.h5
Epoch 4/32
229/229 [==============================] - 242s 1s/step - loss: 0.2725 - acc: 0.9010 - val_loss: 0.6704 - val_acc: 0.7831

Epoch 00004: val_loss did not improve
Epoch 5/32
229/229 [==============================] - 242s 1s/step - loss: 0.2635 - acc: 0.9057 - val_loss: 0.6671 - val_acc: 0.8002

Epoch 00005: val_loss did not improve
Epoch 6/32
229/229 [==============================] - 242s 1s/step - loss: 0.2336 - acc: 0.9156 - val_loss: 0.7169 - val_acc: 0.7868

Epoch 00006: val_loss did not improve
Epoch 7/32
 52/229 [=====>........................] - ETA: 3:01 - loss: 0.2110 - acc: 0.9261

```

2018.5.4

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- keyWords ： Xception
- 模型文件
：neck_r2_Xception_weights.h5 neck_r2_Xception2_weights.h5

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)
X_train.shape, y_train.shape

epochs = 32
batch_size = 32

# Compile the model
adam = Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])


datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)



datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]
save_filepath='{0}_r2_Xception2_weights.h5'.format(prefix_cls)
load_filepath='{0}_r2_Xception_weights.h5'.format(prefix_cls)

%%time


checkpointer = ModelCheckpoint(filepath=save_filepath, verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights(load_filepath)

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X, y, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=7), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果
lr=0.001  neck_r2_Xception_weights.h5

```{.python .input}
Epoch 1/32
 - 253s - loss: 1.2467 - acc: 0.4773 - val_loss: 1.3787 - val_acc: 0.5490

Epoch 00001: val_loss improved from inf to 1.37867, saving model to neck_r2_Xception_weights.h5
Epoch 2/32
 - 263s - loss: 0.9529 - acc: 0.6281 - val_loss: 0.9729 - val_acc: 0.6385

Epoch 00002: val_loss improved from 1.37867 to 0.97294, saving model to neck_r2_Xception_weights.h5
Epoch 3/32
 - 263s - loss: 0.8437 - acc: 0.6789 - val_loss: 0.7016 - val_acc: 0.7377

Epoch 00003: val_loss improved from 0.97294 to 0.70156, saving model to neck_r2_Xception_weights.h5
Epoch 4/32
 - 263s - loss: 0.7741 - acc: 0.7042 - val_loss: 0.8907 - val_acc: 0.7047

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 263s - loss: 0.7142 - acc: 0.7299 - val_loss: 0.6447 - val_acc: 0.7488

Epoch 00005: val_loss improved from 0.70156 to 0.64475, saving model to neck_r2_Xception_weights.h5
Epoch 6/32
 - 263s - loss: 0.6780 - acc: 0.7429 - val_loss: 0.5672 - val_acc: 0.8027

Epoch 00006: val_loss improved from 0.64475 to 0.56718, saving model to neck_r2_Xception_weights.h5
Epoch 7/32
 - 263s - loss: 0.6307 - acc: 0.7601 - val_loss: 0.5279 - val_acc: 0.8051

Epoch 00007: val_loss improved from 0.56718 to 0.52786, saving model to neck_r2_Xception_weights.h5
Epoch 8/32
 - 263s - loss: 0.6010 - acc: 0.7743 - val_loss: 0.4263 - val_acc: 0.8346

Epoch 00008: val_loss improved from 0.52786 to 0.42635, saving model to neck_r2_Xception_weights.h5
Epoch 9/32
 - 263s - loss: 0.5794 - acc: 0.7855 - val_loss: 0.4636 - val_acc: 0.8407

Epoch 00009: val_loss did not improve
Epoch 10/32
 - 264s - loss: 0.5479 - acc: 0.7928 - val_loss: 0.4220 - val_acc: 0.8554

Epoch 00010: val_loss improved from 0.42635 to 0.42202, saving model to neck_r2_Xception_weights.h5
Epoch 11/32
 - 263s - loss: 0.4988 - acc: 0.8120 - val_loss: 0.4963 - val_acc: 0.8517

Epoch 00011: val_loss did not improve
Epoch 12/32
 - 262s - loss: 0.4985 - acc: 0.8140 - val_loss: 0.4577 - val_acc: 0.8358

Epoch 00012: val_loss did not improve
Epoch 13/32
KeyboardInterrupt
CPU times: user 56min 7s, sys: 9min 43s, total: 1h 5min 51s
Wall time: 53min 23s

```

r=0.0001  neck_r2_Xception2_weights.h5

```{.python .input}




```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- keyWords ： Xception
- 模型文件
：TrainModel/neck_new1_Xception_weights.h5
TrainModel/neck_new1_Xception_weights.h5

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=0.001)

model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])
# Set a learning rate annealer
# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',
#                                             patience=3,
#                                             verbose=1,
#                                             factor=0.1,
#                                             min_lr=0.00001)
epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train)

checkpointer = ModelCheckpoint(filepath='TrainModel/{0}_new1_Xception_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('{0}3_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')






```

训练结果
lr=0.001,TrainModel/neck_new1_Xception_weights.h5(不小心失误，已变成fine-tune的模型参数)

```{.python .input}
Epoch 1/32
 - 176s - loss: 1.1236 - acc: 0.5575 - val_loss: 1.3500 - val_acc: 0.6199

Epoch 00001: val_loss improved from inf to 1.34996, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 2/32
 - 169s - loss: 0.8231 - acc: 0.6929 - val_loss: 1.1761 - val_acc: 0.6915

Epoch 00002: val_loss improved from 1.34996 to 1.17610, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 3/32
 - 169s - loss: 0.6970 - acc: 0.7432 - val_loss: 0.7805 - val_acc: 0.7646

Epoch 00003: val_loss improved from 1.17610 to 0.78051, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 4/32
 - 169s - loss: 0.6211 - acc: 0.7656 - val_loss: 0.9463 - val_acc: 0.6564

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 169s - loss: 0.5524 - acc: 0.8045 - val_loss: 0.8459 - val_acc: 0.7558

Epoch 00005: val_loss did not improve
Epoch 6/32
 - 169s - loss: 0.5408 - acc: 0.8005 - val_loss: 0.6968 - val_acc: 0.7617

Epoch 00006: val_loss improved from 0.78051 to 0.69681, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 7/32
 - 169s - loss: 0.5069 - acc: 0.8117 - val_loss: 0.8152 - val_acc: 0.7588

Epoch 00007: val_loss did not improve
Epoch 8/32
 - 169s - loss: 0.4572 - acc: 0.8357 - val_loss: 0.7412 - val_acc: 0.7939

Epoch 00008: val_loss did not improve
Epoch 9/32
 - 169s - loss: 0.4154 - acc: 0.8490 - val_loss: 0.8275 - val_acc: 0.7544

Epoch 00009: val_loss did not improve
Epoch 10/32
 - 169s - loss: 0.3859 - acc: 0.8586 - val_loss: 0.6303 - val_acc: 0.7895

Epoch 00010: val_loss improved from 0.69681 to 0.63025, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 11/32
 - 169s - loss: 0.3739 - acc: 0.8614 - val_loss: 0.6765 - val_acc: 0.7968

Epoch 00011: val_loss did not improve
Epoch 12/32
 - 169s - loss: 0.3592 - acc: 0.8690 - val_loss: 0.6217 - val_acc: 0.8187

Epoch 00012: val_loss improved from 0.63025 to 0.62175, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 13/32
 - 169s - loss: 0.3282 - acc: 0.8820 - val_loss: 0.7543 - val_acc: 0.7734

Epoch 00013: val_loss did not improve
Epoch 14/32

```

lr=0.0001,TrainModel/neck_new2_Xception_weights.h5

```{.python .input}
Epoch 1/32
 - 180s - loss: 0.2410 - acc: 0.9111 - val_loss: 0.4795 - val_acc: 0.8465

Epoch 00001: val_loss improved from inf to 0.47954, saving model to TrainModel/neck_new2_Xception_weights.h5
Epoch 2/32
 - 175s - loss: 0.1795 - acc: 0.9363 - val_loss: 0.4952 - val_acc: 0.8626

Epoch 00002: val_loss did not improve
Epoch 3/32
 - 174s - loss: 0.1589 - acc: 0.9457 - val_loss: 0.4851 - val_acc: 0.8538

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 175s - loss: 0.1364 - acc: 0.9517 - val_loss: 0.5104 - val_acc: 0.8596

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 175s - loss: 0.1274 - acc: 0.9573 - val_loss: 0.5603 - val_acc: 0.8567

Epoch 00005: val_loss did not improve
Epoch 6/32
 - 175s - loss: 0.1105 - acc: 0.9581 - val_loss: 0.5637 - val_acc: 0.8655

Epoch 00006: val_loss did not improve
Epoch 7/32
 - 175s - loss: 0.0961 - acc: 0.9639 - val_loss: 0.6007 - val_acc: 0.8626

Epoch 00007: val_loss did not improve
Epoch 8/32
 - 175s - loss: 0.0971 - acc: 0.9667 - val_loss: 0.6131 - val_acc: 0.8582

Epoch 00008: val_loss did not improve
Epoch 9/32
 - 175s - loss: 0.0951 - acc: 0.9657 - val_loss: 0.6059 - val_acc: 0.8655

Epoch 00009: val_loss did not improve
Epoch 10/32
 - 175s - loss: 0.0739 - acc: 0.9722 - val_loss: 0.6494 - val_acc: 0.8670

Epoch 00010: val_loss did not improve

```

- fine-tune 该代码 lr=0.0001

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)
model.load_weights('TrainModel/neck_new2_Xception_weights.h5'.format(prefix_cls))

#冻结block14层之前
set_trainable = False
for layer in model.layers[2].layers:#model.layers[2]是xception
    if layer.name == 'block14_sepconv1':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False

```

训练结果 TrainModel/neck_new1_Xception_weights.h5

```{.python .input}
Epoch 1/32
 - 85s - loss: 0.1847 - acc: 0.9309 - val_loss: 0.5006 - val_acc: 0.8596

Epoch 00001: val_loss improved from inf to 0.50061, saving model to TrainModel/neck_new1_Xception_weights.h5
Epoch 2/32
 - 81s - loss: 0.1808 - acc: 0.9361 - val_loss: 0.5132 - val_acc: 0.8523

Epoch 00002: val_loss did not improve
Epoch 3/32
 - 80s - loss: 0.1824 - acc: 0.9313 - val_loss: 0.5222 - val_acc: 0.8538

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 84s - loss: 0.1818 - acc: 0.9329 - val_loss: 0.5184 - val_acc: 0.8523

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 81s - loss: 0.1769 - acc: 0.9357 - val_loss: 0.5215 - val_acc: 0.8567

Epoch 00005: val_loss did not improve
Epoch 6/32
 - 80s - loss: 0.1703 - acc: 0.9393 - val_loss: 0.5283 - val_acc: 0.8538

Epoch 00006: val_loss did not improve
Epoch 7/32
 - 81s - loss: 0.1722 - acc: 0.9359 - val_loss: 0.5306 - val_acc: 0.8655

Epoch 00007: val_loss did not improve
Epoch 8/32
 - 80s - loss: 0.1848 - acc: 0.9333 - val_loss: 0.5325 - val_acc: 0.8567

Epoch 00008: val_loss did not improve
Epoch 9/32
 - 80s - loss: 0.1767 - acc: 0.9383 - val_loss: 0.5364 - val_acc: 0.8553

Epoch 00009: val_loss did not improve
Epoch 10/32
 - 79s - loss: 0.1856 - acc: 0.9353 - val_loss: 0.5293 - val_acc: 0.8567

Epoch 00010: val_loss did not improve
Epoch 11/32
 - 82s - loss: 0.1664 - acc: 0.9373 - val_loss: 0.5222 - val_acc: 0.8567

Epoch 00011: val_loss did not improve
Epoch 12/32
 - 79s - loss: 0.1799 - acc: 0.9313 - val_loss: 0.5245 - val_acc: 0.8611

Epoch 00012: val_loss did not improve
Epoch 13/32
 - 78s - loss: 0.1764 - acc: 0.9371 - val_loss: 0.5360 - val_acc: 0.8582

Epoch 00013: val_loss did not improve
Epoch 14/32

```

- 训练类型：neck
- 前提条件：
- 训练参数和模型
- keyWords ： InceptionV3
- 模型文件
：TrainModel/neck_InceptionV3_weights.h5
TrainModel/neck2_InceptionV3_weights.h5
TrainModel/neck3_InceptionV3_weights.h5
TrainModel/neck4_InceptionV3_weights.h5
TrainModel/neck5_InceptionV3_weights.h5
=>neck_0420b.csv （
neck4_InceptionV3_weights.h5)

```{.python .input}
cnn_model = InceptionV3(include_top=False, input_shape=(width, width, 3),
                              weights='imagenet' #first time
                             #weights=None
                       )

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

adam = Adam(lr=0.0001)
#sgd=SGD(lr=0.0001, momentum=0.9, decay=0.0, nesterov=True)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.01,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='TrainModel/{0}2_InceptionV3_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('TrainModel/{0}_InceptionV3_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

lr=0.001

```{.python .input}
Epoch 1/24
 - 99s - loss: 1.3495 - acc: 0.4333 - val_loss: 1.6213 - val_acc: 0.3743

Epoch 00001: val_loss improved from inf to 1.62132, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 2/24
 - 81s - loss: 1.0487 - acc: 0.5986 - val_loss: 1.8485 - val_acc: 0.4576

Epoch 00002: val_loss did not improve
Epoch 3/24
 - 83s - loss: 0.9549 - acc: 0.6308 - val_loss: 0.9294 - val_acc: 0.7003

Epoch 00003: val_loss improved from 1.62132 to 0.92936, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 4/24
 - 83s - loss: 0.8671 - acc: 0.6723 - val_loss: 0.9269 - val_acc: 0.7178

Epoch 00004: val_loss improved from 0.92936 to 0.92690, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 5/24
 - 82s - loss: 0.8160 - acc: 0.6907 - val_loss: 1.8018 - val_acc: 0.6257

Epoch 00005: val_loss did not improve
Epoch 6/24
 - 82s - loss: 0.7841 - acc: 0.7095 - val_loss: 1.0184 - val_acc: 0.7105

Epoch 00006: val_loss did not improve
Epoch 7/24
 - 82s - loss: 0.7439 - acc: 0.7143 - val_loss: 1.2574 - val_acc: 0.5833

Epoch 00007: val_loss did not improve
Epoch 8/24
 - 82s - loss: 0.7151 - acc: 0.7346 - val_loss: 0.8137 - val_acc: 0.6930

Epoch 00008: val_loss improved from 0.92690 to 0.81370, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 9/24
 - 82s - loss: 0.6934 - acc: 0.7322 - val_loss: 1.1297 - val_acc: 0.6725

Epoch 00009: val_loss did not improve
Epoch 10/24
 - 82s - loss: 0.6504 - acc: 0.7530 - val_loss: 0.8517 - val_acc: 0.7237

Epoch 00010: val_loss did not improve
Epoch 11/24
 - 82s - loss: 0.6448 - acc: 0.7626 - val_loss: 0.9297 - val_acc: 0.6886

Epoch 00011: val_loss did not improve
Epoch 12/24
 - 82s - loss: 0.6042 - acc: 0.7804 - val_loss: 0.9315 - val_acc: 0.7398

Epoch 00012: val_loss did not improve
Epoch 13/24
 - 82s - loss: 0.6047 - acc: 0.7736 - val_loss: 0.7621 - val_acc: 0.7515

Epoch 00013: val_loss improved from 0.81370 to 0.76213, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 14/24
 - 81s - loss: 0.5703 - acc: 0.7909 - val_loss: 1.0146 - val_acc: 0.7208

Epoch 00014: val_loss did not improve
Epoch 15/24
 - 82s - loss: 0.5738 - acc: 0.7808 - val_loss: 0.8936 - val_acc: 0.7091

Epoch 00015: val_loss did not improve
Epoch 16/24
 - 82s - loss: 0.5367 - acc: 0.8025 - val_loss: 0.9879 - val_acc: 0.6842

Epoch 00016: val_loss did not improve
Epoch 17/24
 - 82s - loss: 0.5143 - acc: 0.8047 - val_loss: 0.7577 - val_acc: 0.7763

Epoch 00017: val_loss improved from 0.76213 to 0.75767, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 18/24
 - 82s - loss: 0.5141 - acc: 0.8069 - val_loss: 0.8203 - val_acc: 0.7632
Epoch 1/24
 - 95s - loss: 0.5510 - acc: 0.8027 - val_loss: 0.8083 - val_acc: 0.7398

Epoch 00001: val_loss improved from inf to 0.80826, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 2/24
 - 81s - loss: 0.5159 - acc: 0.8061 - val_loss: 0.7993 - val_acc: 0.7632

Epoch 00002: val_loss improved from 0.80826 to 0.79928, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 3/24
 - 83s - loss: 0.4878 - acc: 0.8177 - val_loss: 0.7027 - val_acc: 0.7705

Epoch 00003: val_loss improved from 0.79928 to 0.70272, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 4/24
 - 83s - loss: 0.4635 - acc: 0.8315 - val_loss: 0.7817 - val_acc: 0.7427

Epoch 00004: val_loss did not improve
Epoch 5/24
 - 83s - loss: 0.4692 - acc: 0.8269 - val_loss: 0.9805 - val_acc: 0.7310

Epoch 00005: val_loss did not improve
Epoch 6/24
 - 83s - loss: 0.4457 - acc: 0.8345 - val_loss: 0.9173 - val_acc: 0.7456

Epoch 00006: val_loss did not improve
Epoch 7/24
 - 82s - loss: 0.4530 - acc: 0.8313 - val_loss: 0.9833 - val_acc: 0.7266

Epoch 00007: val_loss did not improve
Epoch 8/24
 - 83s - loss: 0.4150 - acc: 0.8440 - val_loss: 0.7158 - val_acc: 0.7807

Epoch 00008: val_loss did not improve
Epoch 9/24
 - 82s - loss: 0.4327 - acc: 0.8415 - val_loss: 1.2350 - val_acc: 0.6740

Epoch 00009: val_loss did not improve
Epoch 10/24
 - 82s - loss: 0.3993 - acc: 0.8592 - val_loss: 0.7636 - val_acc: 0.7778

Epoch 00010: val_loss did not improve
Epoch 11/24
 - 93s - loss: 0.3846 - acc: 0.8598 - val_loss: 0.7601 - val_acc: 0.7807

Epoch 00011: val_loss did not improve
Epoch 12/24
 - 94s - loss: 0.3840 - acc: 0.8654 - val_loss: 0.7333 - val_acc: 0.7705

Epoch 00012: val_loss did not improve
Epoch 13/24
 - 94s - loss: 0.3774 - acc: 0.8592 - val_loss: 0.7847 - val_acc: 0.7865

Epoch 00013: val_loss did not improve
Epoch 14/24
 - 95s - loss: 0.3729 - acc: 0.8622 - val_loss: 0.6803 - val_acc: 0.8085

Epoch 00014: val_loss improved from 0.70272 to 0.68029, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 15/24
 - 97s - loss: 0.3575 - acc: 0.8680 - val_loss: 0.7423 - val_acc: 0.7982

Epoch 00015: val_loss did not improve
Epoch 16/24
 - 92s - loss: 0.3517 - acc: 0.8682 - val_loss: 0.6504 - val_acc: 0.8099

Epoch 00016: val_loss improved from 0.68029 to 0.65043, saving model to TrainModel/neck_InceptionV3_weights.h5
Epoch 17/24
 - 107s - loss: 0.3326 - acc: 0.8778 - val_loss: 0.7736 - val_acc: 0.7632

Epoch 00017: val_loss did not improve
Epoch 18/24
 - 107s - loss: 0.3517 - acc: 0.8748 - val_loss: 0.7154 - val_acc: 0.7909

Epoch 00018: val_loss did not improve
Epoch 19/24
 - 106s - loss: 0.3392 - acc: 0.8752 - val_loss: 0.7926 - val_acc: 0.7895

Epoch 00019: val_loss did not improve
Epoch 20/24
 - 98s - loss: 0.3237 - acc: 0.8814 - val_loss: 0.7409 - val_acc: 0.7997

Epoch 00020: val_loss did not improve
Epoch 21/24
 - 97s - loss: 0.3123 - acc: 0.8848 - val_loss: 0.8125 - val_acc: 0.7851

Epoch 00021: val_loss did not improve
```

lr=0.0001

```{.python .input}
Epoch 1/32
 - 108s - loss: 0.2639 - acc: 0.9008 - val_loss: 0.5109 - val_acc: 0.8392

Epoch 00001: val_loss improved from inf to 0.51090, saving model to TrainModel/neck2_InceptionV3_weights.h5
Epoch 2/32
 - 100s - loss: 0.2256 - acc: 0.9175 - val_loss: 0.5575 - val_acc: 0.8406

Epoch 00002: val_loss did not improve
Epoch 3/32
 - 101s - loss: 0.2142 - acc: 0.9195 - val_loss: 0.5807 - val_acc: 0.8436

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 100s - loss: 0.2125 - acc: 0.9219 - val_loss: 0.5960 - val_acc: 0.8450

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 100s - loss: 0.1943 - acc: 0.9265 - val_loss: 0.6412 - val_acc: 0.8392

Epoch 00005: val_loss did not improve
Epoch 6/32

neck2_InceptionV3_weights.h5
Epoch 1/32
 - 91s - loss: 0.2270 - acc: 0.9143 - val_loss: 0.6111 - val_acc: 0.8348

Epoch 00001: val_loss improved from inf to 0.61110, saving model to TrainModel/neck2_InceptionV3_weights.h5
Epoch 2/32
 - 96s - loss: 0.2070 - acc: 0.9225 - val_loss: 0.5861 - val_acc: 0.8421

Epoch 00002: val_loss improved from 0.61110 to 0.58614, saving model to TrainModel/neck2_InceptionV3_weights.h5
Epoch 3/32
 - 102s - loss: 0.2047 - acc: 0.9237 - val_loss: 0.6224 - val_acc: 0.8392

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 101s - loss: 0.1918 - acc: 0.9289 - val_loss: 0.6175 - val_acc: 0.8348

Epoch 00004: val_loss did not improve
Epoch 5/32
 - 99s - loss: 0.1849 - acc: 0.9313 - val_loss: 0.6254 - val_acc: 0.8392

Epoch 00005: val_loss did not improve
Epoch 6/32
 - 97s - loss: 0.1710 - acc: 0.9363 - val_loss: 0.6691 - val_acc: 0.8363

Epoch 00006: val_loss did not improve
Epoch 7/32
 - 97s - loss: 0.1787 - acc: 0.9347 - val_loss: 0.6559 - val_acc: 0.8377

Epoch 00007: val_loss did not improve
Epoch 8/32
 - 91s - loss: 0.1812 - acc: 0.9349 - val_loss: 0.6722 - val_acc: 0.8480

Epoch 00008: val_loss did not improve
Epoch 9/32
 - 93s - loss: 0.1696 - acc: 0.9315 - val_loss: 0.6740 - val_acc: 0.8436

Epoch 00009: val_loss did not improve
Epoch 10/32
 - 91s - loss: 0.1559 - acc: 0.9421 - val_loss: 0.7060 - val_acc: 0.8421

Epoch 00010: val_loss did not improve
Epoch 11/32
 - 88s - loss: 0.1498 - acc: 0.9419 - val_loss: 0.6835 - val_acc: 0.8465

Epoch 00011: val_loss did not improve
Epoch 12/32
 - 88s - loss: 0.1603 - acc: 0.9403 - val_loss: 0.7116 - val_acc: 0.8392

Epoch 00012: val_loss did not improve
Epoch 13/32

neck3_InceptionV3_weights.h5
Epoch 1/32
 - 91s - loss: 0.1876 - acc: 0.9265 - val_loss: 0.5960 - val_acc: 0.8465

Epoch 00001: val_loss improved from inf to 0.59600, saving model to TrainModel/neck3_InceptionV3_weights.h5
Epoch 2/32
 - 91s - loss: 0.1877 - acc: 0.9277 - val_loss: 0.6605 - val_acc: 0.8450

Epoch 00002: val_loss did not improve
Epoch 3/32
 - 90s - loss: 0.1881 - acc: 0.9279 - val_loss: 0.6203 - val_acc: 0.8509

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 92s - loss: 0.1742 - acc: 0.9375 - val_loss: 0.6409 - val_acc: 0.8421

Epoch 00004: val_loss did not improve
Epoch 5/32
KeyboardInterrupt

neck4_InceptionV3_weights.h5
Epoch 1/32
 - 91s - loss: 0.1913 - acc: 0.9277 - val_loss: 0.6071 - val_acc: 0.8494

Epoch 00001: val_loss improved from inf to 0.60713, saving model to TrainModel/neck4_InceptionV3_weights.h5
Epoch 2/32
 - 91s - loss: 0.1754 - acc: 0.9335 - val_loss: 0.6258 - val_acc: 0.8406

Epoch 00002: val_loss did not improve
Epoch 3/32
 - 92s - loss: 0.1787 - acc: 0.9295 - val_loss: 0.6439 - val_acc: 0.8333

Epoch 00003: val_loss did not improve
Epoch 4/32
 - 90s - loss: 0.1707 - acc: 0.9357 - val_loss: 0.6521 - val_acc: 0.8406

Epoch 00004: val_loss did not improve
Epoch 5/32
KeyboardInterrupt

TrainModel/neck5_InceptionV3_weights.h5
Epoch 1/32
 - 95s - loss: 0.1854 - acc: 0.9319 - val_loss: 0.6303 - val_acc: 0.8538

Epoch 00001: val_loss improved from inf to 0.63028, saving model to TrainModel/neck5_InceptionV3_weights.h5
Epoch 2/32
 - 82s - loss: 0.1776 - acc: 0.9367 - val_loss: 0.6329 - val_acc: 0.8480

Epoch 00002: val_loss did not improve
Epoch 3/32
KeyboardInterrupt
```

>>> 18.4.20

- 训练类型：neck
- 前提条件：neck_crip3_weights.h5
- 训练参数和模型
- keyWords ：
clip图 adam lr=0.0001
- 模型文件 ：neck_crip4_weights.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
                              #weights='imagenet' #first time
                             weights=None)


inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)  

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
#adam = Adam(lr=0.0001)
sgd=SGD(lr=0.0001, momentum=0.9, decay=0.0, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0.1, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_crip3_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}_crip2_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

- 训练结果

- 训练类型：neck
- 前提条件：neck_crip3_weights.h5
- 训练参数和模型
- keyWords ： clip图
adam
lr=0.0001
- 模型文件 ：neck_crip4_adam_weights.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
                              #weights='imagenet' #first time
                             weights=None)


inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)  

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=0.0001)
#sgd=SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=True)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0.1, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_crip4_adam_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}_crip3_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

- 训练结果

```{.python .input}
Epoch 1/24
 - 189s - loss: 0.2505 - acc: 0.9149 - val_loss: 0.6059 - val_acc: 0.8099

Epoch 00001: val_loss improved from inf to 0.60590, saving model to neck_crip4_adam_weights.h5
Epoch 2/24
 - 162s - loss: 0.2243 - acc: 0.9259 - val_loss: 0.6638 - val_acc: 0.7924

Epoch 00002: val_loss did not improve
Epoch 3/24
 - 161s - loss: 0.2127 - acc: 0.9253 - val_loss: 0.6274 - val_acc: 0.8056

Epoch 00003: val_loss did not improve
Epoch 4/24
 - 161s - loss: 0.1803 - acc: 0.9371 - val_loss: 0.6918 - val_acc: 0.7953

Epoch 00004: val_loss did not improve
Epoch 5/24
KeyboardInterrupt


```

- 训练类型：neck
- 前提条件：neck_crip2_weights.h5
- 训练参数和模型
- keyWords ： clip图 sgd
- 模型文件
：neck_crip3_weights.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
                              #weights='imagenet' #first time
                             weights=None)


inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)  

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
#adam = Adam(lr=0.0001)
sgd=SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0.1, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_crip3_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}_crip2_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

- 训练结果

```{.python .input}
Epoch 1/24
 - 175s - loss: 0.2879 - acc: 0.8976 - val_loss: 0.5868 - val_acc: 0.8056

Epoch 00001: val_loss improved from inf to 0.58685, saving model to neck_crip3_weights.h5
Epoch 2/24
 - 157s - loss: 0.2727 - acc: 0.9107 - val_loss: 0.5859 - val_acc: 0.8026

Epoch 00002: val_loss improved from 0.58685 to 0.58589, saving model to neck_crip3_weights.h5
Epoch 3/24
 - 156s - loss: 0.2456 - acc: 0.9131 - val_loss: 0.5747 - val_acc: 0.8070

Epoch 00003: val_loss improved from 0.58589 to 0.57466, saving model to neck_crip3_weights.h5
Epoch 4/24
 - 155s - loss: 0.2541 - acc: 0.9169 - val_loss: 0.5719 - val_acc: 0.8026

Epoch 00004: val_loss improved from 0.57466 to 0.57188, saving model to neck_crip3_weights.h5
Epoch 5/24
 - 155s - loss: 0.2444 - acc: 0.9171 - val_loss: 0.5854 - val_acc: 0.8012

Epoch 00005: val_loss did not improve
Epoch 6/24
 - 155s - loss: 0.2343 - acc: 0.9207 - val_loss: 0.5924 - val_acc: 0.8012

Epoch 00006: val_loss did not improve
Epoch 7/24
 - 156s - loss: 0.2306 - acc: 0.9207 - val_loss: 0.5986 - val_acc: 0.8041

Epoch 00007: val_loss did not improve
Epoch 8/24
 - 155s - loss: 0.2407 - acc: 0.9203 - val_loss: 0.5962 - val_acc: 0.7982

Epoch 00008: val_loss did not improve
Epoch 9/24
```

- 训练类型：neck
- 前提条件：neck_crip1_weights.h5
- 训练参数和模型
- keyWords ： clip图 sgd
- 模型文件
：neck_crip2_weights.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
                              #weights='imagenet' #first time
                             weights=None)

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)                             


X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
#adam = Adam(lr=0.0001)
sgd=SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 24
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0.1, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_crip2_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}_crip1_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')



```

训练结果

```{.python .input}
Epoch 1/24
 - 175s - loss: 0.3816 - acc: 0.8680 - val_loss: 0.6555 - val_acc: 0.7792

Epoch 00001: val_loss improved from inf to 0.65546, saving model to neck_crip2_weights.h5
Epoch 2/24
 - 157s - loss: 0.3622 - acc: 0.8698 - val_loss: 0.7266 - val_acc: 0.7617

Epoch 00002: val_loss did not improve
Epoch 3/24
 - 157s - loss: 0.3583 - acc: 0.8766 - val_loss: 0.5786 - val_acc: 0.7939

Epoch 00003: val_loss improved from 0.65546 to 0.57856, saving model to neck_crip2_weights.h5
Epoch 4/24
 - 156s - loss: 0.3343 - acc: 0.8810 - val_loss: 0.6811 - val_acc: 0.7851

Epoch 00004: val_loss did not improve
Epoch 5/24
 - 156s - loss: 0.3048 - acc: 0.8904 - val_loss: 0.6285 - val_acc: 0.7763

Epoch 00005: val_loss did not improve
Epoch 6/24
 - 156s - loss: 0.3047 - acc: 0.8916 - val_loss: 0.6630 - val_acc: 0.8026

Epoch 00006: val_loss did not improve
Epoch 7/24
 - 156s - loss: 0.2727 - acc: 0.9052 - val_loss: 0.8113 - val_acc: 0.7705

Epoch 00007: val_loss did not improve
Epoch 8/24
 - 157s - loss: 0.2529 - acc: 0.9103 - val_loss: 0.7674 - val_acc: 0.7734

Epoch 00008: val_loss did not improve
```

- 训练类型：neck
- 前提条件：neck_crip_3_weights.h5
- 训练参数和模型
- keyWords ： clip图、sgd、
-
模型文件 ：neck_crip_4_weights.h5

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
# weights='imagenet' #first time
weights='None')#load parameters
inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_t


sgd=SGD(lr=0.001, momentum=0.9, decay=0.0, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 24
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0, # randomly zoom image
        width_shift_range = 0.1, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.0, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_crip_3_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}_crip_2_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')



```

训练结果

```{.python .input}
poch 1/24
 - 177s - loss: 0.3225 - acc: 0.8900 - val_loss: 0.5453 - val_acc: 0.8099

Epoch 00001: val_loss improved from inf to 0.54525, saving model to neck_crip_3_weights.h5
Epoch 2/24
 - 154s - loss: 0.3040 - acc: 0.9014 - val_loss: 0.5588 - val_acc: 0.8070

Epoch 00002: val_loss did not improve
Epoch 3/24
 - 154s - loss: 0.2942 - acc: 0.8996 - val_loss: 0.5518 - val_acc: 0.8041

Epoch 00003: val_loss did not improve
Epoch 4/24
 - 154s - loss: 0.2837 - acc: 0.9018 - val_loss: 0.5641 - val_acc: 0.8099

Epoch 00004: val_loss did not improve
Epoch 5/24
 - 154s - loss: 0.2809 - acc: 0.9030 - val_loss: 0.5747 - val_acc: 0.8041

Epoch 00005: val_loss did not improve
Epoch 6/24
 - 154s - loss: 0.2814 - acc: 0.9028 - val_loss: 0.5707 - val_acc: 0.8085

Epoch 00006: val_loss did not improve
Epoch 7/24
 - 154s - loss: 0.2636 - acc: 0.9054 - val_loss: 0.5637 - val_acc: 0.8129

Epoch 00007: val_loss did not improve
Epoch 8/24
 - 154s - loss: 0.2640 - acc: 0.9103 - val_loss: 0.5627 - val_acc: 0.8143

Epoch 00008: val_loss did not improve
Epoch 9/24
 - 154s - loss: 0.2599 - acc: 0.9111 - val_loss: 0.5729 - val_acc: 0.8085

Epoch 00009: val_loss did not improve
Epoch 10/24
KeyboardInterrupt

```

- 训练类型：neck
- 前提条件：neck_crip1_weights.h5，lr=0.001，epoch=8
- 训练参数和模型
- keyWords ：
clip图
- 模型文件 ：neck_crip_2_weights.h5

```{.python .input}
crip=int(width/2)
X_padding=np.zeros_like(X[0][crip:])
for i in tqdm(range(n)):    
    X[i]=np.concatenate((X[i][:crip],X_padding))

from keras.layers import *
from keras.models import *
from keras.callbacks import *
from keras.optimizers import *
from keras.applications import *
from keras.regularizers import *
from keras.applications.inception_v3 import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
from keras.optimizers import RMSprop
import keras


cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 24
batch_size = 32

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_crip2_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}10_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=20), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

- 训练结果

```{.python .input}
Epoch 1/24
 - 159s - loss: 0.7055 - acc: 0.7328 - val_loss: 1.6100 - val_acc: 0.2295

Epoch 00001: val_loss improved from inf to 1.60997, saving model to neck_crip_2_weights.h5
Epoch 2/24
 - 159s - loss: 0.6119 - acc: 0.7688 - val_loss: 1.1753 - val_acc: 0.7120

Epoch 00002: val_loss improved from 1.60997 to 1.17532, saving model to neck_crip_2_weights.h5
Epoch 3/24
 - 159s - loss: 0.5873 - acc: 0.7796 - val_loss: 0.6056 - val_acc: 0.7865

Epoch 00003: val_loss improved from 1.17532 to 0.60563, saving model to neck_crip_2_weights.h5
Epoch 4/24
 - 159s - loss: 0.5465 - acc: 0.7955 - val_loss: 0.5784 - val_acc: 0.8012

Epoch 00004: val_loss improved from 0.60563 to 0.57836, saving model to neck_crip_2_weights.h5
Epoch 5/24
 - 159s - loss: 0.5199 - acc: 0.8105 - val_loss: 0.6131 - val_acc: 0.7909

Epoch 00005: val_loss did not improve
Epoch 6/24
 - 159s - loss: 0.4982 - acc: 0.8141 - val_loss: 0.5588 - val_acc: 0.8070

Epoch 00006: val_loss improved from 0.57836 to 0.55876, saving model to neck_crip_2_weights.h5
Epoch 7/24
 - 159s - loss: 0.4925 - acc: 0.8179 - val_loss: 0.5818 - val_acc: 0.8041

Epoch 00007: val_loss did not improve
Epoch 8/24
 - 159s - loss: 0.4598 - acc: 0.8293 - val_loss: 0.6323 - val_acc: 0.7909

Epoch 00008: val_loss did not improve
Epoch 9/24
 - 159s - loss: 0.4471 - acc: 0.8367 - val_loss: 0.6313 - val_acc: 0.7924

Epoch 00009: val_loss did not improve
Epoch 10/24
 - 159s - loss: 0.4294 - acc: 0.8414 - val_loss: 0.6304 - val_acc: 0.7982

Epoch 00010: val_loss did not improve
Epoch 11/24
 - 159s - loss: 0.4155 - acc: 0.8476 - val_loss: 0.6500 - val_acc: 0.7909

Epoch 00011: val_loss did not improve
Epoch 12/24
 - 159s - loss: 0.4106 - acc: 0.8456 - val_loss: 0.6057 - val_acc: 0.8041

Epoch 00012: val_loss did not improve
Epoch 13/24
 - 159s - loss: 0.3904 - acc: 0.8574 - val_loss: 0.6130 - val_acc: 0.8056

Epoch 00013: val_loss did not improve
Epoch 14/24
 - 159s - loss: 0.3605 - acc: 0.8660 - val_loss: 0.6811 - val_acc: 0.7851

Epoch 00014: val_loss did not improve
Epoch 15/24
 - 159s - loss: 0.3438 - acc: 0.8732 - val_loss: 0.6674 - val_acc: 0.8026

Epoch 00015: val_loss did not improve
Epoch 16/24
 - 159s - loss: 0.3444 - acc: 0.8666 - val_loss: 0.6482 - val_acc: 0.8085

Epoch 00016: val_loss did not improve
Epoch 17/24
 - 159s - loss: 0.3191 - acc: 0.8812 - val_loss: 0.6902 - val_acc: 0.7939

Epoch 00017: val_loss did not improve
Epoch 18/24
KeyboardInterrupt

```

- 训练类型：neck
- 前提条件：无
- 训练参数和模型 modelgluo1.params
- keyWords ： lr=0.0001
- 模型文件
：modelgluo2.params

```{.python .input}
mxnet baseline

```

- 训练结果

```{.python .input}
```
- 训练类型：neck
- 前提条件：无
- 训练参数和模型
- keyWords ： lr=0.001
- 模型文件 ：modelgluo1.params
```

mxnet baseline

```{.python .input}
- 训练结果
```

[Epoch 0] Train-acc: 0.373, mAP: 0.599, loss: 1.446 | Val-acc: 0.480, mAP:
0.683, loss: 1.238 | time: 48.9
[Epoch 1] Train-acc: 0.614, mAP: 0.769, loss:
0.990 | Val-acc: 0.634, mAP: 0.786, loss: 0.927 | time: 41.7
[Epoch 2] Train-
acc: 0.729, mAP: 0.841, loss: 0.735 | Val-acc: 0.705, mAP: 0.826, loss: 0.786 |
time: 42.2
[Epoch 3] Train-acc: 0.792, mAP: 0.880, loss: 0.578 | Val-acc: 0.721,
mAP: 0.839, loss: 0.710 | time: 42.2
[Epoch 4] Train-acc: 0.830, mAP: 0.904,
loss: 0.477 | Val-acc: 0.747, mAP: 0.852, loss: 0.680 | time: 41.8
[Epoch 5]
Train-acc: 0.870, mAP: 0.928, loss: 0.367 | Val-acc: 0.733, mAP: 0.845, loss:
0.714 | time: 41.9
[Epoch 6] Train-acc: 0.902, mAP: 0.946, loss: 0.294 | Val-
acc: 0.752, mAP: 0.853, loss: 0.706 | time: 42.1
[Epoch 7] Train-acc: 0.921,
mAP: 0.958, loss: 0.238 | Val-acc: 0.754, mAP: 0.859, loss: 0.711 | time: 41.8
[Epoch 8] Train-acc: 0.945, mAP: 0.971, loss: 0.184 | Val-acc: 0.764, mAP:
0.861, loss: 0.740 | time: 42.1
[Epoch 9] Train-acc: 0.958, mAP: 0.978, loss:
0.145 | Val-acc: 0.759, mAP: 0.859, loss: 0.746 | time: 42.3
[Epoch 10] Train-
acc: 0.967, mAP: 0.983, loss: 0.115 | Val-acc: 0.756, mAP: 0.858, loss: 0.793 |
time: 43.0
[Epoch 11] Train-acc: 0.976, mAP: 0.988, loss: 0.094 | Val-acc:
0.750, mAP: 0.857, loss: 0.825 | time: 43.9
[Epoch 12] Train-acc: 0.978, mAP:
0.988, loss: 0.093 | Val-acc: 0.768, mAP: 0.867, loss: 0.846 | time: 44.5
[Epoch
13] Train-acc: 0.984, mAP: 0.992, loss: 0.069 | Val-acc: 0.768, mAP: 0.865,
loss: 0.814 | time: 43.9
[Epoch 14] Train-acc: 0.986, mAP: 0.993, loss: 0.056 |
Val-acc: 0.777, mAP: 0.868, loss: 0.856 | time: 44.1
[Epoch 15] Train-acc:
0.985, mAP: 0.992, loss: 0.052 | Val-acc: 0.754, mAP: 0.857, loss: 0.876 | time:
44.1
[Epoch 16] Train-acc: 0.987, mAP: 0.993, loss: 0.054 | Val-acc: 0.749, mAP:
0.854, loss: 0.856 | time: 44.3
[Epoch 17] Train-acc: 0.987, mAP: 0.993, loss:
0.047 | Val-acc: 0.752, mAP: 0.853, loss: 0.903 | time: 44.2
[Epoch 18] Train-
acc: 0.990, mAP: 0.995, loss: 0.041 | Val-acc: 0.729, mAP: 0.842, loss: 1.042 |
time: 44.0
[Epoch 19] Train-acc: 0.990, mAP: 0.995, loss: 0.036 | Val-acc:
0.750, mAP: 0.853, loss: 0.945 | time: 44.5
[Epoch 20] Train-acc: 0.994, mAP:
0.997, loss: 0.037 | Val-acc: 0.756, mAP: 0.856, loss: 0.900 | time: 43.7
[Epoch
21] Train-acc: 0.994, mAP: 0.997, loss: 0.028 | Val-acc: 0.773, mAP: 0.867,
loss: 0.915 | time: 42.0
[Epoch 22] Train-acc: 0.996, mAP: 0.998, loss: 0.022 |
Val-acc: 0.757, mAP: 0.859, loss: 0.887 | time: 42.3
[Epoch 23] Train-acc:
0.996, mAP: 0.998, loss: 0.024 | Val-acc: 0.754, mAP: 0.857, loss: 0.948 | time:
43.9
[Epoch 24] Train-acc: 0.995, mAP: 0.998, loss: 0.023 | Val-acc: 0.764, mAP:
0.862, loss: 0.976 | time: 43.9
[Epoch 25] Train-acc: 0.996, mAP: 0.998, loss:
0.018 | Val-acc: 0.764, mAP: 0.861, loss: 0.975 | time: 42.9
[Epoch 26] Train-
acc: 0.996, mAP: 0.998, loss: 0.018 | Val-acc: 0.759, mAP: 0.859, loss: 1.009 |
time: 42.1
[Epoch 27] Train-acc: 0.995, mAP: 0.998, loss: 0.020 | Val-acc:
0.754, mAP: 0.859, loss: 0.951 | time: 42.2
[Epoch 28] Train-acc: 0.996, mAP:
0.998, loss: 0.021 | Val-acc: 0.757, mAP: 0.859, loss: 0.966 | time: 42.3
[Epoch
29] Train-acc: 0.997, mAP: 0.998, loss: 0.018 | Val-acc: 0.766, mAP: 0.863,
loss: 0.936 | time: 42.4
CPU times: user 16min 59s, sys: 4min 4s, total: 21min
4s
Wall time: 21min 35s

```{.python .input}

>>> 18.4.19

- 训练类型：coat
- 前提条件：coat1_Xception_weights.h5
- 训练参数和模型
- keyWords ： 全量训练
- 模型文件 ：coat2_Xception_weights.h5
=>coat_0419b_Xception_tta.csv

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam =
Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center = False, # set input mean to 0
over the dataset
        samplewise_center = False, # set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift
images horizontally (fraction of total width)
        height_shift_range =
0.075, # randomly shift images vertivally (fraction of total heigth)
horizontal_flip = False, # randomly flip images
        vertical_flip = False,
shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)
datagen.fit(X,seed=123)

prefix_cls = cur_class.split('_')[0]
save_filepath='{0}2_Xception_weights.h5'.format(prefix_cls)
load_filepath='{0}1_Xception_weights.h5'.format(prefix_cls)

%%time
prefix_cls =
cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath=save_filepath,
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')

model.load_weights(load_filepath)

try:
# Fit the model
    history = model.fit_generator(datagen.flow(X, y,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=7), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
- 训练结果

```

Epoch 1/32
 - 346s - loss: 0.0976 - acc: 0.9707 - val_loss: 0.1197 - val_acc:
0.9654

Epoch 00001: val_loss improved from inf to 0.11970, saving model to
coat2_Xception_weights.h5
Epoch 2/32
 - 343s - loss: 0.0654 - acc: 0.9812 -
val_loss: 0.0788 - val_acc: 0.9779

Epoch 00002: val_loss improved from 0.11970
to 0.07885, saving model to coat2_Xception_weights.h5
Epoch 3/32
 - 343s - loss:
0.0518 - acc: 0.9847 - val_loss: 0.0645 - val_acc: 0.9801

Epoch 00003: val_loss
improved from 0.07885 to 0.06452, saving model to coat2_Xception_weights.h5
Epoch 4/32
 - 343s - loss: 0.0359 - acc: 0.9890 - val_loss: 0.0447 - val_acc:
0.9853

Epoch 00004: val_loss improved from 0.06452 to 0.04474, saving model to
coat2_Xception_weights.h5
Epoch 5/32
 - 343s - loss: 0.0330 - acc: 0.9905 -
val_loss: 0.0312 - val_acc: 0.9904

Epoch 00005: val_loss improved from 0.04474
to 0.03122, saving model to coat2_Xception_weights.h5
Epoch 6/32
 - 343s - loss:
0.0239 - acc: 0.9928 - val_loss: 0.0340 - val_acc: 0.9912

Epoch 00006: val_loss
did not improve
Epoch 7/32
 - 343s - loss: 0.0222 - acc: 0.9930 - val_loss:
0.0221 - val_acc: 0.9919

Epoch 00007: val_loss improved from 0.03122 to
0.02209, saving model to coat2_Xception_weights.h5
Epoch 8/32
KeyboardInterrupt
CPU times: user 47min 44s, sys: 7min 42s, total: 55min 27s
Wall time: 43min 22s

```{.python .input}


- 训练类型：neck
- 前提条件：neck1_Xception_weights.h5
- 训练参数和模型
- keyWords ： 全量训练 +
- 模型文件 ：neck2_Xception_weights.h5
=>neck_0419b_Xception_tta.csv
```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam =
Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 32
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X,seed=123)

prefix_cls =
cur_class.split('_')[0]
save_filepath='{0}2_Xception_weights.h5'.format(prefix_cls)
load_filepath='{0}1_Xception_weights.h5'.format(prefix_cls)

%%time
prefix_cls =
cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath=save_filepath,
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')

model.load_weights(load_filepath)

try:
# Fit the model
    history = model.fit_generator(datagen.flow(X, y,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=7), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}

- 训练结果

```

Epoch 1/32
 - 181s - loss: 0.3268 - acc: 0.8815 - val_loss: 0.1697 - val_acc:
0.9459

Epoch 00001: val_loss improved from inf to 0.16970, saving model to
neck2_Xception_weights.h5
Epoch 2/32
 - 174s - loss: 0.2976 - acc: 0.8958 -
val_loss: 0.1513 - val_acc: 0.9444

Epoch 00002: val_loss improved from 0.16970
to 0.15126, saving model to neck2_Xception_weights.h5
Epoch 3/32
 - 174s - loss:
0.2848 - acc: 0.8944 - val_loss: 0.1399 - val_acc: 0.9605

Epoch 00003: val_loss
improved from 0.15126 to 0.13991, saving model to neck2_Xception_weights.h5
Epoch 4/32
 - 173s - loss: 0.2728 - acc: 0.9025 - val_loss: 0.1251 - val_acc:
0.9547

Epoch 00004: val_loss improved from 0.13991 to 0.12515, saving model to
neck2_Xception_weights.h5
Epoch 5/32
 - 173s - loss: 0.2465 - acc: 0.9098 -
val_loss: 0.1292 - val_acc: 0.9576

Epoch 00005: val_loss did not improve
Epoch
6/32
 - 173s - loss: 0.2495 - acc: 0.9074 - val_loss: 0.1005 - val_acc: 0.9635
Epoch 00006: val_loss improved from 0.12515 to 0.10049, saving model to
neck2_Xception_weights.h5
Epoch 7/32
 - 174s - loss: 0.2360 - acc: 0.9149 -
val_loss: 0.0860 - val_acc: 0.9708

Epoch 00007: val_loss improved from 0.10049
to 0.08598, saving model to neck2_Xception_weights.h5
Epoch 8/32
 - 173s - loss:
0.2415 - acc: 0.9138 - val_loss: 0.0767 - val_acc: 0.9722

Epoch 00008: val_loss
improved from 0.08598 to 0.07668, saving model to neck2_Xception_weights.h5
Epoch 9/32
 - 173s - loss: 0.1995 - acc: 0.9281 - val_loss: 0.0703 - val_acc:
0.9766

Epoch 00009: val_loss improved from 0.07668 to 0.07026, saving model to
neck2_Xception_weights.h5
Epoch 10/32
 - 173s - loss: 0.1997 - acc: 0.9266 -
val_loss: 0.0709 - val_acc: 0.9766

Epoch 00010: val_loss did not improve
Epoch
11/32
 - 173s - loss: 0.2100 - acc: 0.9214 - val_loss: 0.0631 - val_acc: 0.9795
Epoch 00011: val_loss improved from 0.07026 to 0.06314, saving model to
neck2_Xception_weights.h5
Epoch 12/32
 - 173s - loss: 0.1773 - acc: 0.9366 -
val_loss: 0.0586 - val_acc: 0.9839

Epoch 00012: val_loss improved from 0.06314
to 0.05857, saving model to neck2_Xception_weights.h5
Epoch 13/32
 - 173s -
loss: 0.1803 - acc: 0.9325 - val_loss: 0.0551 - val_acc: 0.9868

Epoch 00013:
val_loss improved from 0.05857 to 0.05508, saving model to
neck2_Xception_weights.h5
Epoch 14/32
 - 174s - loss: 0.1757 - acc: 0.9380 -
val_loss: 0.0466 - val_acc: 0.9854

Epoch 00014: val_loss improved from 0.05508
to 0.04659, saving model to neck2_Xception_weights.h5
Epoch 15/32
 - 173s -
loss: 0.1619 - acc: 0.9421 - val_loss: 0.0362 - val_acc: 0.9898

Epoch 00015:
val_loss improved from 0.04659 to 0.03621, saving model to
neck2_Xception_weights.h5
Epoch 16/32
 - 173s - loss: 0.1445 - acc: 0.9493 -
val_loss: 0.0426 - val_acc: 0.9868

Epoch 00016: val_loss did not improve
Epoch
17/32
 - 173s - loss: 0.1371 - acc: 0.9497 - val_loss: 0.0279 - val_acc: 0.9927
Epoch 00017: val_loss improved from 0.03621 to 0.02786, saving model to
neck2_Xception_weights.h5
Epoch 18/32
 - 173s - loss: 0.1409 - acc: 0.9459 -
val_loss: 0.0381 - val_acc: 0.9898

Epoch 00018: val_loss did not improve
Epoch
19/32
 - 173s - loss: 0.1439 - acc: 0.9435 - val_loss: 0.0307 - val_acc: 0.9898
Epoch 00019: val_loss did not improve
Epoch 20/32
 - 173s - loss: 0.1322 - acc:
0.9518 - val_loss: 0.0229 - val_acc: 0.9927

Epoch 00020: val_loss improved from
0.02786 to 0.02291, saving model to neck2_Xception_weights.h5
Epoch 21/32
 -
173s - loss: 0.1264 - acc: 0.9529 - val_loss: 0.0231 - val_acc: 0.9942

Epoch
00021: val_loss did not improve
Epoch 22/32
Epoch 00021: val_loss did not
improve
Epoch 22/32
 - 173s - loss: 0.1286 - acc: 0.9512 - val_loss: 0.0196 -
val_acc: 0.9971

Epoch 00022: val_loss improved from 0.02291 to 0.01956, saving
model to neck2_Xception_weights.h5
Epoch 23/32
 - 173s - loss: 0.1221 - acc:
0.9565 - val_loss: 0.0221 - val_acc: 0.9956

Epoch 00023: val_loss did not
improve
Epoch 24/32
 - 173s - loss: 0.1152 - acc: 0.9573 - val_loss: 0.0111 -
val_acc: 0.9985

Epoch 00024: val_loss improved from 0.01956 to 0.01110, saving
model to neck2_Xception_weights.h5
Epoch 25/32
KeyboardInterrupt
CPU times: user
1h 15min 15s, sys: 12min 45s, total: 1h 28min
Wall time: 1h 10min 45s

```{.python .input}

- 训练类型：neck
- 前提条件：neck10_weights.h5
- 训练参数和模型
- keyWords ： 全量训练
- 模型文件 ：neck11_weights.h5
=>neck_0419b_tta.csv
```

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

epochs = 32
batch_size = 32
datagen = ImageDataGenerator(
        featurewise_center =False ,# set input
mean to 0 over the dataset
        samplewise_center = False ,# set each sample
mean to 0
        featurewise_std_normalization = False, # divide inputs by std
of the dataset
        samplewise_std_normalization = False, # divide each input
by its std
        zca_whitening = False, # apply ZCA whitening
rotation_range =
10, # randomly rotate images in the range (degrees, 0 to 180)
zoom_range = 0.3,
# randomly zoom image
        width_shift_range = 0.2, #
randomly shift images
horizontally (fraction of total width)
height_shift_range = 0.2, # randomly
shift images vertivally (fraction of total
heigth)
        horizontal_flip =
True, # randomly flip images
vertical_flip = False,
        shear_range = 0.1,
fill_mode =
'constant',
        cval = 0)

datagen.fit(X,seed=123)

prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}11_weights.h5'.format(prefix_cls), verbose=1,
save_best_only=True, save_weights_only=True,mode='val_acc')
model.load_weights('{0}10_weights.h5'.format(prefix_cls))

try:
    # Fit the
model
    history = model.fit_generator(datagen.flow(X, y,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=7), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
- 训练结果

```

Epoch 1/32
 - 191s - loss: 0.2589 - acc: 0.9106 - val_loss: 0.0970 - val_acc:
0.9737

Epoch 00001: val_loss improved from inf to 0.09698, saving model to
neck11_weights.h5
Epoch 2/32
 - 163s - loss: 0.2506 - acc: 0.9118 - val_loss:
0.0880 - val_acc: 0.9751

Epoch 00002: val_loss improved from 0.09698 to
0.08798, saving model to neck11_weights.h5
Epoch 3/32
 - 163s - loss: 0.2381 -
acc: 0.9147 - val_loss: 0.0867 - val_acc: 0.9722

Epoch 00003: val_loss improved
from 0.08798 to 0.08667, saving model to neck11_weights.h5
Epoch 4/32
 - 163s -
loss: 0.2295 - acc: 0.9139 - val_loss: 0.0760 - val_acc: 0.9766

Epoch 00004:
val_loss improved from 0.08667 to 0.07602, saving model to neck11_weights.h5
Epoch 5/32
 - 163s - loss: 0.2211 - acc: 0.9231 - val_loss: 0.0736 - val_acc:
0.9737

Epoch 00005: val_loss improved from 0.07602 to 0.07364, saving model to
neck11_weights.h5
Epoch 6/32
 - 163s - loss: 0.2121 - acc: 0.9198 - val_loss:
0.0696 - val_acc: 0.9766

Epoch 00006: val_loss improved from 0.07364 to
0.06963, saving model to neck11_weights.h5
Epoch 7/32
 - 163s - loss: 0.2142 -
acc: 0.9251 - val_loss: 0.0640 - val_acc: 0.9781

Epoch 00007: val_loss improved
from 0.06963 to 0.06402, saving model to neck11_weights.h5
Epoch 8/32
 - 163s -
loss: 0.1985 - acc: 0.9281 - val_loss: 0.0625 - val_acc: 0.9825

Epoch 00008:
val_loss improved from 0.06402 to 0.06253, saving model to neck11_weights.h5
Epoch 9/32
 - 163s - loss: 0.1949 - acc: 0.9289 - val_loss: 0.0541 - val_acc:
0.9839

Epoch 00009: val_loss improved from 0.06253 to 0.05405, saving model to
neck11_weights.h5
Epoch 10/32
 - 163s - loss: 0.1947 - acc: 0.9316 - val_loss:
0.0489 - val_acc: 0.9839

Epoch 00010: val_loss improved from 0.05405 to
0.04893, saving model to neck11_weights.h5
Epoch 11/32
 - 163s - loss: 0.1868 -
acc: 0.9302 - val_loss: 0.0517 - val_acc: 0.9825

Epoch 00011: val_loss did not
improve
Epoch 12/32
 - 163s - loss: 0.1758 - acc: 0.9374 - val_loss: 0.0476 -
val_acc: 0.9825

Epoch 00012: val_loss improved from 0.04893 to 0.04756, saving
model to neck11_weights.h5
Epoch 13/32
 - 163s - loss: 0.1819 - acc: 0.9307 -
val_loss: 0.0479 - val_acc: 0.9839

Epoch 00013: val_loss did not improve
Epoch
14/32
 - 163s - loss: 0.1807 - acc: 0.9313 - val_loss: 0.0442 - val_acc: 0.9810
Epoch 00014: val_loss improved from 0.04756 to 0.04423, saving model to
neck11_weights.h5
Epoch 15/32
 - 163s - loss: 0.1625 - acc: 0.9402 - val_loss:
0.0421 - val_acc: 0.9839

Epoch 00015: val_loss improved from 0.04423 to
0.04213, saving model to neck11_weights.h5
Epoch 16/32
 - 163s - loss: 0.1669 -
acc: 0.9406 - val_loss: 0.0372 - val_acc: 0.9868

Epoch 00016: val_loss improved
from 0.04213 to 0.03723, saving model to neck11_weights.h5
Epoch 17/32
 - 163s -
loss: 0.1681 - acc: 0.9384 - val_loss: 0.0347 - val_acc: 0.9839

Epoch 00017:
val_loss improved from 0.03723 to 0.03469, saving model to neck11_weights.h5
Epoch 18/32
 - 163s - loss: 0.1694 - acc: 0.9364 - val_loss: 0.0298 - val_acc:
0.9912

Epoch 00018: val_loss improved from 0.03469 to 0.02985, saving model to
neck11_weights.h5
Epoch 19/32
 - 163s - loss: 0.1596 - acc: 0.9429 - val_loss:
0.0385 - val_acc: 0.9854

Epoch 00019: val_loss did not improve
Epoch 20/32
 -
163s - loss: 0.1517 - acc: 0.9412 - val_loss: 0.0324 - val_acc: 0.9883

Epoch
00020: val_loss did not improve
Epoch 21/32
 - 163s - loss: 0.1534 - acc: 0.9456
- val_loss: 0.0344 - val_acc: 0.9883

Epoch 00021: val_loss did not improve
Epoch 22/32
 - 163s - loss: 0.1607 - acc: 0.9389 - val_loss: 0.0259 - val_acc:
0.9912

Epoch 00022: val_loss improved from 0.02985 to 0.02590, saving model to
neck11_weights.h5
Epoch 23/32
 - 163s - loss: 0.1555 - acc: 0.9397 - val_loss:
0.0283 - val_acc: 0.9912

Epoch 00023: val_loss did not improve
Epoch 24/32
 -
163s - loss: 0.1463 - acc: 0.9473 - val_loss: 0.0278 - val_acc: 0.9883

Epoch
00024: val_loss did not improve
Epoch 25/32
 - 163s - loss: 0.1433 - acc: 0.9489
- val_loss: 0.0306 - val_acc: 0.9883

Epoch 00025: val_loss did not improve
Epoch 26/32
 - 163s - loss: 0.1287 - acc: 0.9525 - val_loss: 0.0226 - val_acc:
0.9942

Epoch 00026: val_loss improved from 0.02590 to 0.02264, saving model to
neck11_weights.h5
Epoch 27/32
 - 163s - loss: 0.1500 - acc: 0.9449 - val_loss:
0.0286 - val_acc: 0.9898

Epoch 00027: val_loss did not improve
Epoch 28/32
KeyboardInterrupt

```{.python .input}


>>> 18.4.3


- 训练类型：neck
- 前提条件：无
- 训练参数和模型
- keyWords resnet50 ,lr=0.001
```

cnn_model = ResNet50(include_top=False, input_shape=(width, width, 3),
weights='imagenet')


inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[0], loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}_ResNet50_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}_Xception_new_weights.h5'.format(prefix_cls))

try:
#
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果,收敛太差，手动暂停
```

Epoch 1/64
 - 76s - loss: 1.7928 - acc: 0.2512 - val_loss: 2.2014 - val_acc:
0.1959

Epoch 00001: val_loss improved from inf to 2.20137, saving model to
neck_ResNet50_weights.h5
Epoch 2/64
 - 61s - loss: 1.6771 - acc: 0.2388 -
val_loss: 4.8440 - val_acc: 0.1506

Epoch 00002: val_loss did not improve
Epoch
3/64
 - 61s - loss: 1.6223 - acc: 0.2420 - val_loss: 1.6114 - val_acc: 0.2588
Epoch 00003: val_loss improved from 2.20137 to 1.61135, saving model to
neck_ResNet50_weights.h5
Epoch 4/64
 - 61s - loss: 1.5944 - acc: 0.2596 -
val_loss: 1.5676 - val_acc: 0.2749

Epoch 00004: val_loss improved from 1.61135
to 1.56757, saving model to neck_ResNet50_weights.h5
Epoch 5/64
 - 61s - loss:
1.5842 - acc: 0.2734 - val_loss: 1.5979 - val_acc: 0.2456

Epoch 00005: val_loss
did not improve
Epoch 6/64
 - 61s - loss: 1.5662 - acc: 0.2835 - val_loss:
2.4354 - val_acc: 0.2602

Epoch 00006: val_loss did not improve
Epoch 7/64
 -
61s - loss: 1.5738 - acc: 0.2778 - val_loss: 1.5763 - val_acc: 0.2617

Epoch
00007: val_loss did not improve
Epoch 8/64
 - 61s - loss: 1.5760 - acc: 0.2875 -
val_loss: 1.8183 - val_acc: 0.1871

Epoch 00008: val_loss did not improve
Epoch
9/64
 - 61s - loss: 1.5873 - acc: 0.2764 - val_loss: 1.6303 - val_acc: 0.2500
Epoch 00009: val_loss did not improve
Epoch 10/64
 - 61s - loss: 1.5843 - acc:
0.2704 - val_loss: 1.5982 - val_acc: 0.2734

Epoch 00010: val_loss did not
improve
Epoch 11/64
 - 61s - loss: 1.5670 - acc: 0.2825 - val_loss: 1.7174 -
val_acc: 0.2398

Epoch 00011: val_loss did not improve
Epoch 12/64
 - 62s -
loss: 1.5489 - acc: 0.2953 - val_loss: 1.8762 - val_acc: 0.2047

Epoch 00012:
val_loss did not improve
Epoch 13/64
 - 61s - loss: 1.5544 - acc: 0.2995 -
val_loss: 1.8126 - val_acc: 0.2325

Epoch 00013: val_loss did not improve
Epoch
14/64
 - 62s - loss: 1.5439 - acc: 0.3067 - val_loss: 1.8296 - val_acc: 0.2412
Epoch 00014: val_loss did not improve
Epoch 15/64
 - 62s - loss: 1.5450 - acc:
0.3029 - val_loss: 1.6188 - val_acc: 0.2909

Epoch 00015: val_loss did not
improve
Epoch 16/64
 - 61s - loss: 1.5369 - acc: 0.3099 - val_loss: 1.6834 -
val_acc: 0.2749

Epoch 00016: val_loss did not improve
Epoch 17/64
 - 61s -
loss: 1.5314 - acc: 0.3217 - val_loss: 1.6301 - val_acc: 0.2763

Epoch 00017:
val_loss did not improve
Epoch 18/64
 - 61s - loss: 1.5074 - acc: 0.3331 -
val_loss: 1.5760 - val_acc: 0.3012

Epoch 00018: val_loss did not improve
Epoch
19/64
 - 61s - loss: 1.5207 - acc: 0.3129 - val_loss: 1.5836 - val_acc: 0.3129
Epoch 00019: val_loss did not improve
Epoch 20/64
 - 61s - loss: 1.4679 - acc:
0.3586 - val_loss: 1.6996 - val_acc: 0.3143

Epoch 00020: val_loss did not
improve
Epoch 21/64
 - 61s - loss: 1.4577 - acc: 0.3600 - val_loss: 3.1463 -
val_acc: 0.2281

Epoch 00021: val_loss did not improve
Epoch 22/64
 - 61s -
loss: 1.4130 - acc: 0.3878 - val_loss: 1.6996 - val_acc: 0.3070

Epoch 00022:
val_loss did not improve
Epoch 23/64
 - 61s - loss: 1.3847 - acc: 0.4067 -
val_loss: 1.9483 - val_acc: 0.3216

Epoch 00023: val_loss did not improve
Epoch
24/64
 - 61s - loss: 1.3356 - acc: 0.4379 - val_loss: 1.5215 - val_acc: 0.3450
Epoch 00024: val_loss improved from 1.56757 to 1.52145, saving model to
neck_ResNet50_weights.h5
Epoch 25/64
 - 61s - loss: 1.2837 - acc: 0.4535 -
val_loss: 1.8419 - val_acc: 0.3670

Epoch 00025: val_loss did not improve
Epoch
26/64
KeyboardInterrupt

```{.python .input}

- 训练类型：neck
- 前提条件：无
- 训练参数和模型
- keyWords resnet152 ,lr=0.01
```

from keras.layers import *
from keras.models import *
from keras.callbacks
import *
from keras.optimizers import *
from keras.applications import *
from
keras.regularizers import *
from keras.applications.inception_v3 import
preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from
keras.callbacks import ReduceLROnPlateau
from keras.optimizers import RMSprop
import keras

from resnet152_keras import *
cnn_model=resnet152_model(weights_path='resnet152_weights_tf.h5')

inputs =
Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input,
name='preprocessing')(x)
x = cnn_model(x)
#x = GlobalAveragePooling2D()(x)
x =
Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)
model = Model(inputs, x)

X_train, X_valid, y_train, y_valid =
train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape,
y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[0], loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}_resnet152_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}_Xception_new_weights.h5'.format(prefix_cls))

try:
#
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果：收敛太差，手动关停
```

Epoch 1/64
 - 194s - loss: 1.9761 - acc: 0.2536 - val_loss: 11.8030 - val_acc:
0.2515

Epoch 00001: val_loss improved from inf to 11.80303, saving model to
neck_resnet152_weights.h5
Epoch 2/64
 - 152s - loss: 2.0343 - acc: 0.2524 -
val_loss: 8.3078 - val_acc: 0.2135

Epoch 00002: val_loss improved from 11.80303
to 8.30782, saving model to neck_resnet152_weights.h5
Epoch 3/64
 - 152s - loss:
2.0425 - acc: 0.2492 - val_loss: 1.6767 - val_acc: 0.2500

Epoch 00003: val_loss
improved from 8.30782 to 1.67671, saving model to neck_resnet152_weights.h5
Epoch 4/64
 - 152s - loss: 1.9510 - acc: 0.2534 - val_loss: 1.5862 - val_acc:
0.2529

Epoch 00004: val_loss improved from 1.67671 to 1.58622, saving model to
neck_resnet152_weights.h5
Epoch 5/64
 - 152s - loss: 1.9679 - acc: 0.2510 -
val_loss: 12.8573 - val_acc: 0.1988

Epoch 00005: val_loss did not improve
Epoch
6/64
 - 152s - loss: 1.9712 - acc: 0.2462 - val_loss: 12.9840 - val_acc: 0.1944
Epoch 00006: val_loss did not improve
Epoch 7/64
 - 152s - loss: 1.9740 - acc:
0.2488 - val_loss: 9.5787 - val_acc: 0.2032

Epoch 00007: val_loss did not
improve
Epoch 8/64
 - 152s - loss: 1.9276 - acc: 0.2528 - val_loss: 2.1995 -
val_acc: 0.2500

Epoch 00008: val_loss did not improve
Epoch 9/64
 - 152s -
loss: 1.9622 - acc: 0.2558 - val_loss: 1.6800 - val_acc: 0.2573

Epoch 00009:
val_loss did not improve
Epoch 10/64
 - 152s - loss: 1.9360 - acc: 0.2542 -
val_loss: 1.5874 - val_acc: 0.2529

Epoch 00010: val_loss did not improve
Epoch
11/64
 - 152s - loss: 1.8996 - acc: 0.2570 - val_loss: 1.5875 - val_acc: 0.2529
Epoch 00011: val_loss did not improve
Epoch 12/64
 - 152s - loss: 1.9270 - acc:
0.2480 - val_loss: 1.5869 - val_acc: 0.2529

Epoch 00012: val_loss did not
improve
Epoch 13/64
KeyboardInterrupt

```{.python .input}



- 训练类型：neck
- 前提条件：无
- 训练参数和模型
- keyWords resnet152 ,lr=0.001
```

from keras.layers import *
from keras.models import *
from keras.callbacks
import *
from keras.optimizers import *
from keras.applications import *
from
keras.regularizers import *
from keras.applications.inception_v3 import
preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from
keras.callbacks import ReduceLROnPlateau
from keras.optimizers import RMSprop
import keras

from resnet152_keras import *
cnn_model=resnet152_model(weights_path='resnet152_weights_tf.h5')

inputs =
Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input,
name='preprocessing')(x)
x = cnn_model(x)
#x = GlobalAveragePooling2D()(x)
x =
Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)
model = Model(inputs, x)

X_train, X_valid, y_train, y_valid =
train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape,
y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[0], loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}_resnet152_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}_Xception_new_weights.h5'.format(prefix_cls))

try:
#
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果：收敛太差，手动关停

```

Epoch 1/64
 - 200s - loss: 1.9833 - acc: 0.2414 - val_loss: 12.9840 - val_acc:
0.1944

Epoch 00001: val_loss improved from inf to 12.98402, saving model to
neck_resnet152_weights.h5
Epoch 2/64
 - 151s - loss: 1.9205 - acc: 0.2498 -
val_loss: 11.6814 - val_acc: 0.2529

Epoch 00002: val_loss improved from
12.98402 to 11.68139, saving model to neck_resnet152_weights.h5
Epoch 3/64
 -
152s - loss: 1.9301 - acc: 0.2534 - val_loss: 2.5365 - val_acc: 0.2442

Epoch
00003: val_loss improved from 11.68139 to 2.53648, saving model to
neck_resnet152_weights.h5
Epoch 4/64
 - 152s - loss: 1.9333 - acc: 0.2542 -
val_loss: 2.0981 - val_acc: 0.2485

Epoch 00004: val_loss improved from 2.53648
to 2.09809, saving model to neck_resnet152_weights.h5
Epoch 5/64
 - 152s - loss:
2.0068 - acc: 0.2496 - val_loss: 10.3627 - val_acc: 0.2529

Epoch 00005:
val_loss did not improve
Epoch 6/64
 - 152s - loss: 1.9200 - acc: 0.2522 -
val_loss: 2.5750 - val_acc: 0.2383

Epoch 00006: val_loss did not improve
Epoch
7/64
 - 152s - loss: 1.9118 - acc: 0.2534 - val_loss: 3.1874 - val_acc: 0.2529
Epoch 00007: val_loss did not improve
Epoch 8/64
 - 152s - loss: 1.8478 - acc:
0.2532 - val_loss: 13.8323 - val_acc: 0.1418

Epoch 00008: val_loss did not
improve
Epoch 9/64
 - 152s - loss: 1.7197 - acc: 0.2484 - val_loss: 2.7001 -
val_acc: 0.2456

Epoch 00009: val_loss did not improve
Epoch 10/64

Epoch 00010:
val_loss improved from 2.09809 to 1.58789, saving model to
neck_resnet152_weights.h5
Epoch 11/64
 - 151s - loss: 1.6680 - acc: 0.2462 -
val_loss: 1.5884 - val_acc: 0.2544

Epoch 00011: val_loss did not improve
Epoch
12/64

```{.python .input}



>>> 18.4.1

- 训练类型：neck
- 前提条件：Xception ,lr=0.001 ,augmention ↑ 19+20 epoch
- 训练参数和模型
- keyWords Xception ,lr=0.0001 augmention ↑
=>neck_Xception_tune9_0330a.csv
```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.0001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[0], loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.3, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.3,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.3,
fill_mode = 'nearest',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}1_Xception_new_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
model.load_weights('{0}_Xception_new_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 180s - loss: 0.2982 - acc: 0.8932 - val_loss: 0.7558 - val_acc:
0.7836

Epoch 00001: val_loss improved from inf to 0.75576, saving model to
neck1_Xception_new_weights.h5
Epoch 2/64
 - 181s - loss: 0.2840 - acc: 0.8986 -
val_loss: 0.6108 - val_acc: 0.8304

Epoch 00002: val_loss improved from 0.75576
to 0.61079, saving model to neck1_Xception_new_weights.h5
Epoch 3/64
 - 181s -
loss: 0.3008 - acc: 0.8855 - val_loss: 1.3094 - val_acc: 0.6959

Epoch 00003:
val_loss did not improve
Epoch 4/64
 - 180s - loss: 0.2720 - acc: 0.8973 -
val_loss: 0.9297 - val_acc: 0.7880

Epoch 00004: val_loss did not improve
Epoch
5/64
 - 181s - loss: 0.2604 - acc: 0.9065 - val_loss: 1.5013 - val_acc: 0.6871
Epoch 00005: val_loss did not improve
Epoch 6/64
 - 181s - loss: 0.2595 - acc:
0.9018 - val_loss: 0.9853 - val_acc: 0.7807

Epoch 00006: val_loss did not
improve
Epoch 7/64
 - 181s - loss: 0.2322 - acc: 0.9164 - val_loss: 0.6766 -
val_acc: 0.8333

Epoch 00007: val_loss did not improve
Epoch 8/64
 - 181s -
loss: 0.2506 - acc: 0.9107 - val_loss: 1.0015 - val_acc: 0.7529

Epoch 00008:
val_loss did not improve
Epoch 9/64
 - 181s - loss: 0.2377 - acc: 0.9105 -
val_loss: 1.1833 - val_acc: 0.7705

Epoch 00009: val_loss did not improve
Epoch
10/64
 - 182s - loss: 0.2412 - acc: 0.9145 - val_loss: 1.0373 - val_acc: 0.7588
Epoch 00010: val_loss did not improve
Epoch 11/64
 - 180s - loss: 0.2247 - acc:
0.9213 - val_loss: 0.6797 - val_acc: 0.8143

Epoch 00011: val_loss did not
improve
Epoch 12/64
 - 181s - loss: 0.2045 - acc: 0.9277 - val_loss: 0.9464 -
val_acc: 0.7807

Epoch 00012: val_loss did not improve
Epoch 13/64
 - 181s -
loss: 0.2249 - acc: 0.9197 - val_loss: 0.8386 - val_acc: 0.8012

Epoch 00013:
val_loss did not improve
Epoch 14/64
 - 181s - loss: 0.2142 - acc: 0.9219 -
val_loss: 0.8242 - val_acc: 0.7822

Epoch 00014: val_loss did not improve
Epoch
15/64
 - 182s - loss: 0.2359 - acc: 0.9151 - val_loss: 0.8859 - val_acc: 0.7690
Epoch 00015: val_loss did not improve
Epoch 16/64
 - 181s - loss: 0.2094 - acc:
0.9211 - val_loss: 1.0975 - val_acc: 0.7456

Epoch 00016: val_loss did not
improve
Epoch 17/64

```{.python .input}

- 训练类型：neck
- 前提条件：无
- 训练参数和模型
- keyWords Xception ,lr=0.001 ,augmention ↑
```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[0], loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 32

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 20, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.3, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.3,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.3,
fill_mode = 'nearest',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}_Xception_new_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 181s - loss: 1.2331 - acc: 0.4927 - val_loss: 1.3214 - val_acc:
0.6199

Epoch 00001: val_loss improved from inf to 1.32135, saving model to
neck_Xception_new_weights.h5
Epoch 2/64
 - 178s - loss: 0.9147 - acc: 0.6534 -
val_loss: 1.2514 - val_acc: 0.5892

Epoch 00002: val_loss improved from 1.32135
to 1.25139, saving model to neck_Xception_new_weights.h5
Epoch 3/64
 - 178s -
loss: 0.7995 - acc: 0.7035 - val_loss: 0.9370 - val_acc: 0.6316

Epoch 00003:
val_loss improved from 1.25139 to 0.93699, saving model to
neck_Xception_new_weights.h5
Epoch 4/64
 - 178s - loss: 0.7311 - acc: 0.7244 -
val_loss: 0.8389 - val_acc: 0.7544

Epoch 00004: val_loss improved from 0.93699
to 0.83891, saving model to neck_Xception_new_weights.h5
Epoch 5/64
 - 178s -
loss: 0.6785 - acc: 0.7471 - val_loss: 0.6703 - val_acc: 0.7675

Epoch 00005:
val_loss improved from 0.83891 to 0.67026, saving model to
neck_Xception_new_weights.h5
Epoch 6/64
 - 178s - loss: 0.6572 - acc: 0.7491 -
val_loss: 0.7521 - val_acc: 0.7588

Epoch 00006: val_loss did not improve
Epoch
7/64
 - 179s - loss: 0.6012 - acc: 0.7792 - val_loss: 1.0472 - val_acc: 0.6652
Epoch 00007: val_loss did not improve
Epoch 8/64
 - 178s - loss: 0.5760 - acc:
0.7857 - val_loss: 1.2059 - val_acc: 0.6652

Epoch 00008: val_loss did not
improve
Epoch 9/64
 - 178s - loss: 0.5628 - acc: 0.7864 - val_loss: 0.8366 -
val_acc: 0.7383

Epoch 00009: val_loss did not improve
Epoch 10/64
 - 178s -
loss: 0.5139 - acc: 0.8101 - val_loss: 0.8602 - val_acc: 0.7251

Epoch 00010:
val_loss did not improve
Epoch 11/64
 - 178s - loss: 0.5168 - acc: 0.8070 -
val_loss: 1.0340 - val_acc: 0.7193

Epoch 00011: val_loss did not improve
Epoch
12/64
 - 178s - loss: 0.5070 - acc: 0.8062 - val_loss: 0.7757 - val_acc: 0.7763
Epoch 00012: val_loss did not improve
Epoch 13/64
 - 178s - loss: 0.4472 - acc:
0.8296 - val_loss: 0.7195 - val_acc: 0.7544

Epoch 00013: val_loss did not
improve
Epoch 14/64
 - 177s - loss: 0.4510 - acc: 0.8351 - val_loss: 2.2454 -
val_acc: 0.5219

Epoch 00014: val_loss did not improve
Epoch 15/64
 - 179s -
loss: 0.4219 - acc: 0.8417 - val_loss: 0.7975 - val_acc: 0.7865

Epoch 00015:
val_loss did not improve
Epoch 16/64
 - 177s - loss: 0.3938 - acc: 0.8561 -
val_loss: 0.6657 - val_acc: 0.7895

Epoch 00016: val_loss improved from 0.67026
to 0.66569, saving model to neck_Xception_new_weights.h5
Epoch 17/64
 - 178s -
loss: 0.4241 - acc: 0.8444 - val_loss: 0.6783 - val_acc: 0.7997

Epoch 00017:
val_loss did not improve
Epoch 18/64
 - 177s - loss: 0.3655 - acc: 0.8604 -
val_loss: 1.1978 - val_acc: 0.6696

Epoch 00018: val_loss did not improve
Epoch
19/64
 - 178s - loss: 0.3558 - acc: 0.8668 - val_loss: 0.7299 - val_acc: 0.8041
Epoch 00019: val_loss did not improve
Epoch 20/64

Epoch 1/64
 - 178s - loss:
0.4083 - acc: 0.8425 - val_loss: 1.0037 - val_acc: 0.7237

Epoch 00001: val_loss
improved from inf to 1.00371, saving model to neck_Xception_new_weights.h5
Epoch
2/64
 - 172s - loss: 0.3774 - acc: 0.8589 - val_loss: 1.4000 - val_acc: 0.7339
Epoch 00002: val_loss did not improve
Epoch 3/64
 - 172s - loss: 0.3810 - acc:
0.8606 - val_loss: 0.7281 - val_acc: 0.8070

Epoch 00003: val_loss improved from
1.00371 to 0.72807, saving model to neck_Xception_new_weights.h5
Epoch 4/64
 -
172s - loss: 0.3569 - acc: 0.8697 - val_loss: 0.8882 - val_acc: 0.7354

Epoch
00004: val_loss did not improve
Epoch 5/64
 - 172s - loss: 0.3371 - acc: 0.8714
- val_loss: 0.7348 - val_acc: 0.7953

Epoch 00005: val_loss did not improve
Epoch 6/64
 - 171s - loss: 0.3385 - acc: 0.8777 - val_loss: 0.9213 - val_acc:
0.7763

Epoch 00006: val_loss did not improve
Epoch 7/64
 - 171s - loss: 0.3208
- acc: 0.8806 - val_loss: 0.8312 - val_acc: 0.7851

Epoch 00007: val_loss did
not improve
Epoch 8/64
 - 171s - loss: 0.3205 - acc: 0.8853 - val_loss: 0.8834 -
val_acc: 0.7909

Epoch 00008: val_loss did not improve
Epoch 9/64
 - 171s -
loss: 0.2959 - acc: 0.8908 - val_loss: 0.7002 - val_acc: 0.7953

Epoch 00009:
val_loss improved from 0.72807 to 0.70019, saving model to
neck_Xception_new_weights.h5
Epoch 10/64
 - 172s - loss: 0.2763 - acc: 0.8976 -
val_loss: 0.9959 - val_acc: 0.7602

Epoch 00010: val_loss did not improve
Epoch
11/64
 - 172s - loss: 0.2656 - acc: 0.9028 - val_loss: 0.8655 - val_acc: 0.7997
Epoch 00011: val_loss did not improve
Epoch 12/64
 - 171s - loss: 0.2629 - acc:
0.9029 - val_loss: 0.7759 - val_acc: 0.8099

Epoch 00012: val_loss did not
improve
Epoch 13/64
 - 171s - loss: 0.2844 - acc: 0.8986 - val_loss: 0.9893 -
val_acc: 0.7427

Epoch 00013: val_loss did not improve
Epoch 14/64
 - 171s -
loss: 0.2475 - acc: 0.9105 - val_loss: 1.1105 - val_acc: 0.7822

Epoch 00014:
val_loss did not improve
Epoch 15/64
 - 171s - loss: 0.2542 - acc: 0.9055 -
val_loss: 1.2127 - val_acc: 0.7295

Epoch 00015: val_loss did not improve
Epoch
16/64
 - 171s - loss: 0.2576 - acc: 0.9026 - val_loss: 1.1651 - val_acc: 0.7690
Epoch 00016: val_loss did not improve
Epoch 17/64
 - 171s - loss: 0.2525 - acc:
0.9071 - val_loss: 0.9610 - val_acc: 0.7836

Epoch 00017: val_loss did not
improve
Epoch 18/64
 - 171s - loss: 0.2234 - acc: 0.9175 - val_loss: 0.8501 -
val_acc: 0.7880

Epoch 00018: val_loss did not improve
Epoch 19/64

```{.python .input}




>>> 18.3.30
>>>

- 训练类型：neck
- 前提条件：18epoch Xception lr=0.001+
32epoch Xception lr=0.0001
- 训练参数和模型
- keyWords Xception ,lr=0.0001 ,冻结block13层之前

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]

#
Compile the model


model.compile(optimizer=list_opt[0],
loss='categorical_crossentropy', metrics=['accuracy'])
model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

#冻结block9层之前
set_trainable = False
for layer in cnn_model.layers:
    if layer.name ==
'block13_sepconv1':
        set_trainable = True
    if set_trainable:
layer.trainable = True
    else:
        layer.trainable = False

epochs = 64
batch_size = 16


datagen = ImageDataGenerator(
        featurewise_center
=False ,# set input mean to 0 over the dataset
        samplewise_center = False
,# set each sample mean to 0
        featurewise_std_normalization = False, #
divide inputs by std of the dataset
        samplewise_std_normalization =
False, # divide each input by its std
        zca_whitening = False, # apply ZCA
whitening
        rotation_range = 10, # randomly rotate images in the range
(degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
width_shift_range = 0.2, # randomly shift images horizontally (fraction of total
width)
        height_shift_range = 0.2, # randomly shift images vertivally
(fraction of total heigth)
        horizontal_flip = True, # randomly flip
images
        vertical_flip = False,
        shear_range = 0.1,
fill_mode =
'constant',
        cval = 0)

datagen.fit(X_train,seed=123)
prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}4_Xception_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 188s - loss: 0.2977 - acc: 0.8888 - val_loss: 0.5512 - val_acc:
0.8143

Epoch 00001: val_loss improved from inf to 0.55117, saving model to
neck4_Xception_weights.h5
Epoch 2/64
 - 171s - loss: 0.2954 - acc: 0.8908 -
val_loss: 0.5619 - val_acc: 0.8202

Epoch 00002: val_loss did not improve
Epoch
3/64
 - 171s - loss: 0.2475 - acc: 0.9103 - val_loss: 0.6245 - val_acc: 0.8173
Epoch 00003: val_loss did not improve
Epoch 4/64
 - 171s - loss: 0.2447 - acc:
0.9097 - val_loss: 0.6191 - val_acc: 0.8289

Epoch 00004: val_loss did not
improve
Epoch 5/64
 - 171s - loss: 0.2293 - acc: 0.9169 - val_loss: 0.6214 -
val_acc: 0.8129

Epoch 00005: val_loss did not improve
Epoch 6/64
 - 172s -
loss: 0.2201 - acc: 0.9203 - val_loss: 0.6382 - val_acc: 0.8231

Epoch 00006:
val_loss did not improve
Epoch 7/64
 - 171s - loss: 0.2064 - acc: 0.9231 -
val_loss: 0.6774 - val_acc: 0.8202

Epoch 00007: val_loss did not improve
Epoch
8/64
 - 171s - loss: 0.1934 - acc: 0.9277 - val_loss: 0.6249 - val_acc: 0.8158
Epoch 00008: val_loss did not improve
Epoch 9/64
 - 172s - loss: 0.1685 - acc:
0.9367 - val_loss: 0.6755 - val_acc: 0.8363

Epoch 00009: val_loss did not
improve
Epoch 10/64
 - 171s - loss: 0.1767 - acc: 0.9385 - val_loss: 0.6529 -
val_acc: 0.8246

Epoch 00010: val_loss did not improve
Epoch 11/64
 - 171s -
loss: 0.1621 - acc: 0.9431 - val_loss: 0.6381 - val_acc: 0.8406

Epoch 00011:
val_loss did not improve
Epoch 12/64
KeyboardInterrupt

Epoch 1/64
 - 183s -
loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 -
val_acc: 0.0000e+00
Epoch 00001: val_loss improved from inf to 0.00000, saving
model to
neck4_Xception_weights.h5
Epoch 2/64
 - 169s - loss: 0.0000e+00 - acc:
0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00

```{.python .input}

- 训练类型：neck
- 前提条件：18epoch Xception lr=0.001+
32epoch Xception lr=0.0001
- 训练参数和模型
- keyWords Xception ,lr=0.0001 ,冻结block13层之前

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]

#
Compile the model


model.compile(optimizer=list_opt[0],
loss='categorical_crossentropy', metrics=['accuracy'])
model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

#冻结block9层之前
set_trainable = False
for layer in cnn_model.layers:
    if layer.name ==
'block13_sepconv1':
        set_trainable = True
    if set_trainable:
layer.trainable = True
    else:
        layer.trainable = False

epochs = 64
batch_size = 16


datagen = ImageDataGenerator(
        featurewise_center
=False ,# set input mean to 0 over the dataset
        samplewise_center = False
,# set each sample mean to 0
        featurewise_std_normalization = False, #
divide inputs by std of the dataset
        samplewise_std_normalization =
False, # divide each input by its std
        zca_whitening = False, # apply ZCA
whitening
        rotation_range = 10, # randomly rotate images in the range
(degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
width_shift_range = 0.2, # randomly shift images horizontally (fraction of total
width)
        height_shift_range = 0.2, # randomly shift images vertivally
(fraction of total heigth)
        horizontal_flip = True, # randomly flip
images
        vertical_flip = False,
        shear_range = 0.1,
fill_mode =
'constant',
        cval = 0)

datagen.fit(X_train,seed=123)
prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}4_Xception_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 183s - loss: 0.0000e+00 - acc: 0.0000e+00 - val_loss: 0.0000e+00 -
val_acc: 0.0000e+00

Epoch 00001: val_loss improved from inf to 0.00000, saving
model to neck4_Xception_weights.h5
Epoch 2/64
 - 169s - loss: 0.0000e+00 - acc:
0.0000e+00 - val_loss: 0.0000e+00 - val_acc: 0.0000e+00

```{.python .input}


- 训练类型：neck
- 前提条件：18epoch Xception lr=0.001+
32epoch Xception lr=0.0001
- 训练参数和模型
- keyWords Xception ,lr=0.0001 ,冻结block9层之前

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]

#
Compile the model


model.compile(optimizer=list_opt[0],
loss='categorical_crossentropy', metrics=['accuracy'])
model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

#冻结block9层之前
set_trainable = False
for layer in cnn_model.layers:
    if layer.name ==
'block9_sepconv1':
        set_trainable = True
    if set_trainable:
layer.trainable = True
    else:
        layer.trainable = False

epochs = 64
batch_size = 16


datagen = ImageDataGenerator(
        featurewise_center
=False ,# set input mean to 0 over the dataset
        samplewise_center = False
,# set each sample mean to 0
        featurewise_std_normalization = False, #
divide inputs by std of the dataset
        samplewise_std_normalization =
False, # divide each input by its std
        zca_whitening = False, # apply ZCA
whitening
        rotation_range = 10, # randomly rotate images in the range
(degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
width_shift_range = 0.2, # randomly shift images horizontally (fraction of total
width)
        height_shift_range = 0.2, # randomly shift images vertivally
(fraction of total heigth)
        horizontal_flip = True, # randomly flip
images
        vertical_flip = False,
        shear_range = 0.1,
fill_mode =
'constant',
        cval = 0)

datagen.fit(X_train,seed=123)
prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}3_Xception_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 180s - loss: 0.2930 - acc: 0.8924 - val_loss: 0.5292 - val_acc:
0.8187

Epoch 00001: val_loss improved from inf to 0.52919, saving model to
neck3_Xception_weights.h5
Epoch 2/64
 - 169s - loss: 0.2845 - acc: 0.9000 -
val_loss: 0.5762 - val_acc: 0.8216

Epoch 00002: val_loss did not improve
Epoch
3/64
 - 169s - loss: 0.2525 - acc: 0.9040 - val_loss: 0.5209 - val_acc: 0.8348
Epoch 00003: val_loss improved from 0.52919 to 0.52089, saving model to
neck3_Xception_weights.h5
Epoch 4/64
 - 169s - loss: 0.2463 - acc: 0.9105 -
val_loss: 0.5563 - val_acc: 0.8275

Epoch 00004: val_loss did not improve
Epoch
5/64
 - 169s - loss: 0.2343 - acc: 0.9139 - val_loss: 0.6264 - val_acc: 0.8216
Epoch 00005: val_loss did not improve
Epoch 6/64
 - 169s - loss: 0.2039 - acc:
0.9257 - val_loss: 0.6371 - val_acc: 0.8246

Epoch 00006: val_loss did not
improve
Epoch 7/64
 - 169s - loss: 0.1982 - acc: 0.9291 - val_loss: 0.5582 -
val_acc: 0.8480

Epoch 00007: val_loss did not improve
Epoch 8/64
 - 169s -
loss: 0.1798 - acc: 0.9309 - val_loss: 0.6258 - val_acc: 0.8363

Epoch 00008:
val_loss did not improve
Epoch 9/64
 - 169s - loss: 0.1788 - acc: 0.9361 -
val_loss: 0.6369 - val_acc: 0.8377

Epoch 00009: val_loss did not improve
Epoch
10/64
 - 169s - loss: 0.1816 - acc: 0.9325 - val_loss: 0.6329 - val_acc: 0.8377
Epoch 00010: val_loss did not improve
Epoch 11/64
 - 169s - loss: 0.1610 - acc:
0.9411 - val_loss: 0.6878 - val_acc: 0.8319

Epoch 00011: val_loss did not
improve
Epoch 12/64
 - 169s - loss: 0.1542 - acc: 0.9413 - val_loss: 0.6385 -
val_acc: 0.8480

Epoch 00012: val_loss did not improve
Epoch 13/64
KeyboardInterrupt

```{.python .input}





- 训练类型：neck
- 前提条件：18epoch Xception lr=0.001+
32epoch Xception lr=0.0001
- 训练参数和模型
- keyWords Xception ,lr=0.0001 ,冻结block4层之前

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]

#
Compile the model


model.compile(optimizer=list_opt[0],
loss='categorical_crossentropy', metrics=['accuracy'])
model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

#冻结block4层之前
set_trainable = False
for layer in cnn_model.layers:
    if layer.name ==
'block4_sepconv1':
        set_trainable = True
    if set_trainable:
layer.trainable = True
    else:
        layer.trainable = False

epochs = 64
batch_size = 16


datagen = ImageDataGenerator(
        featurewise_center
=False ,# set input mean to 0 over the dataset
        samplewise_center = False
,# set each sample mean to 0
        featurewise_std_normalization = False, #
divide inputs by std of the dataset
        samplewise_std_normalization =
False, # divide each input by its std
        zca_whitening = False, # apply ZCA
whitening
        rotation_range = 10, # randomly rotate images in the range
(degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
width_shift_range = 0.2, # randomly shift images horizontally (fraction of total
width)
        height_shift_range = 0.2, # randomly shift images vertivally
(fraction of total heigth)
        horizontal_flip = True, # randomly flip
images
        vertical_flip = False,
        shear_range = 0.1,
fill_mode =
'constant',
        cval = 0)

datagen.fit(X_train,seed=123)
prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}2_Xception_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 182s - loss: 0.2961 - acc: 0.8926 - val_loss: 0.5343 - val_acc:
0.8158

Epoch 00001: val_loss improved from inf to 0.53429, saving model to
neck2_Xception_weights.h5
Epoch 2/64
 - 172s - loss: 0.2814 - acc: 0.8982 -
val_loss: 0.5287 - val_acc: 0.8129

Epoch 00002: val_loss improved from 0.53429
to 0.52869, saving model to neck2_Xception_weights.h5
Epoch 3/64
 - 172s - loss:
0.2597 - acc: 0.9070 - val_loss: 0.5493 - val_acc: 0.8231

Epoch 00003: val_loss
did not improve
Epoch 4/64
 - 172s - loss: 0.2322 - acc: 0.9143 - val_loss:
0.5723 - val_acc: 0.8216

Epoch 00004: val_loss did not improve
Epoch 5/64
 -
172s - loss: 0.2275 - acc: 0.9135 - val_loss: 0.5503 - val_acc: 0.8348

Epoch
00005: val_loss did not improve
Epoch 6/64
 - 172s - loss: 0.2222 - acc: 0.9181
- val_loss: 0.6120 - val_acc: 0.8231

Epoch 00006: val_loss did not improve
Epoch 7/64
 - 172s - loss: 0.1892 - acc: 0.9309 - val_loss: 0.6435 - val_acc:
0.8275

Epoch 00007: val_loss did not improve
Epoch 8/64
 - 171s - loss: 0.1996
- acc: 0.9273 - val_loss: 0.6454 - val_acc: 0.8158

Epoch 00008: val_loss did
not improve
Epoch 9/64
 - 172s - loss: 0.1823 - acc: 0.9349 - val_loss: 0.7268 -
val_acc: 0.8114

Epoch 00009: val_loss did not improve
Epoch 10/64
 - 172s -
loss: 0.1663 - acc: 0.9413 - val_loss: 0.6282 - val_acc: 0.8275

Epoch 00010:
val_loss did not improve
Epoch 11/64
 - 172s - loss: 0.1552 - acc: 0.9459 -
val_loss: 0.6095 - val_acc: 0.8377

Epoch 00011: val_loss did not improve
Epoch
12/64
 - 172s - loss: 0.1603 - acc: 0.9393 - val_loss: 0.6263 - val_acc: 0.8465
Epoch 00012: val_loss did not improve
Epoch 13/64
 - 172s - loss: 0.1538 - acc:
0.9431 - val_loss: 0.6067 - val_acc: 0.8304

Epoch 00013: val_loss did not
improve
Epoch 14/64
 - 172s - loss: 0.1340 - acc: 0.9517 - val_loss: 0.7526 -
val_acc: 0.8143

Epoch 00014: val_loss did not improve
Epoch 15/64
 - 171s -
loss: 0.1381 - acc: 0.9525 - val_loss: 0.6861 - val_acc: 0.8377

Epoch 00015:
val_loss did not improve
Epoch 16/64
 - 171s - loss: 0.1326 - acc: 0.9507 -
val_loss: 0.6884 - val_acc: 0.8246

Epoch 00016: val_loss did not improve
Epoch
17/64
 - 172s - loss: 0.1321 - acc: 0.9513 - val_loss: 0.6875 - val_acc: 0.8319
Epoch 00017: val_loss did not improve
Epoch 18/64
 - 172s - loss: 0.1277 - acc:
0.9505 - val_loss: 0.7002 - val_acc: 0.8319

Epoch 00018: val_loss did not
improve
Epoch 19/64
 - 172s - loss: 0.1157 - acc: 0.9603 - val_loss: 0.7256 -
val_acc: 0.8333

Epoch 00019: val_loss did not improve
Epoch 20/64
 - 172s -
loss: 0.1069 - acc: 0.9597 - val_loss: 0.6894 - val_acc: 0.8363

Epoch 00020:
val_loss did not improve
Epoch 21/64
 - 171s - loss: 0.1029 - acc: 0.9625 -
val_loss: 0.7025 - val_acc: 0.8275

Epoch 00021: val_loss did not improve
Epoch
22/64
KeyboardInterrupt

```{.python .input}


- 训练类型：neck
- 前提条件：无
- 训练参数和模型
- keyWords Xception ,lr=0.0001 ,冻结block14层之前

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

#冻结block14层之前
set_trainable = False
for layer in
cnn_model.layers:
    if layer.name == 'block14_sepconv1':
        set_trainable
= True
    if set_trainable:
        layer.trainable = True
    else:
layer.trainable = False


inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)


X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[0], loss='categorical_crossentropy',
metrics=['accuracy'])


epochs = 64
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}_Xception_layer14_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}1_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
 - 88s - loss: 1.2711 - acc: 0.4880 - val_loss: 1.1864 - val_acc:
0.5468

Epoch 00001: val_loss improved from inf to 1.18636, saving model to
neck_Xception_layer14_weights.h5
Epoch 2/64
 - 87s - loss: 1.0661 - acc: 0.5803
- val_loss: 1.2171 - val_acc: 0.5322

Epoch 00002: val_loss did not improve
Epoch 3/64
 - 87s - loss: 0.9769 - acc: 0.6084 - val_loss: 1.1552 - val_acc:
0.5424

Epoch 00003: val_loss improved from 1.18636 to 1.15517, saving model to
neck_Xception_layer14_weights.h5
Epoch 4/64
 - 87s - loss: 0.8738 - acc: 0.6575
- val_loss: 1.3944 - val_acc: 0.5146

Epoch 00004: val_loss did not improve
Epoch 5/64
 - 87s - loss: 0.7948 - acc: 0.6929 - val_loss: 1.2021 - val_acc:
0.5833

Epoch 00005: val_loss did not improve
Epoch 6/64
 - 87s - loss: 0.7139 -
acc: 0.7382 - val_loss: 1.4360 - val_acc: 0.5336

Epoch 00006: val_loss did not
improve
Epoch 7/64
 - 87s - loss: 0.6404 - acc: 0.7578 - val_loss: 1.3907 -
val_acc: 0.5556

Epoch 00007: val_loss did not improve
Epoch 8/64
 - 87s - loss:
0.5584 - acc: 0.7897 - val_loss: 1.6418 - val_acc: 0.5029

Epoch 00008: val_loss
did not improve
Epoch 9/64
 - 87s - loss: 0.5167 - acc: 0.8039 - val_loss:
1.3696 - val_acc: 0.5468

Epoch 00009: val_loss did not improve
Epoch 10/64
 -
86s - loss: 0.4591 - acc: 0.8303 - val_loss: 1.5824 - val_acc: 0.5512

Epoch
00010: val_loss did not improve
Epoch 11/64
 - 86s - loss: 0.4116 - acc: 0.8532
- val_loss: 2.0368 - val_acc: 0.5249

Epoch 00011: val_loss did not improve
Epoch 12/64
 - 86s - loss: 0.3467 - acc: 0.8762 - val_loss: 1.9657 - val_acc:
0.5585

Epoch 00012: val_loss did not improve
Epoch 13/64

```{.python .input}




>>> 18.3.29

- 训练类型：coat
- 前提条件：DenseNet201 20epoch lr=0.001 +
DenseNet201 20epoch lr=0.0001
- 训练参数和模型
- keyWords DenseNet201 lr=0.00001

```

from keras.applications.densenet import DenseNet201

cnn_model =
DenseNet201(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam =
Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center = False, # set input mean to 0
over the dataset
        samplewise_center = False, # set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift
images horizontally (fraction of total width)
        height_shift_range =
0.075, # randomly shift images vertivally (fraction of total heigth)
horizontal_flip = False, # randomly flip images
        vertical_flip = False,
shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)
#datagen.fit(X_train,seed=123)
datagen.fit(X_train,seed=123)

prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}2_DenseNet201_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
model.load_weights('{0}1_DenseNet201_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}

- 训练结果

```

Epoch 1/64
Epoch 00001: val_loss improved from inf to 0.46924, saving model to
coat2_DenseNet201_weights.h5
 - 367s - loss: 0.1742 - acc: 0.9410 - val_loss:
0.4692 - val_acc: 0.8550
Epoch 2/64
Epoch 00002: val_loss did not improve
 -
317s - loss: 0.1647 - acc: 0.9447 - val_loss: 0.4733 - val_acc: 0.8536

```{.python .input}


- 训练类型：coat
- 前提条件：DenseNet201 20epoch lr=0.001
- 训练参数和模型
- keyWords DenseNet201 lr=0.0001

```

from keras.applications.densenet import DenseNet201

cnn_model =
DenseNet201(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam =
Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center = False, # set input mean to 0
over the dataset
        samplewise_center = False, # set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift
images horizontally (fraction of total width)
        height_shift_range =
0.075, # randomly shift images vertivally (fraction of total heigth)
horizontal_flip = False, # randomly flip images
        vertical_flip = False,
shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)
#datagen.fit(X_train,seed=123)
datagen.fit(X_train,seed=123)

prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}1_DenseNet201_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
model.load_weights('{0}_DenseNet201_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}

训练结果

```

Epoch 1/64
Epoch 00001: val_loss improved from inf to 0.46759, saving model to
coat1_DenseNet201_weights.h5
 - 365s - loss: 0.2402 - acc: 0.9172 - val_loss:
0.4676 - val_acc: 0.8536
Epoch 2/64
Epoch 00002: val_loss did not improve
 -
317s - loss: 0.1739 - acc: 0.9401 - val_loss: 0.5312 - val_acc: 0.8381
Epoch
3/64
Epoch 00003: val_loss did not improve
 - 317s - loss: 0.1404 - acc: 0.9495
- val_loss: 0.5264 - val_acc: 0.8477
Epoch 4/64
Epoch 00004: val_loss did not
improve
 - 317s - loss: 0.1181 - acc: 0.9573 - val_loss: 0.5793 - val_acc:
0.8447
Epoch 5/64
Epoch 00005: val_loss did not improve
 - 317s - loss: 0.1039 -
acc: 0.9629 - val_loss: 0.5915 - val_acc: 0.8440
Epoch 6/64
Epoch 00006:
val_loss did not improve
 - 316s - loss: 0.0941 - acc: 0.9669 - val_loss: 0.6764
- val_acc: 0.8381
Epoch 7/64
Epoch 00007: val_loss did not improve
 - 317s -
loss: 0.0803 - acc: 0.9723 - val_loss: 0.6494 - val_acc: 0.8492
Epoch 8/64
Epoch
00008: val_loss did not improve
 - 316s - loss: 0.0742 - acc: 0.9730 - val_loss:
0.6884 - val_acc: 0.8484
Epoch 9/64
Epoch 00009: val_loss did not improve
 -
317s - loss: 0.0704 - acc: 0.9755 - val_loss: 0.6615 - val_acc: 0.8477
Epoch
10/64
Epoch 00010: val_loss did not improve
 - 317s - loss: 0.0555 - acc: 0.9814
- val_loss: 0.6844 - val_acc: 0.8536
Epoch 11/64
Epoch 00011: val_loss did not
improve
 - 317s - loss: 0.0564 - acc: 0.9816 - val_loss: 0.7025 - val_acc:
0.8477
Epoch 12/64
Epoch 00012: val_loss did not improve
 - 316s - loss: 0.0549
- acc: 0.9805 - val_loss: 0.7019 - val_acc: 0.8528
Epoch 13/64
Epoch 00013:
val_loss did not improve
 - 315s - loss: 0.0492 - acc: 0.9823 - val_loss: 0.7581
- val_acc: 0.8389
Epoch 14/64
Epoch 00014: val_loss did not improve
 - 315s -
loss: 0.0463 - acc: 0.9834 - val_loss: 0.7598 - val_acc: 0.8330
Epoch 15/64
Epoch 00015: val_loss did not improve
 - 316s - loss: 0.0457 - acc: 0.9827 -
val_loss: 0.7503 - val_acc: 0.8389
Epoch 16/64
Epoch 00016: val_loss did not
improve
 - 315s - loss: 0.0450 - acc: 0.9850 - val_loss: 0.7712 - val_acc:
0.8425
Epoch 17/64
Epoch 00017: val_loss did not improve
 - 315s - loss: 0.0359
- acc: 0.9872 - val_loss: 0.8233 - val_acc: 0.8322
Epoch 18/64
Epoch 00018:
val_loss did not improve
 - 315s - loss: 0.0435 - acc: 0.9844 - val_loss: 0.7915
- val_acc: 0.8389
Epoch 19/64
Epoch 00019: val_loss did not improve
 - 315s -
loss: 0.0391 - acc: 0.9861 - val_loss: 0.7690 - val_acc: 0.8396
Epoch 20/64
Epoch 00020: val_loss did not improve
 - 315s - loss: 0.0303 - acc: 0.9893 -
val_loss: 0.8347 - val_acc: 0.8418
Epoch 21/64

```{.python .input}





- 训练类型：coat
- 前提条件：无
- 训练参数和模型
- keyWords DenseNet201 lr=0.001
```

from keras.applications.densenet import DenseNet201

cnn_model =
DenseNet201(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam =
Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center = False, # set input mean to 0
over the dataset
        samplewise_center = False, # set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift
images horizontally (fraction of total width)
        height_shift_range =
0.075, # randomly shift images vertivally (fraction of total heigth)
horizontal_flip = False, # randomly flip images
        vertical_flip = False,
shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)
#datagen.fit(X_train,seed=123)
datagen.fit(X_train,seed=123)

prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}_DenseNet201_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}_DenseNet201_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
Epoch 00001: val_loss improved from inf to 1.83712, saving model to
coat_DenseNet201_weights.h5
 - 367s - loss: 1.6332 - acc: 0.3935 - val_loss:
1.8371 - val_acc: 0.3996
Epoch 2/64
Epoch 00002: val_loss improved from 1.83712
to 1.33373, saving model to coat_DenseNet201_weights.h5
 - 317s - loss: 1.1727 -
acc: 0.5521 - val_loss: 1.3337 - val_acc: 0.5541
Epoch 3/64
Epoch 00003:
val_loss did not improve
 - 316s - loss: 0.9999 - acc: 0.6278 - val_loss: 1.4445
- val_acc: 0.4753
Epoch 4/64
Epoch 00004: val_loss improved from 1.33373 to
0.82598, saving model to coat_DenseNet201_weights.h5
 - 317s - loss: 0.8839 -
acc: 0.6723 - val_loss: 0.8260 - val_acc: 0.6924
Epoch 5/64
Epoch 00005:
val_loss did not improve
 - 316s - loss: 0.7936 - acc: 0.7050 - val_loss: 0.9818
- val_acc: 0.6858
Epoch 6/64
Epoch 00006: val_loss improved from 0.82598 to
0.73433, saving model to coat_DenseNet201_weights.h5
 - 317s - loss: 0.7369 -
acc: 0.7267 - val_loss: 0.7343 - val_acc: 0.7344
Epoch 7/64
Epoch 00007:
val_loss improved from 0.73433 to 0.70912, saving model to
coat_DenseNet201_weights.h5
 - 317s - loss: 0.6820 - acc: 0.7483 - val_loss:
0.7091 - val_acc: 0.7461
Epoch 8/64
Epoch 00008: val_loss did not improve
 -
316s - loss: 0.6419 - acc: 0.7636 - val_loss: 1.0831 - val_acc: 0.6534
Epoch
9/64
Epoch 00009: val_loss did not improve
 - 316s - loss: 0.6006 - acc: 0.7779
- val_loss: 0.8181 - val_acc: 0.7196
Epoch 10/64
Epoch 00010: val_loss improved
from 0.70912 to 0.60273, saving model to coat_DenseNet201_weights.h5
 - 316s -
loss: 0.5683 - acc: 0.7900 - val_loss: 0.6027 - val_acc: 0.7859
Epoch 11/64
Epoch 00011: val_loss did not improve
 - 316s - loss: 0.5467 - acc: 0.8031 -
val_loss: 0.6763 - val_acc: 0.7807
Epoch 12/64
Epoch 00012: val_loss did not
improve
 - 316s - loss: 0.5013 - acc: 0.8182 - val_loss: 0.8988 - val_acc:
0.7049
Epoch 13/64
Epoch 00013: val_loss did not improve
 - 316s - loss: 0.4681
- acc: 0.8274 - val_loss: 0.6805 - val_acc: 0.7741
Epoch 14/64
Epoch 00014:
val_loss did not improve
 - 316s - loss: 0.4425 - acc: 0.8356 - val_loss: 0.7655
- val_acc: 0.7520
Epoch 15/64
Epoch 00015: val_loss did not improve
 - 317s -
loss: 0.4248 - acc: 0.8465 - val_loss: 0.7243 - val_acc: 0.7601
Epoch 16/64
Epoch 00016: val_loss improved from 0.60273 to 0.56953, saving model to
coat_DenseNet201_weights.h5
 - 317s - loss: 0.3941 - acc: 0.8547 - val_loss:
0.5695 - val_acc: 0.8006
Epoch 17/64
Epoch 00017: val_loss did not improve
 -
316s - loss: 0.3641 - acc: 0.8645 - val_loss: 0.6253 - val_acc: 0.8057
Epoch
18/64
Epoch 00018: val_loss did not improve
 - 316s - loss: 0.3559 - acc: 0.8665
- val_loss: 0.6280 - val_acc: 0.7969
Epoch 19/64
Epoch 00019: val_loss did not
improve
 - 316s - loss: 0.3316 - acc: 0.8803 - val_loss: 0.6551 - val_acc:
0.8035
Epoch 20/64

```{.python .input}

- 训练类型：coat
- 前提条件：Xception lr=0.001 10epoch
- 训练参数和模型
- keyWords Xception lr=0.0001

<=coat_Xception_0329a.csv


```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=42)
X_train.shape, y_train.shape

adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 64
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center = False, # set input mean to 0
over the dataset
        samplewise_center = False, # set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift
images horizontally (fraction of total width)
        height_shift_range =
0.075, # randomly shift images vertivally (fraction of total heigth)
horizontal_flip = False, # randomly flip images
        vertical_flip = False,
shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)
#datagen.fit(X_train,seed=123)
datagen.fit(X_train,seed=123)


prefix_cls =
cur_class.split('_')[0]

checkpointer =
ModelCheckpoint(filepath='{0}1_Xception_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
model.load_weights('{0}_Xception_weights.h5'.format(prefix_cls))



try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果

```

Epoch 1/64
Epoch 00001: val_loss improved from inf to 0.53447, saving model to
coat1_Xception_weights.h5
 - 382s - loss: 0.0780 - acc: 0.9727 - val_loss:
0.5345 - val_acc: 0.8639
Epoch 2/64
Epoch 00002: val_loss did not improve
 -
368s - loss: 0.0424 - acc: 0.9854 - val_loss: 0.5939 - val_acc: 0.8580
Epoch
3/64
Epoch 00003: val_loss did not improve
 - 367s - loss: 0.0308 - acc: 0.9898
- val_loss: 0.6466 - val_acc: 0.8609
Epoch 4/64
Epoch 00004: val_loss did not
improve
 - 367s - loss: 0.0229 - acc: 0.9926 - val_loss: 0.6228 - val_acc:
0.8742
Epoch 5/64
Epoch 00005: val_loss did not improve
 - 367s - loss: 0.0198 -
acc: 0.9940 - val_loss: 0.6400 - val_acc: 0.8734
Epoch 6/64

```{.python .input}


- 训练类型：coat
- 前提条件：无
- 训练参数和模型
- keyWords Xception


cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

adam = Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 64
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.075, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = False, # randomly flip images
        vertical_flip = False,
        shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)

#datagen.fit(X_train,seed=123)
datagen.fit(X_train,seed=123)


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_Xception_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('{0}3_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

- 训练结果

```{.python .input}
Epoch 1/64
Epoch 00001: val_loss improved from inf to 0.64970, saving model to coat_Xception_weights.h5
 - 386s - loss: 0.3967 - acc: 0.8581 - val_loss: 0.6497 - val_acc: 0.8013
Epoch 2/64
Epoch 00002: val_loss did not improve
 - 369s - loss: 0.3451 - acc: 0.8774 - val_loss: 0.6999 - val_acc: 0.7954
Epoch 3/64
Epoch 00003: val_loss did not improve
 - 369s - loss: 0.2941 - acc: 0.8930 - val_loss: 0.7265 - val_acc: 0.7829
Epoch 4/64
Epoch 00004: val_loss did not improve
 - 369s - loss: 0.2575 - acc: 0.9088 - val_loss: 0.6542 - val_acc: 0.7866
Epoch 5/64
Epoch 00005: val_loss did not improve
 - 369s - loss: 0.2191 - acc: 0.9208 - val_loss: 0.6741 - val_acc: 0.8072
Epoch 6/64
Epoch 00006: val_loss improved from 0.64970 to 0.63014, saving model to coat_Xception_weights.h5
 - 368s - loss: 0.2032 - acc: 0.9306 - val_loss: 0.6301 - val_acc: 0.8109
Epoch 7/64
Epoch 00007: val_loss did not improve
 - 367s - loss: 0.1855 - acc: 0.9330 - val_loss: 0.7996 - val_acc: 0.7881
Epoch 8/64
Epoch 00008: val_loss did not improve
 - 367s - loss: 0.1749 - acc: 0.9406 - val_loss: 0.6436 - val_acc: 0.8227
Epoch 9/64

```

- 训练类型：nect
- 前提条件：无
- 训练参数和模型
- keyWords Xception Adamax

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)


X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape


adam = Adam(lr=0.0001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]


model.compile(optimizer=list_opt[2], loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 64
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_Xception_adamax_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('{0}_Xception_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')


```

训练结果

Epoch 1/64
Epoch 00001: val_loss improved from inf to 1.58989, saving
model to
neck_Xception_adamax_weights.h5
 - 191s - loss: 1.5941 - acc: 0.2490 -
val_loss:
1.5899 - val_acc: 0.2661
Epoch 2/64
Epoch 00002: val_loss did not
improve
 -
189s - loss: 1.5891 - acc: 0.2478 - val_loss: 1.5935 - val_acc:
0.2822
Epoch
3/64
Epoch 00003: val_loss improved from 1.58989 to 1.58211, saving
model to
neck_Xception_adamax_weights.h5
 - 190s - loss: 1.5745 - acc: 0.2672 -
val_loss:
1.5821 - val_acc: 0.2851
Epoch 4/64
Epoch 00004: val_loss did not
improve
 -
190s - loss: 1.5695 - acc: 0.2736 - val_loss: 1.6385 - val_acc:
0.3041
Epoch
5/64
Epoch 00005: val_loss improved from 1.58211 to 1.56214, saving
model to
neck_Xception_adamax_weights.h5
 - 190s - loss: 1.5703 - acc: 0.2806 -
val_loss:
1.5621 - val_acc: 0.3231
Epoch 6/64
Epoch 00006: val_loss did not
improve
 -
190s - loss: 1.5670 - acc: 0.2776 - val_loss: 1.5639 - val_acc:
0.3158
Epoch
7/64
Epoch 00007: val_loss did not improve
 - 190s - loss: 1.5569 -
acc: 0.2802
- val_loss: 1.6591 - val_acc: 0.2836
Epoch 8/64
Epoch 00008:
val_loss did not
improve
 - 190s - loss: 1.5465 - acc: 0.2901 - val_loss: 1.5680
- val_acc:
0.2822
Epoch 9/64
Epoch 00009: val_loss improved from 1.56214 to
1.54690, saving
model to neck_Xception_adamax_weights.h5
 - 190s - loss: 1.5396
- acc: 0.2991 -
val_loss: 1.5469 - val_acc: 0.3304
Epoch 10/64
Epoch 00010:
val_loss did not
improve
 - 190s - loss: 1.5342 - acc: 0.3027 - val_loss: 1.5641
- val_acc:
0.2939
Epoch 11/64
Epoch 00011: val_loss did not improve
 - 190s -
loss: 1.5258
- acc: 0.3033 - val_loss: 1.5600 - val_acc: 0.3275
Epoch 12/64
Epoch 00012:
val_loss improved from 1.54690 to 1.53710, saving model to
neck_Xception_adamax_weights.h5
 - 190s - loss: 1.5252 - acc: 0.3031 - val_loss:
1.5371 - val_acc: 0.3260

```{.python .input}
- 训练类型：nect
- 前提条件：无
- 训练参数和模型
- keyWords Xception Nadam

```

cnn_model = Xception(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)


X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape


adam = Adam(lr=0.0001)
nadam=Nadam(lr=0.002)
adamax=Adamax(lr=0.002)
list_opt=[adam,nadam,adamax]
model.compile(optimizer=list_opt[1], loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}_Xception_nadam_weights.h5'.format(prefix_cls),
verbose=1,
                               save_best_only=True,
save_weights_only=True,mode='val_acc')
#model.load_weights('{0}_Xception_weights.h5'.format(prefix_cls))

try:
    #
Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}

训练结果

```

Epoch 1/32
Epoch 00001: val_loss improved from inf to 1.89588, saving model to
neck_Xception_nadam_weights.h5
 - 198s - loss: 1.6013 - acc: 0.2394 - val_loss:
1.8959 - val_acc: 0.2763
Epoch 2/32
Epoch 00002: val_loss improved from 1.89588
to 1.58935, saving model to neck_Xception_nadam_weights.h5
 - 195s - loss:
1.5905 - acc: 0.2560 - val_loss: 1.5893 - val_acc: 0.2763
Epoch 3/32
Epoch
00003: val_loss did not improve
 - 196s - loss: 1.5936 - acc: 0.2480 - val_loss:
11.9487 - val_acc: 0.1637
Epoch 4/32
Epoch 00004: val_loss did not improve
 -
196s - loss: 1.6264 - acc: 0.2488 - val_loss: 2.0790 - val_acc: 0.2763
Epoch
5/32
Epoch 00005: val_loss did not improve
 - 195s - loss: 1.6095 - acc: 0.2452
- val_loss: 1.6895 - val_acc: 0.2734
Epoch 6/32
Epoch 00006: val_loss did not
improve
 - 195s - loss: 1.6088 - acc: 0.2514 - val_loss: 1.5943 - val_acc:
0.2632
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 195s - loss: 1.6049 -
acc: 0.2452 - val_loss: 13.4968 - val_acc: 0.1520
Epoch 8/32
KeyboardInterrupt

```{.python .input}












>>> 18.3.28


- 训练类型：coat
- 前提条件：32epoch base dataautomention lr=0.001   +
5epoch base dataautomention lr=0.0001
=> coat_0326a.csv
- 训练参数和模型
- keyWords X
=>coat_0328a.csv

```cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)


X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=42)
X_train.shape, y_train.shape

# Compile the model
adam = Adam(lr=0.00001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 16
batch_size = 16


datagen = ImageDataGenerator(
        featurewise_center = False, # set input mean to 0 over the dataset
        samplewise_center = False, # set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.05, # randomly zoom image
        width_shift_range = 0.075, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.075, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = False, # randomly flip images
        vertical_flip = False,
        shear_range = 0.075,
        fill_mode = 'constant',
        cval = 0)

#datagen.fit(X_train,seed=123)
datagen.fit(X,seed=123)


prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}4_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}3_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X, y, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')

```

训练结果

```{.python .input}
Epoch 1/16
Epoch 00001: val_loss improved from inf to 0.42880, saving model to coat4_weights.h5
 - 434s - loss: 0.1250 - acc: 0.9623 - val_loss: 0.4288 - val_acc: 0.8698
Epoch 2/16
Epoch 00002: val_loss improved from 0.42880 to 0.39738, saving model to coat4_weights.h5
 - 402s - loss: 0.1267 - acc: 0.9618 - val_loss: 0.3974 - val_acc: 0.8712
Epoch 3/16
Epoch 00003: val_loss improved from 0.39738 to 0.36986, saving model to coat4_weights.h5
 - 397s - loss: 0.1172 - acc: 0.9613 - val_loss: 0.3699 - val_acc: 0.8837
Epoch 4/16
Epoch 00004: val_loss improved from 0.36986 to 0.34866, saving model to coat4_weights.h5
 - 398s - loss: 0.1206 - acc: 0.9602 - val_loss: 0.3487 - val_acc: 0.8882
Epoch 5/16
Epoch 00005: val_loss improved from 0.34866 to 0.32728, saving model to coat4_weights.h5
 - 396s - loss: 0.1136 - acc: 0.9633 - val_loss: 0.3273 - val_acc: 0.8985
Epoch 6/16
KeyboardInterrupt

```

- 训练类型：neck
- 前提条件：18epoch Xception lr=0.001
- 训练参数和模型

- keywords :Xception
lr=0.0001
=>neck_Xception_0328a.csv

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}1_Xception_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('{0}_Xception_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.54545, saving model to neck1_Xception_weights.h5
 - 201s - loss: 0.3862 - acc: 0.8560 - val_loss: 0.5455 - val_acc: 0.8143
Epoch 2/32
Epoch 00002: val_loss improved from 0.54545 to 0.54035, saving model to neck1_Xception_weights.h5
 - 192s - loss: 0.3357 - acc: 0.8724 - val_loss: 0.5403 - val_acc: 0.8275
Epoch 3/32
Epoch 00003: val_loss did not improve
 - 192s - loss: 0.3221 - acc: 0.8786 - val_loss: 0.5608 - val_acc: 0.8289
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 191s - loss: 0.2940 - acc: 0.8898 - val_loss: 0.5588 - val_acc: 0.8260
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 191s - loss: 0.2629 - acc: 0.8978 - val_loss: 0.6038 - val_acc: 0.8202
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 191s - loss: 0.2648 - acc: 0.9024 - val_loss: 0.5909 - val_acc: 0.8173
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 191s - loss: 0.2334 - acc: 0.9151 - val_loss: 0.6134 - val_acc: 0.8231
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 191s - loss: 0.2236 - acc: 0.9177 - val_loss: 0.6213 - val_acc: 0.8363
Epoch 9/32
Epoch 00009: val_loss did not improve
 - 191s - loss: 0.2085 - acc: 0.9199 - val_loss: 0.6219 - val_acc: 0.8333
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 191s - loss: 0.2125 - acc: 0.9221 - val_loss: 0.6276 - val_acc: 0.8363
Epoch 11/32
Epoch 00011: val_loss did not improve
 - 191s - loss: 0.2036 - acc: 0.9263 - val_loss: 0.6640 - val_acc: 0.8216
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 191s - loss: 0.1854 - acc: 0.9315 - val_loss: 0.6508 - val_acc: 0.8202
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 191s - loss: 0.1809 - acc: 0.9339 - val_loss: 0.6325 - val_acc: 0.8333
Epoch 14/32
Epoch 00014: val_loss did not improve
 - 189s - loss: 0.1676 - acc: 0.9379 - val_loss: 0.6188 - val_acc: 0.8450
Epoch 15/32
Epoch 00015: val_loss did not improve
 - 185s - loss: 0.1529 - acc: 0.9431 - val_loss: 0.7273 - val_acc: 0.8173
Epoch 16/32
Epoch 00016: val_loss did not improve
 - 186s - loss: 0.1677 - acc: 0.9365 - val_loss: 0.7790 - val_acc: 0.8070
Epoch 17/32
Epoch 00017: val_loss did not improve
 - 185s - loss: 0.1535 - acc: 0.9411 - val_loss: 0.7609 - val_acc: 0.8216
Epoch 18/32
Epoch 00018: val_loss did not improve
 - 185s - loss: 0.1495 - acc: 0.9391 - val_loss: 0.7341 - val_acc: 0.8406
Epoch 19/32
Epoch 00019: val_loss did not improve
 - 185s - loss: 0.1512 - acc: 0.9449 - val_loss: 0.7295 - val_acc: 0.8348
Epoch 20/32
Epoch 00020: val_loss did not improve
 - 185s - loss: 0.1251 - acc: 0.9541 - val_loss: 0.7607 - val_acc: 0.8246
Epoch 21/32
Epoch 00021: val_loss did not improve
 - 185s - loss: 0.1327 - acc: 0.9533 - val_loss: 0.7566 - val_acc: 0.8260
Epoch 22/32
Epoch 00022: val_loss did not improve
 - 185s - loss: 0.1195 - acc: 0.9577 - val_loss: 0.7964 - val_acc: 0.8085
Epoch 23/32
Epoch 00023: val_loss did not improve
 - 185s - loss: 0.1192 - acc: 0.9567 - val_loss: 0.7522 - val_acc: 0.8246
Epoch 24/32
Epoch 00024: val_loss did not improve
 - 185s - loss: 0.1211 - acc: 0.9543 - val_loss: 0.8125 - val_acc: 0.8231
Epoch 25/32
Epoch 00025: val_loss did not improve
 - 185s - loss: 0.1256 - acc: 0.9551 - val_loss: 0.7540 - val_acc: 0.8348
Epoch 26/32
Epoch 00026: val_loss did not improve
 - 185s - loss: 0.1144 - acc: 0.9573 - val_loss: 0.7754 - val_acc: 0.8377
Epoch 27/32
Epoch 00027: val_loss did not improve
 - 185s - loss: 0.1148 - acc: 0.9605 - val_loss: 0.7680 - val_acc: 0.8392
Epoch 28/32
Epoch 00028: val_loss did not improve
 - 185s - loss: 0.1012 - acc: 0.9615 - val_loss: 0.8007 - val_acc: 0.8304
Epoch 29/32
Epoch 00029: val_loss did not improve
 - 185s - loss: 0.1008 - acc: 0.9639 - val_loss: 0.7508 - val_acc: 0.8363
Epoch 30/32
Epoch 00030: val_loss did not improve
 - 185s - loss: 0.1057 - acc: 0.9607 - val_loss: 0.7623 - val_acc: 0.8392
Epoch 31/32
Epoch 00031: val_loss did not improve
 - 185s - loss: 0.0897 - acc: 0.9667 - val_loss: 0.8080 - val_acc: 0.8406
Epoch 32/32

```

- 训练类型：neck
- 前提条件：无
- 训练参数和模型

- keywords :Xception lr=0.001

```{.python .input}
cnn_model = Xception(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}_Xception_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

#model.load_weights('{0}_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 1.58919, saving model to neck_Xception_weights.h5
 - 206s - loss: 1.2529 - acc: 0.4900 - val_loss: 1.5892 - val_acc: 0.5673
Epoch 2/32
Epoch 00002: val_loss did not improve
 - 188s - loss: 0.9539 - acc: 0.6364 - val_loss: 1.6966 - val_acc: 0.5190
Epoch 3/32
Epoch 00003: val_loss improved from 1.58919 to 0.76638, saving model to neck_Xception_weights.h5
 - 191s - loss: 0.8549 - acc: 0.6775 - val_loss: 0.7664 - val_acc: 0.7178
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 189s - loss: 0.7615 - acc: 0.7167 - val_loss: 0.8102 - val_acc: 0.7368
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 189s - loss: 0.7306 - acc: 0.7326 - val_loss: 1.1564 - val_acc: 0.6506
Epoch 6/32
Epoch 00006: val_loss improved from 0.76638 to 0.72056, saving model to neck_Xception_weights.h5
 - 191s - loss: 0.6749 - acc: 0.7516 - val_loss: 0.7206 - val_acc: 0.7719
Epoch 7/32
Epoch 00007: val_loss improved from 0.72056 to 0.67715, saving model to neck_Xception_weights.h5
 - 190s - loss: 0.6604 - acc: 0.7602 - val_loss: 0.6772 - val_acc: 0.7719
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 189s - loss: 0.6075 - acc: 0.7833 - val_loss: 0.8048 - val_acc: 0.7500
Epoch 9/32
Epoch 00009: val_loss did not improve
 - 189s - loss: 0.5809 - acc: 0.7796 - val_loss: 0.8567 - val_acc: 0.7500
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 190s - loss: 0.5559 - acc: 0.7951 - val_loss: 0.7849 - val_acc: 0.7412
Epoch 11/32
Epoch 00011: val_loss improved from 0.67715 to 0.60128, saving model to neck_Xception_weights.h5
 - 191s - loss: 0.5504 - acc: 0.7955 - val_loss: 0.6013 - val_acc: 0.7719
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 189s - loss: 0.5223 - acc: 0.8113 - val_loss: 0.7071 - val_acc: 0.7573
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 189s - loss: 0.4876 - acc: 0.8189 - val_loss: 0.6544 - val_acc: 0.7953
Epoch 14/32
Epoch 00014: val_loss did not improve
 - 189s - loss: 0.4693 - acc: 0.8287 - val_loss: 0.6359 - val_acc: 0.7763
Epoch 15/32
Epoch 00015: val_loss did not improve
 - 189s - loss: 0.4701 - acc: 0.8263 - val_loss: 0.6519 - val_acc: 0.7792
Epoch 16/32
Epoch 00016: val_loss did not improve
 - 189s - loss: 0.4281 - acc: 0.8446 - val_loss: 0.6084 - val_acc: 0.8026
Epoch 17/32
Epoch 00017: val_loss did not improve
 - 189s - loss: 0.4071 - acc: 0.8494 - val_loss: 0.6374 - val_acc: 0.8070
Epoch 18/32
KeyboardInterrupt

```

- 训练类型：neck
- 前提条件：baseline data automention 训练了32epoch+data automention
参数激进调整64epoch+加入lr =0.00001 +10 epoch
- 训练参数和模型

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}11_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}10_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.65416, saving model to neck12_weights.h5
 - 230s - loss: 0.2159 - acc: 0.9247 - val_loss: 0.6542 - val_acc: 0.8333
Epoch 2/32
Epoch 00002: val_loss did not improve
 - 199s - loss: 0.2018 - acc: 0.9225 - val_loss: 0.6585 - val_acc: 0.8289
Epoch 3/32
Epoch 00003: val_loss did not improve
 - 199s - loss: 0.1965 - acc: 0.9269 - val_loss: 0.6652 - val_acc: 0.8246
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 199s - loss: 0.1975 - acc: 0.9279 - val_loss: 0.6582 - val_acc: 0.8275
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 199s - loss: 0.1904 - acc: 0.9285 - val_loss: 0.6648 - val_acc: 0.8246
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 199s - loss: 0.1919 - acc: 0.9297 - val_loss: 0.6662 - val_acc: 0.8289
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 198s - loss: 0.1919 - acc: 0.9275 - val_loss: 0.6759 - val_acc: 0.8289
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 199s - loss: 0.2005 - acc: 0.9221 - val_loss: 0.6746 - val_acc: 0.8260
Epoch 9/32
KeyboardInterrupt

```

- 训练类型：neck

- 前提条件：baseline data automention 训练了32epoch +
data automention
参数激进调整32epoch lr=0.001 +
data automention 参数激进调整31epoch lr=0.0001 +
data
automention 参数激进调整31epoch lr=0.0001 +

- 训练参数和模型

=> neck_0328a.csv

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}10_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}8_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.65151, saving model to neck10_weights.h5
 - 244s - loss: 0.2337 - acc: 0.9141 - val_loss: 0.6515 - val_acc: 0.8202
Epoch 2/32
Epoch 00002: val_loss improved from 0.65151 to 0.64565, saving model to neck10_weights.h5
 - 205s - loss: 0.2285 - acc: 0.9173 - val_loss: 0.6456 - val_acc: 0.8202
Epoch 3/32
Epoch 00003: val_loss did not improve
 - 199s - loss: 0.2185 - acc: 0.9213 - val_loss: 0.6474 - val_acc: 0.8143
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 199s - loss: 0.2148 - acc: 0.9161 - val_loss: 0.6481 - val_acc: 0.8275
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 198s - loss: 0.1916 - acc: 0.9285 - val_loss: 0.6480 - val_acc: 0.8319
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 198s - loss: 0.1929 - acc: 0.9255 - val_loss: 0.6839 - val_acc: 0.8319
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 198s - loss: 0.1894 - acc: 0.9281 - val_loss: 0.7093 - val_acc: 0.8289
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 197s - loss: 0.1774 - acc: 0.9311 - val_loss: 0.6961 - val_acc: 0.8333
Epoch 9/32
Epoch 00009: val_loss did not improve
 - 197s - loss: 0.1704 - acc: 0.9375 - val_loss: 0.7190 - val_acc: 0.8333
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 197s - loss: 0.1744 - acc: 0.9335 - val_loss: 0.6892 - val_acc: 0.8348
Epoch 11/32
Epoch 00011: val_loss did not improve
 - 197s - loss: 0.1710 - acc: 0.9349 - val_loss: 0.7221 - val_acc: 0.8377
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 198s - loss: 0.1643 - acc: 0.9413 - val_loss: 0.7081 - val_acc: 0.8348
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 198s - loss: 0.1529 - acc: 0.9377 - val_loss: 0.6952 - val_acc: 0.8494
Epoch 14/32
Epoch 00014: val_loss did not improve
 - 198s - loss: 0.1556 - acc: 0.9423 - val_loss: 0.7721 - val_acc: 0.8202
Epoch 15/32
Epoch 00015: val_loss did not improve
 - 198s - loss: 0.1588 - acc: 0.9409 - val_loss: 0.7514 - val_acc: 0.8406
Epoch 16/32
Epoch 00016: val_loss did not improve
 - 199s - loss: 0.1696 - acc: 0.9345 - val_loss: 0.7282 - val_acc: 0.8348
Epoch 17/32
Epoch 00017: val_loss did not improve
 - 198s - loss: 0.1438 - acc: 0.9437 - val_loss: 0.7372 - val_acc: 0.8304
Epoch 18/32
Epoch 00018: val_loss did not improve
 - 198s - loss: 0.1440 - acc: 0.9475 - val_loss: 0.7958 - val_acc: 0.8319
Epoch 19/32
Epoch 00019: val_loss did not improve
 - 199s - loss: 0.1432 - acc: 0.9469 - val_loss: 0.7807 - val_acc: 0.8377
Epoch 20/32
Epoch 00020: val_loss did not improve
 - 198s - loss: 0.1461 - acc: 0.9441 - val_loss: 0.8102 - val_acc: 0.8333
Epoch 21/32
Epoch 00021: val_loss did not improve
 - 200s - loss: 0.1470 - acc: 0.9427 - val_loss: 0.7574 - val_acc: 0.8363
Epoch 22/32
Epoch 00022: val_loss did not improve
 - 200s - loss: 0.1371 - acc: 0.9493 - val_loss: 0.7740 - val_acc: 0.8275
Epoch 23/32
Epoch 00023: val_loss did not improve
 - 201s - loss: 0.1379 - acc: 0.9497 - val_loss: 0.7913 - val_acc: 0.8348
Epoch 24/32
Epoch 00024: val_loss did not improve
 - 200s - loss: 0.1384 - acc: 0.9473 - val_loss: 0.7531 - val_acc: 0.8436
Epoch 25/32


```

- 训练类型：neck
- 前提条件：
- 前提条件：baseline data automention 训练了32epoch +
data
automention 参数激进调整32epoch lr=0.001 +
data automention 参数激进调整31epoch lr=0.0001 +
data automention 参数激进调整31epoch lr=0.0001 +
- 训练参数和模型

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.00001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}9_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}8_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果(被手动暂停)

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.61086, saving model to neck9_weights.h5
 - 243s - loss: 0.2336 - acc: 0.9113 - val_loss: 0.6109 - val_acc: 0.8202
Epoch 2/32
Epoch 00002: val_loss did not improve
 - 199s - loss: 0.2364 - acc: 0.9159 - val_loss: 0.6244 - val_acc: 0.8202
Epoch 3/32
Epoch 00003: val_loss did not improve
 - 200s - loss: 0.2292 - acc: 0.9175 - val_loss: 0.6222 - val_acc: 0.8246
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 200s - loss: 0.2216 - acc: 0.9193 - val_loss: 0.6221 - val_acc: 0.8275
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 200s - loss: 0.2337 - acc: 0.9113 - val_loss: 0.6200 - val_acc: 0.8231
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 200s - loss: 0.2185 - acc: 0.9201 - val_loss: 0.6234 - val_acc: 0.8246
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 201s - loss: 0.2201 - acc: 0.9177 - val_loss: 0.6317 - val_acc: 0.8231
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 199s - loss: 0.2263 - acc: 0.9145 - val_loss: 0.6326 - val_acc: 0.8246
Epoch 9/32
Epoch 00009: val_loss did not improve
 - 198s - loss: 0.2165 - acc: 0.9173 - val_loss: 0.6257 - val_acc: 0.8202
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 198s - loss: 0.2143 - acc: 0.9179 - val_loss: 0.6335 - val_acc: 0.8216
Epoch 11/32
Epoch 00011: val_loss did not improve
 - 198s - loss: 0.2114 - acc: 0.9205 - val_loss: 0.6374 - val_acc: 0.8202
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 198s - loss: 0.2037 - acc: 0.9261 - val_loss: 0.6430 - val_acc: 0.8187
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 198s - loss: 0.2122 - acc: 0.9219 - val_loss: 0.6451 - val_acc: 0.8187
Epoch 14/32
```

- 训练类型：neck
- 前提条件：baseline data automention 训练了32epoch +
data automention
参数激进调整32epoch lr=0.001 +
data automention 参数激进调整31epoch lr=0.0001

- 训练参数和模型

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}8_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}7_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

- 训练结果

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.62065, saving model to neck8_weights.h5
 - 248s - loss: 0.2654 - acc: 0.8990 - val_loss: 0.6207 - val_acc: 0.8202
Epoch 2/32
Epoch 00002: val_loss did not improve
 - 199s - loss: 0.2366 - acc: 0.9143 - val_loss: 0.6551 - val_acc: 0.8158
Epoch 3/32
Epoch 00003: val_loss did not improve
 - 200s - loss: 0.2123 - acc: 0.9169 - val_loss: 0.6579 - val_acc: 0.8275
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 199s - loss: 0.2218 - acc: 0.9135 - val_loss: 0.6314 - val_acc: 0.8304
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 199s - loss: 0.1997 - acc: 0.9283 - val_loss: 0.6525 - val_acc: 0.8260
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 200s - loss: 0.1971 - acc: 0.9287 - val_loss: 0.6885 - val_acc: 0.8289
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 199s - loss: 0.1893 - acc: 0.9287 - val_loss: 0.6866 - val_acc: 0.8158
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 199s - loss: 0.2024 - acc: 0.9235 - val_loss: 0.6811 - val_acc: 0.8260
Epoch 9/32
Epoch 00009: val_loss did not improve
 - 199s - loss: 0.1668 - acc: 0.9359 - val_loss: 0.7179 - val_acc: 0.8275
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 199s - loss: 0.1665 - acc: 0.9369 - val_loss: 0.7657 - val_acc: 0.8173
Epoch 11/32
Epoch 00011: val_loss did not improve
 - 199s - loss: 0.1774 - acc: 0.9331 - val_loss: 0.6907 - val_acc: 0.8260
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 199s - loss: 0.1714 - acc: 0.9369 - val_loss: 0.7375 - val_acc: 0.8406
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 199s - loss: 0.1667 - acc: 0.9373 - val_loss: 0.7335 - val_acc: 0.8304
Epoch 14/32
Epoch 00014: val_loss did not improve
 - 200s - loss: 0.1690 - acc: 0.9373 - val_loss: 0.7577 - val_acc: 0.8231
Epoch 15/32
Epoch 00015: val_loss did not improve
 - 199s - loss: 0.1493 - acc: 0.9423 - val_loss: 0.7811 - val_acc: 0.8289
Epoch 16/32
Epoch 00016: val_loss did not improve
 - 201s - loss: 0.1503 - acc: 0.9435 - val_loss: 0.7406 - val_acc: 0.8289
Epoch 17/32
Epoch 00017: val_loss did not improve
 - 198s - loss: 0.1527 - acc: 0.9439 - val_loss: 0.7043 - val_acc: 0.8450
Epoch 18/32
Epoch 00018: val_loss did not improve
 - 198s - loss: 0.1591 - acc: 0.9411 - val_loss: 0.7374 - val_acc: 0.8275
Epoch 19/32
Epoch 00019: val_loss did not improve
 - 198s - loss: 0.1426 - acc: 0.9493 - val_loss: 0.7679 - val_acc: 0.8275
Epoch 20/32
Epoch 00020: val_loss did not improve
 - 199s - loss: 0.1456 - acc: 0.9463 - val_loss: 0.7384 - val_acc: 0.8319
Epoch 21/32
Epoch 00021: val_loss did not improve
 - 199s - loss: 0.1428 - acc: 0.9441 - val_loss: 0.7827 - val_acc: 0.8289
Epoch 22/32
Epoch 00022: val_loss did not improve
 - 199s - loss: 0.1289 - acc: 0.9531 - val_loss: 0.8127 - val_acc: 0.8275
Epoch 23/32
Epoch 00023: val_loss did not improve
 - 199s - loss: 0.1375 - acc: 0.9491 - val_loss: 0.7275 - val_acc: 0.8392
Epoch 24/32
Epoch 00024: val_loss did not improve
 - 198s - loss: 0.1287 - acc: 0.9531 - val_loss: 0.7715 - val_acc: 0.8319
Epoch 25/32
Epoch 00025: val_loss did not improve
 - 198s - loss: 0.1356 - acc: 0.9501 - val_loss: 0.7857 - val_acc: 0.8289
Epoch 26/32
Epoch 00026: val_loss did not improve
 - 198s - loss: 0.1280 - acc: 0.9537 - val_loss: 0.7904 - val_acc: 0.8333
Epoch 27/32
Epoch 00027: val_loss did not improve
 - 198s - loss: 0.1238 - acc: 0.9551 - val_loss: 0.7926 - val_acc: 0.8421
Epoch 28/32
Epoch 00028: val_loss did not improve
 - 198s - loss: 0.1280 - acc: 0.9541 - val_loss: 0.8087 - val_acc: 0.8319
Epoch 29/32
Epoch 00029: val_loss did not improve
 - 198s - loss: 0.1184 - acc: 0.9571 - val_loss: 0.9051 - val_acc: 0.8275
Epoch 30/32
Epoch 00030: val_loss did not improve
 - 198s - loss: 0.1356 - acc: 0.9521 - val_loss: 0.8313 - val_acc: 0.8319
Epoch 31/32
Epoch 00031: val_loss did not improve
 - 198s - loss: 0.1283 - acc: 0.9519 - val_loss: 0.8873 - val_acc: 0.8202

```

- 训练类型：neck
- 前提条件：baseline data automention 训练了32epoch+data automention
参数激进调整32epoch lr =0.001
- 训练参数和模型

```{.python .input}
cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3), weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x = Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class, activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12, random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen = ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0 over the dataset
        samplewise_center = False ,# set each sample mean to 0
        featurewise_std_normalization = False, # divide inputs by std of the dataset
        samplewise_std_normalization = False, # divide each input by its std
        zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, # randomly rotate images in the range (degrees, 0 to 180)
        zoom_range = 0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift images horizontally (fraction of total width)
        height_shift_range = 0.2, # randomly shift images vertivally (fraction of total heigth)
        horizontal_flip = True, # randomly flip images
        vertical_flip = False,
        shear_range = 0.1,
        fill_mode = 'constant',
        cval = 0)

datagen.fit(X_train,seed=123)

prefix_cls = cur_class.split('_')[0]

checkpointer = ModelCheckpoint(filepath='{0}7_weights.h5'.format(prefix_cls), verbose=1,
                               save_best_only=True, save_weights_only=True,mode='val_acc')

model.load_weights('{0}6_weights.h5'.format(prefix_cls))

try:
    # Fit the model
    history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),
                                  epochs=epochs, validation_data=(X_valid, y_valid),
                                  verbose=2, steps_per_epoch=X_train.shape[0] // batch_size,
                                  callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
    print('KeyboardInterrupt')
```

训练结果

```{.python .input}
Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.59985, saving model to neck7_weights.h5
 - 197s - loss: 0.2834 - acc: 0.8950 - val_loss: 0.5999 - val_acc: 0.8216
Epoch 2/32
Epoch 00002: val_loss did not improve
 - 198s - loss: 0.2589 - acc: 0.9048 - val_loss: 0.6165 - val_acc: 0.8202
Epoch 3/32
Epoch 00003: val_loss did not improve
 - 200s - loss: 0.2464 - acc: 0.9083 - val_loss: 0.6378 - val_acc: 0.8260
Epoch 4/32
Epoch 00004: val_loss did not improve
 - 200s - loss: 0.2334 - acc: 0.9111 - val_loss: 0.6925 - val_acc: 0.8231
Epoch 5/32
Epoch 00005: val_loss did not improve
 - 200s - loss: 0.2116 - acc: 0.9217 - val_loss: 0.6568 - val_acc: 0.8231
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 200s - loss: 0.2146 - acc: 0.9203 - val_loss: 0.6788 - val_acc: 0.8246
Epoch 7/32
Epoch 00007: val_loss did not improve
 - 200s - loss: 0.1839 - acc: 0.9315 - val_loss: 0.6933 - val_acc: 0.8246
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 200s - loss: 0.2028 - acc: 0.9243 - val_loss: 0.7010 - val_acc: 0.8275
Epoch 9/32
Epoch 00009: val_loss did not improve
 - 199s - loss: 0.2045 - acc: 0.9255 - val_loss: 0.6967 - val_acc: 0.8275
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 200s - loss: 0.1874 - acc: 0.9283 - val_loss: 0.7187 - val_acc: 0.8187
Epoch 11/32
Epoch 00011: val_loss did not improve
 - 199s - loss: 0.1841 - acc: 0.9311 - val_loss: 0.7027 - val_acc: 0.8377
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 199s - loss: 0.1693 - acc: 0.9387 - val_loss: 0.7583 - val_acc: 0.8289
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 200s - loss: 0.1762 - acc: 0.9361 - val_loss: 0.7318 - val_acc: 0.8333
Epoch 14/32
Epoch 00014: val_loss did not improve
 - 199s - loss: 0.1597 - acc: 0.9405 - val_loss: 0.7444 - val_acc: 0.8216
Epoch 15/32
Epoch 00015: val_loss did not improve
 - 199s - loss: 0.1547 - acc: 0.9435 - val_loss: 0.7660 - val_acc: 0.8289
Epoch 16/32
Epoch 00016: val_loss did not improve
 - 199s - loss: 0.1661 - acc: 0.9395 - val_loss: 0.7851 - val_acc: 0.8246
Epoch 17/32
Epoch 00017: val_loss did not improve
 - 199s - loss: 0.1618 - acc: 0.9373 - val_loss: 0.7722 - val_acc: 0.8231
Epoch 18/32
Epoch 00018: val_loss did not improve
 - 199s - loss: 0.1658 - acc: 0.9359 - val_loss: 0.7745 - val_acc: 0.8289
Epoch 19/32
Epoch 00019: val_loss did not improve
 - 199s - loss: 0.1509 - acc: 0.9419 - val_loss: 0.8191 - val_acc: 0.8304
Epoch 20/32
Epoch 00020: val_loss did not improve
 - 199s - loss: 0.1431 - acc: 0.9433 - val_loss: 0.8356 - val_acc: 0.8260
Epoch 21/32
Epoch 00021: val_loss did not improve
 - 199s - loss: 0.1458 - acc: 0.9437 - val_loss: 0.8121 - val_acc: 0.8289
Epoch 22/32
Epoch 00022: val_loss did not improve
 - 199s - loss: 0.1386 - acc: 0.9519 - val_loss: 0.8112 - val_acc: 0.8392
Epoch 23/32
Epoch 00023: val_loss did not improve
 - 199s - loss: 0.1516 - acc: 0.9433 - val_loss: 0.8055 - val_acc: 0.8304
Epoch 24/32
Epoch 00024: val_loss did not improve
 - 199s - loss: 0.1392 - acc: 0.9433 - val_loss: 0.7975 - val_acc: 0.8304
Epoch 25/32
Epoch 00025: val_loss did not improve
 - 200s - loss: 0.1375 - acc: 0.9477 - val_loss: 0.8535 - val_acc: 0.8319
Epoch 26/32
Epoch 00026: val_loss did not improve
 - 199s - loss: 0.1356 - acc: 0.9517 - val_loss: 0.8551 - val_acc: 0.8392
Epoch 27/32
Epoch 00027: val_loss did not improve
 - 200s - loss: 0.1296 - acc: 0.9529 - val_loss: 0.8797 - val_acc: 0.8319
Epoch 28/32
Epoch 00028: val_loss did not improve
 - 199s - loss: 0.1210 - acc: 0.9513 - val_loss: 0.8791 - val_acc: 0.8216
Epoch 29/32
Epoch 00029: val_loss did not improve
 - 199s - loss: 0.1263 - acc: 0.9521 - val_loss: 0.8408 - val_acc: 0.8421
Epoch 30/32
Epoch 00030: val_loss did not improve
 - 199s - loss: 0.1200 - acc: 0.9531 - val_loss: 0.8731 - val_acc: 0.8304
Epoch 31/32
Epoch 00031: val_loss did not improve
 - 199s - loss: 0.1204 - acc: 0.9569 - val_loss: 0.8898 - val_acc: 0.8304
 ```







- 训练类型：neck
- 前提条件：baseline data automention 训练了32epoch
- 训练参数和模型

```

cnn_model = InceptionResNetV2(include_top=False, input_shape=(width, width, 3),
weights='imagenet')

inputs = Input((width, width, 3))

x = inputs
x =
Lambda(preprocess_input, name='preprocessing')(x)
x = cnn_model(x)
x =
GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
x = Dense(n_class,
activation='softmax', name='softmax')(x)

model = Model(inputs, x)

X_train,
X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.12,
random_state=41)
X_train.shape, y_train.shape

#Compile the model
adam =
Adam(lr=0.001)
model.compile(optimizer=adam, loss='categorical_crossentropy',
metrics=['accuracy'])

epochs = 32
batch_size = 16

datagen =
ImageDataGenerator(
        featurewise_center =False ,# set input mean to 0
over the dataset
        samplewise_center = False ,# set each sample mean to 0
featurewise_std_normalization = False, # divide inputs by std of the dataset
samplewise_std_normalization = False, # divide each input by its std
zca_whitening = False, # apply ZCA whitening
        rotation_range = 10, #
randomly rotate images in the range (degrees, 0 to 180)
        zoom_range =
0.3, # randomly zoom image
        width_shift_range = 0.2, # randomly shift
images horizontally (fraction of total width)
        height_shift_range = 0.2,
# randomly shift images vertivally (fraction of total heigth)
horizontal_flip =
True, # randomly flip images
        vertical_flip = False,
shear_range = 0.1,
fill_mode = 'constant',
        cval = 0)
datagen.fit(X_train,seed=123)
prefix_cls = cur_class.split('_')[0]
checkpointer =
ModelCheckpoint(filepath='{0}6_weights.h5'.format(prefix_cls),
verbose=1,
save_best_only=True,
save_weights_only=True,mode='val_acc')
model.load_weights('{0}5_weights.h5'.format(prefix_cls))

try:
    # Fit the
model
    history = model.fit_generator(datagen.flow(X_train, y_train,
batch_size=batch_size),
                                  epochs=epochs,
validation_data=(X_valid, y_valid),
                                  verbose=2,
steps_per_epoch=X_train.shape[0] // batch_size,
callbacks=[EarlyStopping(patience=30), checkpointer])
except KeyboardInterrupt:
print('KeyboardInterrupt')

```{.python .input}
训练结果：
```

Epoch 1/32
Epoch 00001: val_loss improved from inf to 0.79877, saving model to
neck6_weights.h5
 - 242s - loss: 0.6528 - acc: 0.7638 - val_loss: 0.7988 -
val_acc: 0.7515
Epoch 2/32
Epoch 00002: val_loss did not improve
 - 200s - loss:
0.6689 - acc: 0.7556 - val_loss: 1.0735 - val_acc: 0.6784
Epoch 3/32
Epoch
00003: val_loss did not improve
 - 200s - loss: 0.6702 - acc: 0.7520 - val_loss:
0.8614 - val_acc: 0.7251
Epoch 4/32
Epoch 00004: val_loss improved from 0.79877
to 0.70471, saving model to neck6_weights.h5
 - 206s - loss: 0.6238 - acc:
0.7766 - val_loss: 0.7047 - val_acc: 0.7398
Epoch 5/32
Epoch 00005: val_loss did
not improve
 - 200s - loss: 0.6014 - acc: 0.7776 - val_loss: 0.7153 - val_acc:
0.7558
Epoch 6/32
Epoch 00006: val_loss did not improve
 - 200s - loss: 0.6106 -
acc: 0.7758 - val_loss: 0.7797 - val_acc: 0.7193
Epoch 7/32
Epoch 00007:
val_loss did not improve
 - 200s - loss: 0.5683 - acc: 0.7937 - val_loss: 0.7770
- val_acc: 0.7325
Epoch 8/32
Epoch 00008: val_loss did not improve
 - 200s -
loss: 0.5342 - acc: 0.8047 - val_loss: 0.7055 - val_acc: 0.7690
Epoch 9/32
Epoch
00009: val_loss improved from 0.70471 to 0.67650, saving model to
neck6_weights.h5
 - 206s - loss: 0.5591 - acc: 0.7951 - val_loss: 0.6765 -
val_acc: 0.7749
Epoch 10/32
Epoch 00010: val_loss did not improve
 - 200s -
loss: 0.5177 - acc: 0.8075 - val_loss: 0.8029 - val_acc: 0.7456
Epoch 11/32
Epoch 00011: val_loss improved from 0.67650 to 0.65336, saving model to
neck6_weights.h5
 - 207s - loss: 0.5083 - acc: 0.8177 - val_loss: 0.6534 -
val_acc: 0.8012
Epoch 12/32
Epoch 00012: val_loss did not improve
 - 200s -
loss: 0.4947 - acc: 0.8193 - val_loss: 0.7508 - val_acc: 0.7515
Epoch 13/32
Epoch 00013: val_loss did not improve
 - 200s - loss: 0.4754 - acc: 0.8185 -
val_loss: 0.7396 - val_acc: 0.7485
Epoch 14/32
Epoch 00014: val_loss did not
improve
 - 200s - loss: 0.4802 - acc: 0.8185 - val_loss: 1.7974 - val_acc:
0.6272
Epoch 15/32
Epoch 00015: val_loss did not improve
 - 200s - loss: 0.4635
- acc: 0.8301 - val_loss: 0.7311 - val_acc: 0.7515
Epoch 16/32
Epoch 00016:
val_loss did not improve
 - 200s - loss: 0.4385 - acc: 0.8385 - val_loss: 0.9764
- val_acc: 0.7295
Epoch 17/32
Epoch 00017: val_loss did not improve
 - 200s -
loss: 0.4357 - acc: 0.8355 - val_loss: 0.7310 - val_acc: 0.7734
Epoch 18/32
Epoch 00018: val_loss did not improve
 - 200s - loss: 0.4030 - acc: 0.8482 -
val_loss: 0.7097 - val_acc: 0.7836
Epoch 19/32
Epoch 00019: val_loss did not
improve
 - 199s - loss: 0.4073 - acc: 0.8544 - val_loss: 0.7676 - val_acc:
0.7749
Epoch 20/32
Epoch 00020: val_loss did not improve
 - 198s - loss: 0.4019
- acc: 0.8520 - val_loss: 0.7188 - val_acc: 0.7734
Epoch 21/32
Epoch 00021:
val_loss improved from 0.65336 to 0.62302, saving model to neck6_weights.h5
 -
205s - loss: 0.4153 - acc: 0.8502 - val_loss: 0.6230 - val_acc: 0.8012
Epoch
22/32
Epoch 00022: val_loss improved from 0.62302 to 0.61616, saving model to
neck6_weights.h5
 - 204s - loss: 0.3936 - acc: 0.8510 - val_loss: 0.6162 -
val_acc: 0.8012
Epoch 23/32
Epoch 00023: val_loss did not improve
 - 197s -
loss: 0.3782 - acc: 0.8592 - val_loss: 0.7070 - val_acc: 0.7807
Epoch 24/32
Epoch 00024: val_loss did not improve
 - 199s - loss: 0.3691 - acc: 0.8616 -
val_loss: 0.7978 - val_acc: 0.7661
Epoch 25/32
Epoch 00025: val_loss did not
improve
 - 198s - loss: 0.3520 - acc: 0.8682 - val_loss: 0.8203 - val_acc:
0.7690
Epoch 26/32
Epoch 00026: val_loss did not improve
 - 198s - loss: 0.3725
- acc: 0.8636 - val_loss: 0.6722 - val_acc: 0.7705
Epoch 27/32
Epoch 00027:
val_loss did not improve
 - 199s - loss: 0.3305 - acc: 0.8812 - val_loss: 0.8246
- val_acc: 0.7632
Epoch 28/32
Epoch 00028: val_loss did not improve
 - 198s -
loss: 0.3312 - acc: 0.8768 - val_loss: 0.7163 - val_acc: 0.8026
Epoch 29/32
Epoch 00029: val_loss did not improve
 - 198s - loss: 0.3331 - acc: 0.8766 -
val_loss: 0.8060 - val_acc: 0.7968
Epoch 30/32
Epoch 00030: val_loss did not
improve
 - 198s - loss: 0.3407 - acc: 0.8788 - val_loss: 0.9027 - val_acc:
0.7939
Epoch 31/32
Epoch 00031: val_loss did not improve
 - 198s - loss: 0.3191
- acc: 0.8886 - val_loss: 0.7577 - val_acc: 0.7895
Epoch 32/32
Epoch 00032:
val_loss did not improve
 - 198s - loss: 0.3125 - acc: 0.8850 - val_loss: 0.8072
- val_acc: 0.7895
 ```
