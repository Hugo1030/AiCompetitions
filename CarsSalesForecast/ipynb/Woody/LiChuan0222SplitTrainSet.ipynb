{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 选用不同特征集\n",
    "* 重新划分测试集进行测试\n",
    "    * 训练集01 2013-01 至 2017-07, 测试集 2017-08\n",
    "    * 训练集02 2013-01 至 2017-08, 测试集 2017-09\n",
    "    * 训练集03 2013-01 至 2017-09, 测试集 2017-10\n",
    "* 后面再重新划分训练集, 最好选用相同时长, 测试结果最佳为宜\n",
    "* 测试 XGBoost/LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选用带时间窗口, 带同比/环比数据集 特征集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../raw/LiChuan/dataset.csv')\n",
    "submit = pd.read_csv('../../raw/CarsSaleForecast/yancheng_testA_20171225.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉 2012 年数据\n",
    "data = data[data['year'] != 2012]\n",
    "labels = data['sale_quantity']\n",
    "data = data.drop(['sale_date', 'class_id', 'sale_quantity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集 2013-01 至 2017-07, 测试集 2017-08\n",
    "train_1 = data[:-560]\n",
    "test_1 = data[-560:-420]\n",
    "labels_1 = labels[:-560]\n",
    "test_labels_1 = labels[-560:-420]\n",
    "\n",
    "# 训练集 2013-01 至 2017-08, 测试集 2017-09\n",
    "train_2 = data[:-420]\n",
    "test_2 = data[-420:-280]\n",
    "labels_2 = labels[:-420]\n",
    "test_labels_2 = labels[-420:-280]\n",
    "\n",
    "# 训练集 2013-01 至 2017-09, 测试集 2017-10\n",
    "train_3 = data[:-280]\n",
    "test_3 = data[-280:-140]\n",
    "labels_3 = labels[:-280]\n",
    "test_labels_3 = labels[-280:-140]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 120.66188708082196\n"
     ]
    }
   ],
   "source": [
    "lgb_train_1 = lgb.Dataset(train_1, labels_1)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.002,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 20,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_1 = lgb.train(params, lgb_train_1, 900)\n",
    "lgb_pred_1 = model_1.predict(test_1)\n",
    "rmsetmp_1 = sp.sqrt(sp.mean((test_labels_1 - lgb_pred_1) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 116.8883026326931\n"
     ]
    }
   ],
   "source": [
    "lgb_train_2 = lgb.Dataset(train_2, labels_2)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.002,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 20,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_2 = lgb.train(params, lgb_train_2, 900)\n",
    "lgb_pred_2 = model_2.predict(test_2)\n",
    "rmsetmp_2 = sp.sqrt(sp.mean((test_labels_2 - lgb_pred_2) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 131.56512907867213\n"
     ]
    }
   ],
   "source": [
    "lgb_train_3 = lgb.Dataset(train_3, labels_3)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.003,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 30,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_3 = lgb.train(params, lgb_train_3, 900)\n",
    "lgb_pred_3 = model_3.predict(test_3)\n",
    "rmsetmp_3 = sp.sqrt(sp.mean((test_labels_3 - lgb_pred_3) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rmse is: 123.03843959739572\n"
     ]
    }
   ],
   "source": [
    "print('The average rmse is: '+ str((rmsetmp_1 + rmsetmp_2 + rmsetmp_3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 加上同比/环比 特征 LightGBM 测试的效果不错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 120.68144567678694\n"
     ]
    }
   ],
   "source": [
    "trainset_1 = xgb.DMatrix(train_1, label=labels_1)\n",
    "testset_1 = xgb.DMatrix(test_1)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L2 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_1 = xgb.train(params, trainset_1, num_boost_round=4000)\n",
    "predict_xgb_1 = model_1.predict(testset_1)\n",
    "rmsetmp_1 = sp.sqrt(sp.mean((test_labels_1 - predict_xgb_1) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 122.20946253396676\n"
     ]
    }
   ],
   "source": [
    "trainset_2 = xgb.DMatrix(train_2, label=labels_2)\n",
    "testset_2 = xgb.DMatrix(test_2)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L2 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_2 = xgb.train(params, trainset_2, num_boost_round=4000)\n",
    "predict_xgb_2 = model_2.predict(testset_2)\n",
    "rmsetmp_2 = sp.sqrt(sp.mean((test_labels_2 - predict_xgb_2) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 117.56370604813299\n"
     ]
    }
   ],
   "source": [
    "trainset_3 = xgb.DMatrix(train_3, label=labels_3)\n",
    "testset_3 = xgb.DMatrix(test_3)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L3 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_3 = xgb.train(params, trainset_3, num_boost_round=4000)\n",
    "predict_xgb_3 = model_3.predict(testset_3)\n",
    "rmsetmp_3 = sp.sqrt(sp.mean((test_labels_3 - predict_xgb_3) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rmse is: 156.572741106\n"
     ]
    }
   ],
   "source": [
    "print('The average rmse is: '+ str((rmsetmp_1 + rmsetmp_2 + rmsetmp_3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 选取全特征, 加上同比/环比 数据, XGBoost 模型效果较差, 不使用这样的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 选用带时间窗口, 不带同比/环比数据集 特征集测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../raw/LiChuan/trainallfeatures.csv')\n",
    "submit = pd.read_csv('../../raw/CarsSaleForecast/yancheng_testA_20171225.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 去掉 2012 年数据\n",
    "data = data[data['year'] != 2012]\n",
    "labels = data['sale_quantity']\n",
    "data = data.drop(['sale_date', 'class_id', 'sale_quantity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练集 2013-01 至 2017-07, 测试集 2017-08\n",
    "train_1 = data[:-560]\n",
    "test_1 = data[-560:-420]\n",
    "labels_1 = labels[:-560]\n",
    "test_labels_1 = labels[-560:-420]\n",
    "\n",
    "# 训练集 2013-01 至 2017-08, 测试集 2017-09\n",
    "train_2 = data[:-420]\n",
    "test_2 = data[-420:-280]\n",
    "labels_2 = labels[:-420]\n",
    "test_labels_2 = labels[-420:-280]\n",
    "\n",
    "# 训练集 2013-01 至 2017-09, 测试集 2017-10\n",
    "train_3 = data[:-280]\n",
    "test_3 = data[-280:-140]\n",
    "labels_3 = labels[:-280]\n",
    "test_labels_3 = labels[-280:-140]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 145.116054859\n"
     ]
    }
   ],
   "source": [
    "lgb_train_1 = lgb.Dataset(train_1, labels_1)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.002,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 20,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_1 = lgb.train(params, lgb_train_1, 900)\n",
    "lgb_pred_1 = model_1.predict(test_1)\n",
    "rmsetmp_1 = sp.sqrt(sp.mean((test_labels_1 - lgb_pred_1) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 116.378154374\n"
     ]
    }
   ],
   "source": [
    "lgb_train_2 = lgb.Dataset(train_2, labels_2)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.002,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 20,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_2 = lgb.train(params, lgb_train_2, 900)\n",
    "lgb_pred_2 = model_2.predict(test_2)\n",
    "rmsetmp_2 = sp.sqrt(sp.mean((test_labels_2 - lgb_pred_2) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 150.31146246\n"
     ]
    }
   ],
   "source": [
    "lgb_train_3 = lgb.Dataset(train_3, labels_3)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.003,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 30,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_3 = lgb.train(params, lgb_train_3, 900)\n",
    "lgb_pred_3 = model_3.predict(test_3)\n",
    "rmsetmp_3 = sp.sqrt(sp.mean((test_labels_3 - lgb_pred_3) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rmse is: 137.268557231\n"
     ]
    }
   ],
   "source": [
    "print('The average rmse is: '+ str((rmsetmp_1 + rmsetmp_2 + rmsetmp_3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LightGBM 效果不如加上同比/环比 特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 193.746542278\n"
     ]
    }
   ],
   "source": [
    "trainset_1 = xgb.DMatrix(train_1, label=labels_1)\n",
    "testset_1 = xgb.DMatrix(test_1)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L2 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_1 = xgb.train(params, trainset_1, num_boost_round=4000)\n",
    "predict_xgb_1 = model_1.predict(testset_1)\n",
    "rmsetmp_1 = sp.sqrt(sp.mean((test_labels_1 - predict_xgb_1) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 135.365487254\n"
     ]
    }
   ],
   "source": [
    "trainset_2 = xgb.DMatrix(train_2, label=labels_2)\n",
    "testset_2 = xgb.DMatrix(test_2)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L2 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_2 = xgb.train(params, trainset_2, num_boost_round=4000)\n",
    "predict_xgb_2 = model_2.predict(testset_2)\n",
    "rmsetmp_2 = sp.sqrt(sp.mean((test_labels_2 - predict_xgb_2) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 131.289320436\n"
     ]
    }
   ],
   "source": [
    "trainset_3 = xgb.DMatrix(train_3, label=labels_3)\n",
    "testset_3 = xgb.DMatrix(test_3)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L3 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_3 = xgb.train(params, trainset_3, num_boost_round=4000)\n",
    "predict_xgb_3 = model_3.predict(testset_3)\n",
    "rmsetmp_3 = sp.sqrt(sp.mean((test_labels_3 - predict_xgb_3) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rmse is: 153.467116656\n"
     ]
    }
   ],
   "source": [
    "print('The average rmse is: '+ str((rmsetmp_1 + rmsetmp_2 + rmsetmp_3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 比上面加同比/环比, 略微强一点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不带时间窗口特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../raw/LiChuan/trainSaleDate.csv')\n",
    "submit = pd.read_csv('../../raw/CarsSaleForecast/yancheng_testA_20171225.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 去掉 2012 年数据\n",
    "data = data[data['year'] != 2012]\n",
    "labels = data['sale_quantity']\n",
    "data = data.drop(['sale_date', 'class_id', 'sale_quantity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 训练集 2013-01 至 2017-07, 测试集 2017-08\n",
    "train_1 = data[:-420]\n",
    "test_1 = data[-420:-280]\n",
    "labels_1 = labels[:-420]\n",
    "test_labels_1 = labels[-420:-280]\n",
    "\n",
    "# 训练集 2013-01 至 2017-08, 测试集 2017-09\n",
    "train_2 = data[:-280]\n",
    "test_2 = data[-280:-140]\n",
    "labels_2 = labels[:-280]\n",
    "test_labels_2 = labels[-280:-140]\n",
    "\n",
    "# 训练集 2013-01 至 2017-09, 测试集 2017-10\n",
    "train_3 = data[:-140]\n",
    "test_3 = data[-140:]\n",
    "labels_3 = labels[:-140]\n",
    "test_labels_3 = labels[-140:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 152.625651074\n"
     ]
    }
   ],
   "source": [
    "lgb_train_1 = lgb.Dataset(train_1, labels_1)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.002,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 20,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_1 = lgb.train(params, lgb_train_1, 900)\n",
    "lgb_pred_1 = model_1.predict(test_1)\n",
    "rmsetmp_1 = sp.sqrt(sp.mean((test_labels_1 - lgb_pred_1) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 120.574974352\n"
     ]
    }
   ],
   "source": [
    "lgb_train_2 = lgb.Dataset(train_2, labels_2)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.002,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 20,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_2 = lgb.train(params, lgb_train_2, 900)\n",
    "lgb_pred_2 = model_2.predict(test_2)\n",
    "rmsetmp_2 = sp.sqrt(sp.mean((test_labels_2 - lgb_pred_2) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 160.703307873\n"
     ]
    }
   ],
   "source": [
    "lgb_train_3 = lgb.Dataset(train_3, labels_3)\n",
    "\n",
    "params = {\n",
    "'learning_rate': 0.003,\n",
    "'boosting_type': 'gbdt',\n",
    "'objective': 'regression',\n",
    "'metric': 'rmse',\n",
    "'sub_feature': 0.8,\n",
    "'num_leaves': 35,\n",
    "'min_data': 30,\n",
    "'min_hessian': 1,\n",
    "'verbose': -1,}\n",
    "\n",
    "model_3 = lgb.train(params, lgb_train_3, 900)\n",
    "lgb_pred_3 = model_3.predict(test_3)\n",
    "rmsetmp_3 = sp.sqrt(sp.mean((test_labels_3 - lgb_pred_3) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rmse is: 144.634644433\n"
     ]
    }
   ],
   "source": [
    "print('The average rmse is: '+ str((rmsetmp_1 + rmsetmp_2 + rmsetmp_3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 140.010796927\n"
     ]
    }
   ],
   "source": [
    "trainset_1 = xgb.DMatrix(train_1, label=labels_1)\n",
    "testset_1 = xgb.DMatrix(test_1)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L2 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_1 = xgb.train(params, trainset_1, num_boost_round=4000)\n",
    "predict_xgb_1 = model_1.predict(testset_1)\n",
    "rmsetmp_1 = sp.sqrt(sp.mean((test_labels_1 - predict_xgb_1) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 163.353862481\n"
     ]
    }
   ],
   "source": [
    "trainset_2 = xgb.DMatrix(train_2, label=labels_2)\n",
    "testset_2 = xgb.DMatrix(test_2)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L2 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_2 = xgb.train(params, trainset_2, num_boost_round=4000)\n",
    "predict_xgb_2 = model_2.predict(testset_2)\n",
    "rmsetmp_2 = sp.sqrt(sp.mean((test_labels_2 - predict_xgb_2) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This time rmse is: 152.222508663\n"
     ]
    }
   ],
   "source": [
    "trainset_3 = xgb.DMatrix(train_3, label=labels_3)\n",
    "testset_3 = xgb.DMatrix(test_3)\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', # 基于树的模型进行计算\n",
    "    'objective': 'reg:linear', # 线性回归\n",
    "    'eval_metric': 'rmse', # RMSE 评价函数\n",
    "    'gamma': 0.1, # 在树的叶子节点上进一步划分所需的最小损失减少。算法越大，越保守。\n",
    "    'min_child_weight': 1.1, # 孩子节点中最小的样本权重和。如果一个叶子节点的样本权重和小于min_child_weight则拆分过程结束。\n",
    "    'max_depth': 5, # 树的最大深度。缺省值为6\n",
    "    'lambda': 10, # L3 正则的惩罚系数\n",
    "    'subsample': 0.8, # 用于训练模型的子样本占整个样本集合的比例。\n",
    "    'colsample_bytree': 0.8, # 在建立树时对特征采样的比例。\n",
    "    'tree_method': 'exact' # 树的构造算法-Exact greedy algorithm(确切贪心算法)\n",
    "    }\n",
    "    \n",
    "model_3 = xgb.train(params, trainset_3, num_boost_round=4000)\n",
    "predict_xgb_3 = model_3.predict(testset_3)\n",
    "rmsetmp_3 = sp.sqrt(sp.mean((test_labels_3 - predict_xgb_3) ** 2))\n",
    "\n",
    "print('This time rmse is: '+ str(rmsetmp_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rmse is: 151.862389357\n"
     ]
    }
   ],
   "source": [
    "print('The average rmse is: '+ str((rmsetmp_1 + rmsetmp_2 + rmsetmp_3)/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* XGBoost 效果似乎也不咋地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.838369395 125.853569134 89.5277146076\n"
     ]
    }
   ],
   "source": [
    "rmse_1 = sp.sqrt(sp.mean((lgb_pred_1 - predict_xgb_1) ** 2))\n",
    "\n",
    "rmse_2 = sp.sqrt(sp.mean((lgb_pred_2 - predict_xgb_2) ** 2))\n",
    "\n",
    "rmse_3 = sp.sqrt(sp.mean((lgb_pred_3 - predict_xgb_3) ** 2))\n",
    "\n",
    "print(rmse_1, rmse_2, rmse_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
